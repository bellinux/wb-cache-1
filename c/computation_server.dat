18|147|Public
50|$|A {{mechanism}} {{is considered to}} be truthful if the agents gain nothing by lying about their or other agents' values.A good example would be a leader election algorithm that selects a <b>computation</b> <b>server</b> within a network. The algorithm specifies that agents should send their total computational power to each other, after which the most powerful agent is chosen as the leader to complete the task. In this algorithm agents may lie about their true computation power because they are potentially in danger of being tasked with CPU-intensive jobs which will reduce their power to complete local jobs. This can be overcome with the help of truthful mechanisms which, without any a priori knowledge of the existing data and inputs of each agent, cause each agent to respond truthfully to requests.|$|E
30|$|The {{location}} of the routers is known to the centralized <b>computation</b> <b>server.</b> This information {{can be used to}} compute the coordinated and non-coordinated links for each pair of nodes.|$|E
40|$|In this paper, {{we study}} the {{security}} of two recently proposed privacy-preserving biometric authentication protocols that employ packed somewhat homomorphic encryption schemes based on ideal lattices and ring-LWE, respectively. These two schemes have the same structure and have distributed architecture consisting of three entities: a client server, a <b>computation</b> <b>server,</b> and an authentication server. We present a simple attack algorithm that enables a malicious <b>computation</b> <b>server</b> to learn the biometric templates in at most 2 N ´ τ queries, where N is the bit-length of a biometric template and τ the authentication threshold. The main enabler of the attack is that a malicious <b>computation</b> <b>server</b> can send an encryption of the inner product of the target biometric template with a bitstring of his own choice, instead of the securely computed Hamming distance between the fresh and stored biometric templates. We also discuss possible countermeasures to mitigate the attack using private information retrieval and signatures of correct computation...|$|E
40|$|We {{investigate}} how {{a system of}} <b>computation</b> <b>servers,</b> i. e. computers which can accept requests to execute jobs, can efficiently share processing power through load balancing [20, 4]. The goal is to send jobs to lightly loaded servers to improve some per- formance metric, such as response time. We assume a distributed system model where each server inde- pendently receives a stream of jobs and has a job- scheduling agent which determines whether each job should execute locally or on a remote server. If a re...|$|R
40|$|New {{levels of}} {{software}} composition become possible through advances in distributed communication services. In this paper {{we focus on}} the composition of megamodules, which are large distributed components or <b>computation</b> <b>servers</b> that are autonomously operated and maintained. The composition of megamodules offers various challenges. Megamodules are not necessarily all accessible by the same distribution protocol (such as CORBA, DCOM, RMI and DCE). Their concurrent nature and potentially long duration of service execution necessitates asynchronous invocation and collection of results. Novel needs and opportunities for optimization arise when composing megamodules. In order to meet these challenges, we have defined a purely compositional language called CHAIMS, and are now developing the architecture supporting this language. In this paper we describe CHAIMS and how it meets the challenges of composing megamodules...|$|R
40|$|Distributed {{problem solving}} {{on the web}} is {{becoming}} more and more important. Client server architectures are often confronted with server overload. The process of browsing a large number of alternative solutions is particularly tedious. In this paper, we present a methodology for distributing the <b>computation</b> between <b>server</b> and client. The idea is to formalize the problem as a constraint satisfaction problem (CSP) ...|$|R
40|$|Adaptive domain {{decomposition}} exemplifies {{the problem of}} integrating heterogeneous software components with intermediate coupling granularity. This paper describes an experiment where a data‐parallel (HPF) client interfaces with a sequential <b>computation</b> <b>server</b> through Java. We show that seamless integration of data‐parallelism is possible, but requires most of the tools from the Java palette: Java Native Interface (JNI), Remote Method Invocation (RMI), callbacks and threads...|$|E
40|$|Abstract- The {{comprehensiveness}} {{and complexity}} of physiological models will increase dramatically during the next decade. Advances in computer power, biological pathway information, information on molecular mechanisms, and new genetic data are all driving factors for these changes. This paper proposes an information system to support complex physiological information models. The system architecture requires three essential parts: a networked Client that generates a request for information; a <b>Computation</b> <b>Server</b> that performs the calculation; and a Model Database that provides detailed parameters to the <b>Computation</b> <b>Server</b> to facilitate solution. The Model Database is organized as an objectoriented hierarchy following the design used to store medical images and their metadata {{in accordance with the}} international DICOM standard for medical images [1]. A fourth critical component is also defined: a Case Database that archives the input data that has been used by the Client to define the case to be computed, and archives the output from the solution. This system is applied to a complex contemporary model of the electrophysiology of cardiac myocyte cells [the WIJ model; see [2] and [3]]. The resulting architecture has been demonstrated over the internet [ICMIT. MIT. EDU/myocyte_model. html]...|$|E
40|$|A <b>computation</b> <b>server</b> {{is needed}} to provide {{analysis}} programs to Korean biologists, especially genome researchers, on GINet. For each analysis program, we implmented an input form with HTML and a CGI program for interface between an input form and an analysis program with C language on GINet computation Web server. We made two construction methods of CGI programs for analysis programs, and implemented all CGI programs based on the methods followd by modifying each CGI program for specific processing of each analysis program. On the server ten programs are available now, which include most frequently used ones and those developed by our team, and most programs which will be ported or developed by our team will be available on the Web server. ope...|$|E
40|$|The Control Structure Interaction Program is a {{technology}} development program for spacecraft that exhibit {{interactions between the}} control system and structural dynamics. The program objectives include development and verification of new design concepts (such as active structure) and new tools (such as a combined structure and control optimization algorithm) and their verification in ground and possibly flight test. The new CSI design methodology is centered around interdisciplinary engineers using new tools that closely integrate structures and controls. Verification is an important CSI theme and analysts will be closely integrated to the CSI Test Bed laboratory. Components, concepts, tools and algorithms will be developed and tested in the lab and in future Shuttle-based flight experiments. The design methodology is summarized in block diagrams depicting {{the evolution of a}} spacecraft design and descriptions of analytical capabilities used in the process. The multiyear JPL CSI implementation plan is described along with the essentials of several new tools. A distributed network of <b>computation</b> <b>servers</b> and workstations was designed that will provide a state-of-the-art development base for the CSI technologies...|$|R
3000|$|...,f). The {{resulting}} ciphertext {{is used as}} fingerprint {{and allows}} <b>computations</b> at the <b>servers.</b> Notice that this scheme is asymmetric: the public key is used for encryption and computations over f; the private key is used to decrypt the result.|$|R
40|$|Smalltalk is an {{object-oriented}} programming language and highly interactive uniformly object-structured programming environment, originally {{developed for the}} Xerox family of personal workstations. The Smalltalk programming environment supports a single user on a single processor in a single object address space. This thesis describes the design and implementation of Distributed Smalltalk. Distributed Smalltalk extends the Smalltalk system to support the interaction of many users on many machines. It provides communication and interaction among geographically remote Smalltalk users, direct access to remote objects, the ability to construct distributed applications in the Smalltalk environment, and a degree of object sharing among users. Applications of Distributed Smalltalk include mail systems, remote <b>computation</b> <b>servers,</b> remote file servers, and collaborative software development. The distributed aspects of the system are largely user transparent and preserve the reactive quality of Smalltalk objects. A system is reactive {{to the degree that}} objects in the system can be easily presented for inspection or modification. Smalltalk is highly reactive in that all objects in the system can be so presented. Inheritance is a fundamental property of Smalltalk that allows objects to acquire behavior from other objects. In designing Distributed Smalltalk, we found that the interaction of inheritance and reactiveness was a major source of difficulty and that this interaction had significant impact on the design of a distributed system. Neither inheritance or reactiveness scaled well fro [...] ...|$|R
40|$|Advertising Traffic Engineering Information in BGP draft-gredler-bgp-te- 00 This {{document}} {{defines a}} new Border Gateway Protocol Network Layer Reachability Information (BGP NLRI) encoding format {{that can be}} used to distribute Traffic Engineering (TE) link information. Links can be either physical links connecting physical nodes, or virtual paths between physical or abstract nodes. The TE information is carried via the BGP, thereby reusing protocol algorithms, operational experience, and administrative processes, such as inter-provider peering agreements. The BGP protocol carrying Traffic Engineering (TE) information would provide a well-defined, uniform, policy-controlled interface from the network to outside servers that need to learn the network topology in real-time, for example an ALTO Server or a Path <b>Computation</b> <b>Server.</b> Having TE information from remote areas and/or Autonomous Systems would allow path computation for inter-area and/or inter-AS sourcerouted unicast and multicast tunnels. Requirements Languag...|$|E
40|$|More {{and more}} secure {{symmetric}} key encryption algorithms with long keys {{are being developed}} and used. But, the threat is still near to us. All the attackers {{have to do is}} guessing the password of the user to make a brute force attack on the encrypted data, not the secret key. In this paper, we propose a blind computation service which {{can be used as a}} part of an encryption and decryption procedure so that the probability of successful cryptanalysis of encrypted data is controlled by both the blind <b>computation</b> <b>server</b> and the user himself. If the blind computation service is not compromised, the user can enjoy the security of using a completely random password. Even when the blind computation service is compromised, the security remains at the same level as a normal secret key protection system which uses only the user's password to encrypt data...|$|E
40|$|This paper {{presents}} an architecture for implementation of multiple access control policies within a CORBASEC framework. CORBASEC is a security service specification provided by OMG for implementation {{of security in}} CORBA compliant systems. Because {{of the differences in}} ORB implementations among vendors and divergent security needs of the CORBA based applications in different commercial markets, CORBASEC provides a general security reference model. The reference model can then be used by implementers to customize a security model depending upon their system and application needs. We have extracted one such model from the CORBASEC security reference model (or framework) to be used in an architecture whose objective is to support multiple access control policies within a single unified system. Throughout this paper we refer to this architecture using the acronym MACP. The central component of MACP is an Access <b>Computation</b> <b>Server</b> (ACS) which hosts multiple modules representing different se [...] ...|$|E
3000|$|... {{as in the}} Hyrec {{recommender}} system [54]. Spotify [6] illustrates the reverse case, in which data is indexed on centralized <b>servers</b> (<b>computation)</b> but data transfers occur in a peer-to-peer fashion between users. User interactions may also be exploited to influence data placement, as {{in the work of}} Pujol et al. [55], where the data of a social network is placed according to how users interact with one another.|$|R
40|$|Multi-player game servers {{present an}} {{interesting}} computational challenge since {{the desire to}} support large numbers of interacting players in real-time places strict constraints on <b>server</b> <b>computation</b> time. These high computational needs make these game servers attractive candidates for parallel processing because at high player counts the server becomes a performance bottleneck. This work investigates the issues in parallelizing such servers by parallelizing one particular application, the QuakeWorld game server. The approach presented in this report focuses on utilizing the spatial locality of players within the game environment to achieve more efficient parallelization. The main idea is to divide the players amongst all available threads {{in such a way}} as to only process clients sequentially if they are in physical proximity to one another. This is done by partitioning the virtual world into distinct areas and processing players within each area sequentially, while each distinct area of the virtual world is processed concurrently. To further increase the capacity, other stages of the <b>server</b> <b>computation</b> are also parallelized. This parallel implementation of the QuakeWorld game server was confirmed to b...|$|R
40|$|Abstract. Due to {{the high}} {{frequency}} in location updates and the expensive cost of continuous query processing, <b>server</b> <b>computation</b> capacity and wireless communication bandwidth are the two limiting factors for large-scale deployment of moving object database systems. Many techniques have been proposed to address the server bottleneck including one using distributed servers. To address both of the scalability factors, P 2 P computing has been considered. These schemes enable moving objects to participate as a peer in query processing to substantially reduce the demand on <b>server</b> <b>computation,</b> and wireless communications associated with location updates. Most of these techniques, however, assume an open-space environment. In this paper, we investigate a P 2 P computing technique for continuous kNN queries in a network environment. Since network distance is different from Euclidean distance, techniques designed specifically for an open space cannot be easily adapted for our environment. We present {{the details of the}} proposed technique, and discuss our simulation study. The performance results indicate that this technique can significantly reduce server workload and wireless communication costs. ...|$|R
40|$|First {{released}} in 1988, CoCoA is a freely available special-purpose system for doing Computations in Commutative Algebra. It belongs to an elite group of highly specialized systems (like "Macaulay 2 " and "Singular") having as their main forte {{the capability to}} calculate Groebner bases. This means that CoCoA is optimized for working with multivariate polynomials, their ideals and modules, and operations on these objects. Other special strengths of CoCoA include polynomial factorization, exact linear algebra, Hilbert functions, zero-dimensional schemes, and toric ideals. About 10 years ago, a new initiative began: namely, to rebuild the whole software in C++ without the inherent limitations of the original. The new software comprises three main components: a C++ library (CoCoALib), an algebra <b>computation</b> <b>server</b> (CoCoAServer), and an interactive system (CoCoA- 5). Of these components CoCoALib is the heart; it embodies all the ``mathematical knowledge'' {{and it is the}} most evolved part. All the new code is free and open source software (released under GPL) ...|$|E
40|$|What is IRISS-C/I? IRISS-C/I is {{a visiting}} {{researchers}} ’ programme at CEPS/INSTEAD, a policy and research centre based in Luxembourg. Its {{mission is to}} organise short visits of researchers willing to undertake empirical research in economics and other social sciences using the archive of micro-data available at the Centre. In 1998, CEPS/INSTEAD has been identified by the European Commission {{as one of the}} few Large Scale Facilities in the social sciences, and, since then, IRISS-C/I fellowships offer researchers (both junior and senior) the opportunity to spend time carrying out their own research using the local research facilities. The expected duration of visits is in the range of 2 to 12 weeks. During their stay, visitors are granted access to the CEPS/INSTEAD archive of public-use micro-data (primarily internationally comparable longitudinal surveys on living conditions) and to extensive data documentation. They are assigned an office (shared or single) and have networked access to a powerful <b>computation</b> <b>server</b> that acts as host for the data archive and supports an array of commercial and open-source statistical software packages. Scientific an...|$|E
40|$|Abstract:   Cloud {{computing}} make {{it possible}} to store large amounts of data. To monitoring User an entity whose data to be stores in the cloud and relies on the cloud for data storage and <b>computation,</b> <b>server</b> is an entity which manages by cloud service provider to provide data storage service and has significant storage space and computation resources and third party is an optional TPA who has expertise and capabilities that users may not have is trusted to assess and expose risk of cloud storage services on behalf of the users upon request. Our proposed work cloud computing architecture for analysis is cloud data storage a user stores his data through a CSP into set of cloud servers which are running in a simultaneous cooperated and distributed. We are proving an option to the user while uploading that is this file is public or group or private for data sharing. Considering the cloud data are dynamic in nature, the proposed design further supports secure and efficient dynamic operations on outsourced data, including block modification, deletion, and append...|$|E
40|$|A novel {{parallel}} hybrid {{particle swarm optimization}} algorithm named hmPSO is presented. The new algorithm combines particle swarm optimization (PSO) with a local search method which aims to accelerate the rate of convergence. The PSO provides initial guesses to the local search method and the local search accelerates PSO with its solutions. The hybrid global optimization algorithm adjusts its searching space through the local search results. Parallelization {{is based on the}} client-server model, which is ideal for asynchronous distributed <b>computations.</b> The <b>server,</b> the center of data exchange, manages requests and coordinates the time-consuming objective function computations undertaken by individual clients which locate in separate processors. A case study in geotechnical engineering demonstrates the effectiveness and efficiency of the proposed algorithm...|$|R
40|$|Cryptographic tools, such as secure {{computation}} or homomorphic encryption, {{are very}} computation-ally expensive. This makes their use for confidentiality protection of client’s data against an untrusted service provider uneconomical in most applications of cloud computing. In this paper we present tech-niques for randomizing data using light-weight operations and then securely outsourcing the <b>computation</b> to a <b>server.</b> We discuss how to formally assess {{the security of}} our approach and present linear program-ming as a case study. ...|$|R
40|$|MMORPG (Massively Multiplayer Online Role Playing Games) is {{the most}} popular genre among network gamers, and now attract {{millions}} of users, who play simultaneously in an evolving virtual world. This huge number of concurrent players requires the availability of high performance <b>computation</b> <b>servers.</b> Additionally, gaming aware distribution mechanisms are needed to distribute game instances among servers to avoid load imbalances that affect performance negatively. In this work, we tackle the problem of game distribution and scalability by means of a hybrid Client-Server/P 2 P architecture that can scale dynamically according to the demand. To manage peak loads that occur during the game, we distribute game computation across the system according to the behavior of MMORPGs. We distinguish between the computation associated with the Main Game, that affects all players, and the computation of Auxiliary Games that affects only a few players and acts in isolation from the execution of the Main Game. Taking this distinction into account, we propose a mechanism that is focused in the distribution of Auxiliary Games, as an entity, across the pool of servers and peers of the underlying hybrid architecture. We evaluate the performance of the balancing mechanism taking the criteria of latency and reliability into account, and we compare the effectiveness of the mechanism with a classic approach that applies load balancing to individually players in a Client-Server system. We show that the balancing mechanism based on the latency criteria provides lower latency than the classical proposal, while in relation to reliability, we obtain a failure probability of under 0. 9 % in the worst case, which is amply compensated by the scalability provided by the use of the P 2 P area. This work was supported by the MEyC-Spain under contract TIN 2011 - 28689 -C 02 - 02 and the CUR of DIUE of GENCAT and the European Social Fun...|$|R
40|$|First {{released}} in 1988 under the scientific direction of Lorenzo Robbiano, CoCoA is a special-purpose system for doing Computations in Commutative Algebra: i. e. {{it is an}} interactive system specialized in the algorithmic treatment of polynomials, and is freely available for most common platforms. About 10 years ago, a new initiative began: namely, to rebuild the CoCoA software laboratory while removing the inherent limitations of the original. The new software comprises three main components: a C++ library (CoCoALib), an algebra <b>computation</b> <b>server</b> (CoCoAServer), and an interactive system (CoCoA- 5). Of these components CoCoALib is the heart; it embodies all the ``mathematical knowledge'' and it is currently the most evolved part. The role {{of the other two}} parts is to make CoCoALib's capabilities more easily accessible. The first result of this approach is the collaboration with the project ApCoCoA (Applied Computations in Commutative Algebra), lead by Martin Kreuzer in Passau, which is built upon CoCoA and CoCoALib. It applies both symbolic and numerical computation to tackle ``real world'' problems. All the new code is free and open source software. It is downloadable from our website ([URL] and released under GPL...|$|E
40|$|Traditional {{standalone}} {{embedded system}} {{is limited in}} their functionality, flexibility, and scalability. Fog computing platform, characterized by pushing the cloud services to the network edge, is a promising solution to support and strengthen traditional embedded system. Resource management is always a critical issue to the system performance. In this paper, we consider a fog computing supported software-defined embedded system, where task images lay in the storage server while computations can be conducted on either embedded device or a <b>computation</b> <b>server.</b> It is significant to design an efficient task scheduling and resource management strategy with minimized task completion time for promoting the user experience. To this end, three issues are investigated in this paper: 1) how to balance the workload on a client device and computation servers, i. e., task scheduling, 2) how to place task images on storage servers, i. e., resource management, and 3) how to balance the I/O interrupt requests among the storage servers. They are jointly considered and formulated as a mixed-integer nonlinear programming problem. To deal with its high computation complexity, a computation-efficient solution is proposed based on our formulation and validated by extensive simulation based studies...|$|E
40|$|We {{investigate}} a wireless computing architecture, where mobile terminals can execute their computation tasks either 1) locally, at the terminal's processor, or 2) remotely, assisted by the network infrastructure, or even 3) combining the former two options. Remote execution involves: 1) sending the task to a <b>computation</b> <b>server</b> via the wireless network, 2) executing {{the task at}} the server, and 3) downloading {{the results of the}} computation back to the terminal. Hence, it results to energy savings at the terminal (sparing its processor from computations) and execution speed gains due to (typically) faster server processor(s), as well as overheads due to the terminal server wireless communication. The net gains (or losses) are contingent on network connectivity and server load. These may vary in time, depending on user mobility, network, and server congestion (due to the concurrent sessions/connections from other terminals). In local execution, the wireless terminal faces the dilemma of power managing the processor, trading-off fast execution versus low energy consumption. We model the system within a Markovian dynamic control framework, allowing the computation of optimal execution policies. We study the associated energy versus delay trade-off and assess the performance gains attained in various test cases in comparison to conventional benchmark policies...|$|E
40|$|Cloud {{computing}} is a {{boon for}} both business and private use, but data security concerns slow its adoption. Fully homomorphic encryption (FHE) offers {{the means by which}} the cloud computing can be performed on encrypted data, obviating the data security concerns. FHE is not without its cost, as FHE operations take orders of magnitude more processing time and memory than the same operations on unencrypted data. Cloud computing can be leveraged to reduce the time taken by bringing to bear parallel processing. This paper presents an implementation of a processing dispatcher which takes an iterative set of operations on FHE encrypted data and splits them between a number of processing engines. A private cloud was implemented to support the processing engines. The processing time was measured with 1, 2, 4, and 8 processing engines. The time taken to perform the calculations with the four levels of parallelization, as well as the amount of time used in data transfers are presented. In addition, the time the <b>computation</b> <b>servers</b> spent in each of addition, subtraction, multiplication, and division are laid out. An analysis of the time gained by parallel processing is presented. The experimental results shows that the proposed parallel processing of Gentry's encryption improves the performance better than the computations on a single node. This research provides the following contributions. A private cloud was built to support parallel processing of homomorphic encryption in the cloud. A client-server model was created to evaluate cloud computing of the Gentry's encryption algorithm. A distributed algorithm was developed to support parallel processing of the Gentry's algorithm for evaluation on the cloud. An experiment was setup for the evaluation of the Gentry's algorithm, and the results of the evaluation show that the distributed algorithm can be used to speed up the processing of the Gentry's algorithm with cloud computing...|$|R
30|$|CPU, Storage and Network {{are common}} <b>server</b> <b>computation,</b> memory and network {{characteristics}} {{that we have}} chosen for this investigation [7]. The CPU time used and utilisation rates are typically the most influential factor. The RAM (memory) utilisation rate and storage access are equally important. In distributed applications, network parameters such as bandwidth, latency and throughput have an influence of service QoS (we consider here the latter). Note that in principle, the specific characteristics could be varied.|$|R
50|$|Thin clients {{occur as}} {{components}} of a broader computing infrastructure, where many clients share their <b>computations</b> with a <b>server</b> or server farm. The server-side infrastructure makes use of cloud computing software such as application virtualization, hosted shared desktop (HSD) or desktop virtualization (VDI). This combination forms what is known today as a cloud based system where desktop resources are centralized into one or more data centers. The benefits of centralization are hardware resource optimization, reduced software maintenance, and improved security.|$|R
40|$|Blind quantum {{computation}} (BQC) {{is a new}} type of {{quantum computation}} model. BQC allows a client (Alice) who does not have enough sophisticated technology and knowledge to perform universal quantum computation and resorts a remote quantum <b>computation</b> <b>server</b> (Bob) to delegate universal quantum computation. During the computation, Bob cannot know Alice's inputs, algorithm and outputs. In single-server BQC protocol, it requires Alice to prepare and distribute single-photon states to Bob. Unfortunately, the distributed single photons will suffer from noise, which not only makes the single-photon state decoherence, but also makes it loss. In this protocol, we describe an anti-noise BQC protocol, which combined the ideas of faithful distribution of single-photon state in collective noise, the feasible quantum nondemolition measurement and Broadbent-Fitzsimons-Kashefi (BFK) protocol. This protocol has several advantages. First, Alice does not require any auxiliary resources, which reduces the client's economic cost. Second, this protocol not only can protect the state from the collective noise, but also can distill the single photon from photon loss. Third, the noise setup in Bob is based on the linear optics, and it is also feasible in experiment. This anti-noise BQC may show {{that it is possible to}} perform the BQC protocol in a noisy environment. Comment: 6 pages, 4 figure...|$|E
40|$|MASH {{stands for}} "Macros for the Automation of SHadow". It allows {{to run a}} set of {{ray-tracing}} simulations, {{for a range of}} photon energies for example, fully automatically. Undulator gaps, crystal angles etc. are tuned automatically. Important output parameters, such as photon flux, photon irradiance, focal spot size, bandwidth, etc. are then directly provided as function of photon energy. A photon energy scan is probably the most commonly requested one, but any parameter or set of parameters can be scanned through as well. Heat load calculations with finite element analysis providing temperatures, stress and deformations (Comsol) are fully integrated. The deformations can be fed back into the ray-tracing process simply by activating a switch. MASH tries to hide program internals such as file names, calls to pre-processors etc., so that the user (nearly) only needs to provide the optical setup. It comes with a web interface, which allows to run it remotely on a central <b>computation</b> <b>server.</b> Hence, no local installation or licenses are required, just a web browser and access to the local network. Numerous tools are provided to look at the ray-tracing results in the web-browser. The results can be also downloaded for local analysis. All files are human readable text files, that can be easily imported into third-party programs for further processing. All set parameters are stored in a single human-readable file in XML format...|$|E
40|$|Many {{services}} that people use daily require computation {{that depends on}} the private data of multiple parties. While the utility of the final result of such interactions outweighs the privacy concerns related to output release, the inputs for such computations are much more sensitive and need to be protected. Secure multiparty computation (MPC) considers the question of constructing computation protocols that reveal nothing more about their inputs than what is inherently leaked by the output. There have been strong theoretical results that demonstrate that every functionality can be computed securely. However, these protocols remain unused in practical solutions since they introduce efficiency overhead prohibitive for most applications. Generic multiparty computation techniques address homogeneous setups with respect to the resources available to the participants and the adversarial model. On the other hand, realistic scenarios present a wide diversity of heterogeneous environments where different participants have different available resources and different incentives to misbehave and collude. In this thesis we introduce techniques for multiparty computation that focus on heterogeneous settings. We present solutions tailored to address different types of asymmetric constraints and improve the efficiency of existing approaches in these scenarios. We tackle the question from three main directions: New Computational Models for MPC - We explore different computational models that enable us to overcome inherent inefficiencies of generic MPC solutions using circuit representation for the evaluated functionality. First, we show how we can use random access machines to construct MPC protocols that add only polylogarithmic overhead to the running time of the insecure version of the underlying functionality. This allows to achieve MPC constructions with computational complexity sublinear in the size for their inputs, which is very important for computations that use large databases. We also consider multivariate polynomials which yield more succinct representations for the functionalities they implement than circuits, {{and at the same time}} a large collection of problems are naturally and efficiently expressed as multivariate polynomials. We construct an MPC protocol for multivariate polynomials, which improves the communication complexity of corresponding circuit solutions, and provides currently the most efficient solution for multiparty set intersection in the fully malicious case. Outsourcing Computation - The goal in this setting is to utilize the resources of a single powerful service provider for the work that computationally weak clients need to perform on their data. We present a new paradigm for constructing verifiable computation (VC) schemes, which enables a computationally limited client to verify efficiently the result of a large computation. Our construction is based on attribute-based encryption and avoids expensive primitives such as fully homomorphic encryption andprobabilistically checkable proofs underlying existing VC schemes. Additionally our solution enjoys two new useful properties: public delegation and verification. We further introduce the model of server-aided computation where we utilize the computational power of an outsourcing party to assist the execution and improve the efficiency of MPC protocols. For this purpose we define a new adversarial model of non-collusion, which provides room for more efficient constructions that rely almost completely only on symmetric key operations, and at the same time captures realistic settings for adversarial behavior. In this model we propose protocols for generic secure computation that offload the work of most of the parties to the <b>computation</b> <b>server.</b> We also construct a specialized server-aided two party set intersection protocol that achieves better efficiencies for the two participants than existing solutions. Outsourcing in many cases concerns only data storage and while outsourcing the data of a single party is useful, providing a way for data sharing among different clients of the service is the more interesting and useful setup. However, this scenario brings new challenges for access control since the access control rules and data accesses become private data for the clients with respect to the service provide. We propose an approach that offers trade-offs between the privacy provided for the clients and the communication overhead incurred for each data access. Efficient Private Search in Practice - We consider the question of private search from a different perspective compared to traditional settings for MPC. We start with strict efficiency requirements motivated by speeds of available hardware and what is considered acceptable overhead from practical point of view. Then we adopt relaxed definitions of privacy, which still provide meaningful security guarantees while allowing us to meet the efficiency requirements. In this setting we design a security architecture and implement a system for data sharing based on encrypted search, which achieves only 30 % overhead compared to non-secure solutions on realistic workloads...|$|E
40|$|Introduction Most {{object-oriented}} database systems and persistent object stores {{have been constructed}} in a workstation-server environment in which client workstations {{are connected to the}} server machines by a local-area network (e. g., an Ethernet). The servers are responsible for the persistent and consistent storage of the object base and for preventing unauthorized access to the data. Application programs are carried out on the client workstations to exploit the resources of the workstations and to avoid <b>computation</b> on <b>servers,</b> the potential bottlenecks of the system. Typically, {{object-oriented database}}s have been used in very computation-intensive and/or highly interactive applications (e. g., engineering applications), and therefore, techniques such as pointer swizzling [Mos 92, KK 95] have been used to speed up the processing of persistent objects at workstations. In this environment, scalabilty means that an arbitrary number of client workstations can be connected to the s...|$|R
40|$|In {{a common}} use case for cloud computing, clients upload data and <b>computation</b> to <b>servers</b> that are managed by a thirdparty {{infrastructure}} provider. We describe MrCrypt, {{a system that}} provides data confidentiality in this setting by executing client computations on encrypted data. MrCrypt statically analyzes a program to identify the set of operations on each input data column, in order to select an appropriate homomorphic encryption scheme for that column, and then transforms the program to operate over encrypted data. The encrypted data and transformed program are uploaded to the server and executed as usual, {{and the result of}} the computation is decrypted on the client side. We have implemented MrCrypt for Java and illustrate its practicality on three standard benchmark suites for the Hadoop MapReduce framework. We have also formalized the approach and proven several soundness and security guarantees. 1...|$|R
40|$|Abstract—The Hadoop Distributed File System (HDFS) is {{designed}} to store very large data sets reliably, and to stream those data sets at high bandwidth to user applications. In a large cluster, thousands of servers both host directly attached storage and execute user application tasks. By distributing storage and <b>computation</b> across many <b>servers,</b> the resource can grow with demand while remaining economical at every size. We describe the architecture of HDFS and report on experience using HDFS to manage 25 petabytes of enterprise data at Yahoo!...|$|R
