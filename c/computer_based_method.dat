33|10000|Public
40|$|Abstract- Sign {{language}} {{is the language}} of communication for the deaf people. It is the collection of standardized gestures, with its own grammar. The objective {{of this paper is to}} design a useful and fully functional real world product that efficiently translates the movements of fingers into American Sign Language. This paper describes <b>computer</b> <b>based</b> <b>method</b> to enhance the communication between normal people and deaf-mute people...|$|E
40|$|For a quick {{estimation}} {{concerning a}} higher automation of an assembly line, the Fraunhofer IPA developed a <b>computer</b> <b>based</b> <b>method</b> for getting a classification {{of the potential}} for higher automation of different assembly processes. This method is based on technical and economical data. It can e used as a database for decision making or to show the potential for automation during a rationalisation process of an assembly-workflow...|$|E
40|$|A summary {{statistics}} such as {{are often the}} first outputs of data analysis. Then next thing {{we want to know}} is the accuracy of The most common measure of an estimator 2 ̆ 7 s accuracy is the standard error. The bootstrap method is a <b>computer</b> <b>based</b> <b>method</b> fo assigning measures of accuracy to statistical. estimates. In this writing we will show how to obtain the standard error of a statistic using the bootstrap...|$|E
30|$|Here {{we present}} optical photon {{reassignment}} microscopy (OPRA). It is an optical realization of these <b>computer</b> <b>based</b> <b>methods</b> which avoids {{the need for}} data processing. Furthermore at a different scaling ratio, our method is applicable to the direct visualisation of high-resolution imaging methods like STED.|$|R
40|$|This paper {{presents}} the principal {{features of the}} models tested by IMD. Methods of storing and manipulating the data using modern computer techniques are discussed. The <b>computer</b> <b>based</b> <b>methods</b> are compared and contrasted with more traditional methods of using randomly acquired model data. Peer reviewed: NoNRC publication: Ye...|$|R
40|$|Impervious cover {{proportion}} is recognised as {{an important}} parameter in hydrologic modelling. Recent advances in the spatial sciences provide new capabilities for estimation of this value. This paper describes an assessment of seven estimation techniques, including human and <b>computer</b> <b>based</b> <b>methods</b> requiring a range of different input data and skill levels. In order to quantify the value of any gain in accuracy, the results from each method were applied to a real catchment using event based (WBNM) and continuous simulation (MUSIC) hydrologic models. The results showed that use of high resolution satellite imagery enabled the most accurate measurements of impervious cover proportion to be made, however in general human and <b>computer</b> <b>based</b> <b>methods</b> are comparable in terms of accuracy. Also {{it was found that}} in some circumstances the sensitivity of peak discharge and runoff volume estimates to error in impervious cover proportion can be very high, confirming the need for accurate measurement...|$|R
40|$|In {{this study}} {{proteins}} were analyzed from pea plants at three different growth stages of stem by spectrophotometric i. e Lowry and Bradford quantitative methods and computer colour intensity based method. Though Spectrophotometric methods {{are regarded as}} classical methods, we report an alternate <b>computer</b> <b>based</b> <b>method</b> which gave comparable results. Computer software was developed the for protein analysis which is easier, time and money saving method {{as compared to the}} classical methods...|$|E
40|$|Purpose A {{new method}} of {{measuring}} visibility taking context into account introduced by Alexander Wertheim has been validated in this study. The {{purpose was to}} see if a simple and practical technique could render robust and useful quantitatively measurable results. Methods An ordinary traffic sign, placed against different backgrounds, was used as a target. Subjects were told to fixate the sign and then slowly deviate with their gaze until they could no longer see the sign in the corner of their eye. This angle was then used as an index for conspicuity. A simple paper and pen method was tested as well as a <b>computer</b> <b>based</b> <b>method.</b> Results Results for the paper and pen method showed low variance among subjects and a clear correlation between the conspicuity index and intuitive judgments of the visibility of the signs against different backgrounds. For the <b>computer</b> <b>based</b> <b>method</b> the variability was higher but the results still significant between the two groups of dynamic versus static signs. Conclusions The conclusion is that the conspicuity index method is a simple and useful method that renders quantitative measurements of the visibility of targets taken context into account...|$|E
40|$|A <b>computer</b> <b>based</b> <b>method</b> {{to reduce}} the {{complexity}} of the higher order controller, based on the minimization of integral square error (ISE) and Dominant Pole Retention method pertaining to unit step input is presented in this paper. In this order-reduction technique, dominant pole of the higher order plant is retained and reduced order model of the plant is obtained using ISE Minimization Technique. Using this reduced order plant a reduced order controller is obtained. The method has built in stability – preserving feature...|$|E
40|$|The Eliza's dream (Il sogno di Eliza) {{is a small}} project {{about the}} {{practical}} use of generative or <b>computer</b> <b>based</b> <b>methods</b> applied on story writing. During this project the author, Motor (a. k. a Angelo Comino) explored all kind of help and interaction with the digital environment in order to write a common, but hopefully pleasant, mistery book, trough generative software,dialogues with classic artificial intelligence, with automated brainstormer or story telling expert systems...|$|R
40|$|The {{continuous}} {{expansion of}} coal mining in the Central Queensland Region is requiring more rail services to transport coal from the mine to port. Rail services have been expanding {{to meet this}} need with more rail routes, duplication of rail lines, increased number of trains and longer trains. When the length of trains are increased {{there is a need}} to review that these longer trains can be driven satisfactorily on the current rail lines. The driving strategies of longer trains need to consider the increased mass and length of the trains to ensure the coupling forces within the trains are within reasonable levels. Driving strategies have to consider multiple parameters such as operating the train safely, efficiently in terms of energy, in a timely manner and with minimal wear and damage to coupling components. This paper presents the use of <b>computer</b> <b>based</b> <b>methods</b> to determine optimal driving strategies for long trains. A number of <b>computer</b> <b>based</b> <b>methods</b> already exist but tend to be based around energy efficiency methods. Energy efficiency methods fail to completely consider the longitudinal coupling forces that are generated within the train. Research at the Centre for Railway Engineering, CQUniversity is gradually investigating the use of a <b>computer</b> <b>based</b> searching <b>method</b> <b>based</b> on ‘Genetic Algorithms’ to determine optimal driving strategies for long trains considering energy, time and longitudinal forces. Initial results have shown energy optimisation is possible by using Genetic Algorithms and a similar method is being developed to optimise for both energy and in-train forces...|$|R
40|$|Background: <b>Computer</b> <b>based</b> <b>methods</b> are {{increasingly}} being used for training workers, although {{our understanding of how}} to structure this training has not kept pace with the changing abilities of computers. Information on a computer can be presented in many different ways and the style of presentation can greatly affect learning outcomes and the effectiveness of the learning intervention. Many questions about how adults learn from different types of presentations and which methods best support learning remain unanswered...|$|R
40|$|Abstract — A <b>computer</b> <b>based</b> <b>method</b> {{to reduce}} the {{complexity}} of the higher order controller, based on the minimization of integral square error (ISE) and Dominant Pole Retention method pertaining to unit step input is presented in this paper. In this order-reduction technique, dominant pole of the higher order plant is retained and reduced order model of the plant is obtained using ISE Minimization Technique. Using this reduced order plant a reduced order controller is obtained. The method has built in stability – preserving feature. Keywords- Controller Design, Dominant Pole, ISE I...|$|E
40|$|A <b>computer</b> <b>based</b> <b>method</b> for an {{integrated}} quality management and process {{control in the}} ceramics industry is presented. Methods used for process and quality optimization include the Shainin method, random balance experimantation, full factorial design, fractional factorial design, orthogonal arrays, central composite design, evolutionary operations, simplex evolutionary operation, genetic algorithms and experiments with mixtures. The developed integrated method has been tested and optimized. The appendix contains the manual of the software program ADOKAV for storage, documentation and analysis of experimental data. (WEN) SIGLEAvailable from TIB Hannover: F 95 B 1000 +a / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekBundesministerium fuer Forschung und Technologie (BMFT), Bonn (Germany) DEGerman...|$|E
40|$|Decision to revascularize {{a patient}} with stable {{coronary}} artery disease {{should be based on}} the detection of myocardial ischemia. If this decision can be straightforward with significant stenosis or in non-significant stenosis, the decision with intermediate stenosis is far more difficult and require invasive measures of functional impact of coronary stenosis on maximal blood (flow fractional flow reserve=FFR). A recent <b>computer</b> <b>based</b> <b>method</b> has been developed and is able to measure FFR with data acquired during a standard coronary CT-scan (FFRcT). Two recent clinical studies (DeFACTO and DISCOVER-FLOW) show that diagnostic performance of FFRcT was associated with improved diagnostic accuracy versus standard coronary CT-scan for the detection of myocardial ischemia although FFRcT need further development...|$|E
40|$|Investigation of genes, {{using data}} {{analysis}} and <b>computer</b> <b>based</b> <b>methods,</b> has gained widespread attention in solving human cancer classification problem. DNA microarray gene expression datasets are readily utilized for this purpose. In this paper, we propose a feature selection method using improved regularized {{linear discriminant analysis}} technique to select important genes, crucial for human cancer classification problem. The experiment is conducted on several DNA microarray gene expression datasets and promising results are obtained when compared with several other existing feature selection methods...|$|R
40|$|Concrete is {{the main}} {{constituent}} material in many structures. The behavior of concrete is nonlinear and complex. Increasing use of <b>computer</b> <b>based</b> <b>methods</b> for designing and simulation have also increased the urge for the exact solution of the problems. This leads to difficulties in simulation and modeling of concrete structures. A good approach {{is to use the}} general purpose finite element software ABAQUS. In this paper a 3 D model of a concrete cube is prepared using smeared crack model and concrete damage plasticity approach. The validation of the model to the desired behavior under monotonic loading is then discussed...|$|R
40|$|Speaking about "computer {{tools for}} editions" most people speak about {{of the more}} refined {{varieties}} of typesetting systems. On the contrary it can be argued, that the availability of <b>computer</b> <b>based</b> <b>methods</b> for the presentation of huge data objects, could reopen a discussion of what an "edition" actually is. An introduction into such a redefinition is given: an "edition" being defined as a complex of interlinking representations of different degrees of abstraction from the original form of the text. The way in which scanned manuscript, transcribed text and formalized representations {{of knowledge about the}} text interrelate, are described...|$|R
40|$|The efficiency, {{accuracy}} and usability of a multiaccess key, a paper based dichotomous key {{and a computer}} based {{version of the same}} were compared. All three identification aids covered the same group of woodlice, using comparable information. The keys were evaluated by first year Biology undergraduates at Sheffield University. Students using a <b>computer</b> <b>based</b> <b>method</b> were slower to reach an identification than those using the paper key. The number of correct identifications was similar for the three keys. Students were more confident of identifications made using the hypertext key but this identification was slightly more likely to be wrong than those made using the other two keys. Students found the computer based keys easier to use, but suggested a number of improvements to them...|$|E
40|$|Two {{methods of}} long- term {{epidemiological}} follow- up were compared by using each {{to study the}} survival of 1622 myocardial infarction patients registered by the Belfast MONICA Project. Length of follow- up ranged between 3 and 5 years during which time 277 deaths were recorded A <b>computer</b> <b>based</b> <b>method</b> for linking MONICA Project registration records with the Registrar General's death certification data identified 273 of the 277 deaths. Follow- up supplied by the Northem Ireland Central Services Agency through the flagging ofpatients in their master patient index identified 271 deaths; four of the sLx deaths which were missed occurred before computerisation of the index was complete. The study illustrates the value of computer- based linkage with death certification data and of flagging in the Central Services Agency master patient index...|$|E
40|$|The {{purpose of}} this study was to clarify the {{strengths}} and weaknesses of both computer based and paper–pencil methods of sensitive data collection in various administrative settings. A total of 180 students signed-up for optional course-credit and were randomly assigned to complete a questionnaire using either a paper–pencil, or a <b>computer</b> <b>based</b> <b>method,</b> and in one of the following administration settings: group, alone in a designated office, or being emailed/mailed the information and filling it out when/where they pleased. Results show that perceptions of anonymity and confidentiality were strongly correlated with self-reported accuracy. However, although perceptions of anonymity and confidentiality differed by condition, this had no statistically significant impact on responses. Though perceptions of anonymity and confidentiality differ between methods, neither method appears to be superior in the context of overall response bias...|$|E
40|$|Proprietary and {{syndicate}} {{surveys are}} often used in assessing appeal and initial quality of new vehicles for automobile manufactures. This study discusses {{the difference between the}} two types of studies, and proposes a <b>computer</b> simulation <b>based</b> <b>method</b> for checking the appropriateness of the comparisons...|$|R
40|$|The {{timetabling}} problem {{comes up}} {{every year in}} educational institutions, which has been solved by leveraging human resource for a long time. The problem is a special version of the optimization problems, it is computationally NP-hard. Although, there are some attempts to apply <b>computer</b> <b>based</b> <b>methods,</b> their use {{is limited by the}} problem's complexity, therefore Genetic Algorithms were applied, because they are robust enough in such a huge problem space. In this paper a new and more flexible timetable representation, the set representation is introduced which meets the demands better than former ones. The proposed method proved to be efficient in real life application of a secondary school, as well. I...|$|R
40|$|The {{field of}} {{medicine}} requires {{a rich and}} diverse use of technical information. <b>Computer</b> <b>based</b> <b>methods</b> of information access have become common. The use of traditional hierarchical and linear software systems are not efficient enough for the flexible access of information. The network structure of hypertext documents allow for a natural representation of information. Our work concentrates {{on the issue of}} navigation in a hypertext information space. The complexity and quantity of medical knowledge means that a user may easily become disoriented when browsing through its hypertext implementation. The weakly structured (non hierarchical) nature of hypertext allows users freedom to explore but such an organization can result in the users becoming lost in the space they are exploring...|$|R
40|$|An {{autonomous}} <b>computer</b> <b>based</b> <b>method</b> {{and system}} is described for personalized production of videos such as team sport videos such as basketball videos from multi-sensored data under limited display resolution. Embodiments {{of the present}} invention relate to {{the selection of a}} view to display from among the multiple video streams captured by the camera network. Technical solutions are provided to provide perceptual comfort as well as an efficient integration of contextual information, which is implemented, for example, by smoothing generated viewpoint/camera sequences to alleviate flickering visual artefacts and discontinuous story-telling artefacts. A design and implementation of the viewpoint selection process is disclosed that has been verified by experiments, which shows that the method and system of the present invention efficiently distribute the processing load across cameras, and effectively selects viewpoints that cover the team action at hand while avoiding major perceptual artefacts...|$|E
40|$|In this article, {{which is}} based on the first part of my PhD thesis, I review the {{statistics}} of the open string sector in T^ 6 /(Z_ 2 xZ_ 2) orientifold compactifications of the type IIA string. After an introduction to the orientifold setup, I discuss the two different techniques that have been developed to analyse the gauge sector statistics, using either a saddle point approximation or a direct <b>computer</b> <b>based</b> <b>method.</b> The two approaches are explained and compared by means of eight- and six-dimensional toy models. In the four-dimensional case the results are presented in detail. Special emphasis is put on models containing phenomenologically interesting gauge groups and chiral matter, in particular those containing a standard model or SU(5) part. Comment: 51 pages, 29 figures; v 2 : ref. added, version to appear in Fortsch. Phys; v 3 : ref. adde...|$|E
40|$|In this presentation, a <b>computer</b> <b>based</b> <b>method</b> {{which uses}} a set of {{algebraic}} equations and statistical data, were used to compute concrete mixes for prescribeable elastic concrete modulus, and vice versa. The computer programs based on Simplex and Regression theories {{can be used to}} predict several mix proportions for obtaining a desired modulus of elasticity of concrete made from crushed granite rock and other materials. The modulus of elasticity of concrete predicted by these programs agreed with experimentally obtained values. The programs are easy and inexpensive to use, and give instant and accurate results. For example, if the modulus of elasticity is specified as input, the computer instantly prints out all possible concrete mix ratios that can yield concrete having the specified elastic modulus. When the concrete mix ratio is specified as input, the computer quickly prints out the elastic modulus of the concrete obtainable from a given concrete mix ratio...|$|E
40|$|The {{ability of}} an {{unmanned}} system to visually determine the direction a radar antenna is facing can significantly improve the SEI collection process and {{provide opportunities for}} autonomous collection. I will investigate <b>computer</b> vision <b>based</b> <b>methods</b> for detecting and analyzing periodic motion of spinning radar in the maritime domain. 1...|$|R
40|$|Wearable {{computing}} and Augmented Reality applications {{call for}} less obtrusive and more intuitive human computer interfaces than keyboards and mice. One way to realize such interfaces is using gestures, e. g., for pointing {{in order to}} replace the mouse. The less obtrusive way of gesture recognition is to use <b>computer</b> vision <b>based</b> <b>methods...</b>|$|R
5000|$|Box {{counting}} is {{a method}} of gathering data for analyzing complex patterns by breaking a dataset, object, image, etc. into smaller and smaller pieces, typically [...] "box"-shaped, and analyzing the pieces at each smaller scale. The essence of {{the process has been}} compared to zooming in or out using optical or <b>computer</b> <b>based</b> <b>methods</b> to examine how observations of detail change with scale. In box counting, however, rather than changing the magnification or resolution of a lens, the investigator changes the size of the element used to inspect the object or pattern (see Figure 1). <b>Computer</b> <b>based</b> box counting algorithms have been applied to patterns in 1-, 2-, and 3-dimensional spaces. The technique is usually implemented in software for use on patterns extracted from digital media, although the fundamental method can be used to investigate some patterns physically. The technique arose out of and is used in fractal analysis. It also has application in related fields such as lacunarity and multifractal analysis.|$|R
40|$|Non {{invasive}} imaging of retina offers easy {{evaluation and}} early diagnosis of various systemic diseases like diabetic retinopathy, arteriosclerosis, hypertension and {{retinopathy of prematurity}} (ROP). Treatment of ROP, {{one of the leading}} causes of childhood blindness is warranted based on the diagnosis of Plus disease which is characterized by high degree of tortuosity and large vessel width for blood vessels in the retina. Here we present a semi-automated <b>computer</b> <b>based</b> <b>method</b> for the screening of Plus disease. This method uses multiscale Gabor filters for the detection of retinal blood vessels and for obtaining their orientations at each pixel. Morphological image processing followed by Gabor filtering helps in obtaining the final vessel map of the images. A 50 image database obtained from KIDROP Bangalore, India were classified as either plus, pre-plus or normal case by quantitatively measuring the amount of tortuosity and width from the vessel map of each images. An overall sensitivity of 1. 0 and specificity 0. 92 obtained for these images...|$|E
40|$|Objective/Hypothesis: Little {{research}} has explored {{the potential of}} computer assisted decision making applied to high-speed videoendoscopy. In this paper, we propose a <b>computer</b> <b>based</b> <b>method</b> for differentiating normal and pathological larynges {{on the basis of}} HSV. Methods: HSV recordings were collected from 101 patients with normal larynges, leukoplakia, nodules or polyps. After pre-processing, samples were analyzed for the number of glottal regions present during the open phase, the symmetry of the glottal area, the convex nature of the vocal folds and the ratio of the minimal to maximal glottal area. A decision tree based method with support vector machines at the tree nodes was used to separate samples. Results: Normal samples were differentiated from pathological samples with a sensitivity of 91. 1 % and a specificity of 81. 8 %. When samples were divided into normal, nodule, polyp and leukoplakia groups, samples were correctly separated 70. 3 % of the time. Conclusions: The combination of SVM and decision tree improves the differentiating capabilities of the parameters employed. While our approach was successful in separating normal from abnormal samples, the classification of unique pathologies requires the development stronger individual parameters...|$|E
40|$|Purpose To {{develop a}} <b>computer</b> <b>based</b> <b>method</b> for the {{automated}} assessment of image {{quality in the}} context of diabetic retinopathy (DR) to guide the photographer. Methods A deep learning framework was trained to grade the images automatically. A large representative set of 7000 color fundus images were used for the experiment which were obtained from the EyePACS that were made available by the California Healthcare Foundation. Three retinal image analysis experts were employed to categorize these images into Accept and Reject classes based on the precise definition of image quality {{in the context of}} DR. A deep learning framework was trained using 3428 images. Results A total of 3572 images were used for the evaluation of the proposed method. The method shows an accuracy of 100 % to successfully categorise Accept and Reject images. Conclusion Image quality is an essential prerequisite for the grading of DR. In this paper we have proposed a deep learning based automated image quality assessment method in the context of DR. The method can be easily incorporated with the fundus image capturing system and thus can guide the photographer whether a recapture is necessary or not. Comment: 23 pages, 9 figure...|$|E
40|$|Full text systems seem {{often to}} be the {{cheapest}} way of introducing <b>computer</b> <b>based</b> <b>methods</b> into historical research, as, {{at least at first}} glance, the almost completely abolish the necessity for coding. It is quite frequently discovered, however, that this easy way of starting a project has to be paid for later, when the uncoded natural language makes it difficult to base results upon broad and well defined categories. Research is described which foucuses upon the introduction of formalized approaches, borrowed from linguistics. Such approaches could ultimately make the plain text, transcribed from a source, much more useful. The emphasis is put upon a concise introduction of the linguistic concepts necessary. These goals are accomplished by defining the classes of knowledge a computing environment needs to process medieval texts, as occuring in charters with a minimum of explicite coding provided...|$|R
40|$|In this review, I will {{describe}} {{the combination of}} pseudopotentials and density functional theory to determine the electronic structure of matter. This combination called the pseudopotential-density functional method (PDFM) represents the most popular technique for examining {{a wide range of}} structural and electronic properties. I will will illustrate applications of the PDFM to problems of current interest: nanostructures and other complex confined systems. 1. Introduction A central goal of materials physics has been the determination of properties using only information about the constituent species. The realization of this goal would allow scientists to predict the existence and properties of new solids or liquids not previously realized in nature, and the possibility of developing <b>computer</b> <b>based</b> <b>methods</b> for producing solids with useful properties such as high temperature alloys and superconductors, superhard matter, low dielectric materials and so on[1]. Achievements of this ki [...] ...|$|R
40|$|The City of Palo Alto, California, has {{developed}} methods {{to ensure that}} data delivered by its conversion vendor meet both contractual requirements and user needs. The complexity of the utility infrastructure systems and a high accuracy standard required {{the development of a}} comprehensive quality control (QC) program integrated with the conversion process. This paper describes the basemapping and conversion process, proposes a typology of errors, and presents manually- and computer-based QC methods to identify errors. <b>Computer</b> <b>based</b> <b>methods</b> include automated checks, computerassisted checks, and a red-lining process. INTRODUCTION Why do so many conversion projects fail? One contributing factor is that the conversion process contains many opportunities to add error beyond that which is inherent in source documents. Quality control procedures are an essential, yet often underutilized way to identify and correct errors introduced by the conversion process. A well designed quality control [...] ...|$|R
