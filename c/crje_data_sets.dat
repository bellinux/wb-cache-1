0|10000|Public
40|$|Results {{from using}} {{different}} breast cancer <b>data</b> <b>set</b> as the training <b>data</b> <b>set</b> in the MCL+superpc approach In order {{to check the}} robustness of our MCL+superpc approach, we used each of four validation <b>data</b> <b>sets</b> as the training <b>data</b> <b>set,</b> and the remaining four <b>data</b> <b>sets</b> as validation <b>data</b> <b>sets.</b> The following tables show these results. Table S 1. Superpc continuous prediction results from breast cancer data analysis. The results were generated by using the GSE 4922 <b>data</b> <b>set</b> as the training <b>data</b> <b>set</b> and four independent <b>data</b> <b>sets</b> as validation <b>data</b> with a threshold value of 1. 10 and 9 selected MCL modules. The training <b>data</b> <b>set</b> is highlighted in red. P-values less than 0. 05 are highlighted in yellow...|$|R
30|$|<b>Data</b> <b>set</b> feature {{processing}} scheme includes feature collection, feature {{conversion and}} reservation. The filter feature of <b>data</b> <b>set</b> would be pre-fetched. The classification characteristics of <b>data</b> <b>set</b> {{could be obtained}} based on the classification accuracy of <b>data</b> <b>set.</b> According to {{the characteristics of the}} <b>data</b> <b>set,</b> the crowd filter would select the appropriate crowd incentive strategy. Based on the complexity characteristics of the <b>data</b> <b>set</b> classification, the transformation of the subset of the <b>data</b> <b>set</b> would be completed.|$|R
30|$|<b>Data</b> <b>set</b> 4 : Reduction, transformation, and {{clustering}} <b>data</b> <b>sets</b> (14 <b>data</b> <b>sets,</b> 2 variables).|$|R
3000|$|... (a) to {{represent}} target <b>data</b> <b>set</b> and auxiliary <b>data</b> <b>set,</b> respectively. Denote the feature matrix of target <b>data</b> <b>set</b> as X^(t)∈R^k× n^(t), the feature matrix of spectral information of auxiliary <b>data</b> <b>set</b> as X^(a)∈R^k× n^(a), and the texture feature information matrix in auxiliary <b>data</b> <b>set</b> as T^(a)∈R^m× n^(a). For target <b>data</b> <b>set,</b> {{we assume that}} each sample corresponds to particular auxiliary information. We use S [...]...|$|R
40|$|A pre-coding {{method and}} device for {{improving}} data compression performance by removing correlation between a first original <b>data</b> <b>set</b> {{and a second}} original <b>data</b> <b>set,</b> each having M members, respectively. The pre-coding method produces a compression-efficiency-enhancing double-difference <b>data</b> <b>set.</b> The method and device produce a double-difference <b>data</b> <b>set,</b> i. e., an adjacent-delta calculation performed on a cross-delta <b>data</b> <b>set</b> or a cross-delta calculation performed on two adjacent-delta <b>data</b> <b>sets,</b> from either one of (1) two adjacent spectral bands coming from two discrete sources, respectively, or (2) two time-shifted <b>data</b> <b>sets</b> coming from a single source. The resulting double-difference <b>data</b> <b>set</b> is then coded using either a distortionless data encoding scheme (entropy encoding) or a lossy data compression scheme. Also, a post-decoding method and device for recovering a second original <b>data</b> <b>set</b> having been represented by such a double-difference <b>data</b> <b>set...</b>|$|R
40|$|Several <b>data</b> <b>sets</b> {{have been}} {{proposed}} for benchmarking in time series prediction. A popular one is <b>Data</b> <b>Set</b> A from the Santa Fe Competition. This <b>data</b> <b>set</b> {{was the subject of}} analysis in many papers. In this note, it is shown that predicting the continuation of <b>Data</b> <b>Set</b> A is nothing else than a pattern matching problem. Looking at studies of this <b>data</b> <b>set,</b> it is remarkable that most of the very good forecasts of <b>Data</b> <b>Set</b> A used upsampled training data. We explain why upsampling is crucial for this <b>data</b> <b>set.</b> Finally, it is demonstrated that simple pattern matching performs as good as sophisticated prediction methods on <b>Data</b> <b>Set</b> A...|$|R
40|$|This paper {{describes}} a prototype system for registering geologic <b>data</b> <b>sets</b> through ontologies {{to assist in}} integrating and querying heterogeneous geologic <b>data</b> <b>sets.</b> The system consists of three components: an ontology repository, the <b>data</b> <b>set</b> registration, and ontology-aware applications. User-defined ontologies in OWL are saved and used by the system. Each <b>data</b> <b>set</b> must be registered before it becomes available, and the registration semi-automatically generates a mapping from <b>data</b> <b>sets</b> to ontologies. The mapping between <b>data</b> <b>sets</b> and ontologies are used by applications to explore and extract information from the <b>data</b> <b>set...</b>|$|R
30|$|The {{experiments}} adopts {{the classic}} <b>data</b> <b>set</b> DataSet 1 provided by KDD Cup’ 99 {{to test the}} correctness of the proposed parallel spectral clustering algorithm; we use respectively 10000 (<b>Data</b> <b>Set</b> DS 1), 50000 (<b>Data</b> <b>Set</b> DS 2), 100000 (<b>Data</b> <b>Set</b> DS 3), 1000000 (<b>Data</b> <b>Set</b> DS 4), 5000000 (<b>Data</b> <b>Set</b> DS 5) to verify {{the superiority of the}} proposed parallel algorithm, and data samples is the multidimensional data listed in literature [20, 21].|$|R
50|$|Wylbur {{provides}} a line editor {{that works with}} temporary <b>data</b> <b>sets,</b> similar to buffers in other editors. At any point in time one of the temporary <b>data</b> <b>sets</b> is designated as default. Wylbur maintains a current line pointer for each temporary <b>data</b> <b>set.</b> The user may specify an explicit working <b>data</b> <b>set</b> on a command; if he omits it, then the default temporary <b>data</b> <b>set</b> is used as the working <b>data</b> <b>set.</b>|$|R
30|$|To {{evaluate}} our approach, we use two <b>data</b> <b>sets,</b> the TEE 2014 <b>data</b> <b>set</b> and the CLEF Replab 2014 <b>data</b> <b>set.</b>|$|R
40|$|The <b>data</b> <b>set</b> {{specifications}} for the NASA Aerospace Safety Information System (NASIS) are presented. The <b>data</b> <b>set</b> specifications describe the content, format, and medium of communication of every <b>data</b> <b>set</b> {{required by the}} system. All relevant information pertinent to a particular <b>data</b> <b>set</b> is prepared in a standard form and centralized in a single document. The format for the <b>data</b> <b>set</b> is provided...|$|R
40|$|Abstract Background The {{information}} from different <b>data</b> <b>sets</b> experimented under different conditions may be inconsistent {{even though they}} are performed with the same research objectives. More than that, even when the <b>data</b> <b>sets</b> were generated from the same platform, the data agreement may be affected by the technical variation among the laboratories. In this case, it is necessary to use the combined <b>data</b> <b>set</b> after adjusting the differences between such <b>data</b> <b>sets,</b> for detecting the more reliable information. Results The proposed method combines <b>data</b> <b>sets</b> posterior to the discretization of <b>data</b> <b>sets</b> based on the ranks of the gene expression ratios, and the statistical method is applied to the combined <b>data</b> <b>set</b> for predictive gene selection. The efficiency of the proposed method was evaluated using five colon cancer related <b>data</b> <b>sets,</b> which were experimented using cDNA microarrays with different RNA sources, and one experiment utilized oligonucleotide arrays. NCI- 60 cell lines <b>data</b> <b>sets</b> were used, which were performed with two different platforms of cDNA microarrays and Affymetrix HU 6800 oligonucleotide arrays. The combined <b>data</b> <b>set</b> by the proposed method predicted the test <b>data</b> <b>sets</b> more accurately than the separated <b>data</b> <b>sets</b> did. The biological significant genes were detected from the combined <b>data</b> <b>set,</b> which were missed on the separated <b>data</b> <b>sets.</b> Conclusion By transforming gene expressions using ranks, the proposed method is not influenced by systematic bias among chips and normalization method. The method may be especially more useful to find predictive genes from <b>data</b> <b>sets</b> which have different scale in gene expressions. </p...|$|R
50|$|Documents and <b>data</b> <b>sets</b> sectionAnother {{section of}} the Facility Information Model {{consists}} of documents and <b>data</b> <b>sets</b> in various formats. Each of those documents and <b>data</b> <b>sets</b> {{is related to the}} element in the facility model about which the document or <b>data</b> <b>set</b> contains information.|$|R
30|$|Hypotheses {{related to}} {{operating}} core (innovation process, cross-functional organisation, {{and implementation of}} tools/technology) and competition-informed pricing The result of H 4 is supported in the full <b>data</b> <b>set</b> (β =  0.170, p <  0.05) and the Malaysian <b>data</b> <b>set</b> (β =  0.255, p <  0.05), while in the Bangladeshi data it was not supported. The result of H 5 was supported in the full <b>data</b> <b>set</b> (β =  0.266, p <  0.05) and the Bangladeshi <b>data</b> <b>set</b> (β =  0.275, p <  0.05), while in the Malaysian <b>data</b> <b>set</b> it was not supported. The result of H 6 was supported in the full <b>data</b> <b>set</b> (β =  0.295, p <  0.01) and the Bangladeshi <b>data</b> <b>set</b> (β =  0.536, p <  0.01), while in the Malaysian <b>data</b> <b>set,</b> H 6 was not supported.|$|R
50|$|In the {{geospatial}} (GIS) domain, {{data fusion}} is often synonymous with data integration. In these applications, {{there is often}} a need to combine diverse <b>data</b> <b>sets</b> into a unified (fused) <b>data</b> <b>set</b> which includes all of the data points and time steps from the input <b>data</b> <b>sets.</b> The fused <b>data</b> <b>set</b> is different from a simple combined superset in that the points in the fused <b>data</b> <b>set</b> contain attributes and metadata which might not have been included for these points in the original <b>data</b> <b>set.</b>|$|R
40|$|This paper {{describes}} {{how to build}} an audit and tracking system using SAS/AF and SCL. There will be a master <b>data</b> <b>set</b> which contains the original information and several transaction <b>data</b> <b>sets</b> which contain the new information. The audit system will take information from the transaction <b>data</b> <b>sets</b> and apply changes to the master <b>data</b> <b>set.</b> An audit <b>data</b> <b>set</b> {{will be used to}} keep track of all changes. The tracking system will display the master <b>data</b> <b>set</b> in the first data table on {{the top half of the}} screen and the audit <b>data</b> <b>set</b> in the second data table on the bottom half...|$|R
40|$|Abstract. This paper {{reviews the}} {{appropriateness}} for application to large <b>data</b> <b>sets</b> of standard machine learning algorithms, which were mainly {{developed in the}} context of small <b>data</b> <b>sets.</b> Sampling and parallelisation have proved useful means for reducing computation time when learning from large <b>data</b> <b>sets.</b> However, such methods assume that algorithms that were designed for use with what are now considered small <b>data</b> <b>sets</b> are also fundamentally suitable for large <b>data</b> <b>sets.</b> It is plausible that optimal learning from large <b>data</b> <b>sets</b> requires a different type of algorithm to optimal learning from small <b>data</b> <b>sets.</b> This paper investigates one respect in which <b>data</b> <b>set</b> size may affect the requirements of a learning algorithm – the bias plus variance decomposition of classification error. Experiments show that learning from large <b>data</b> <b>sets</b> may be more effective when using an algorithm that places greater emphasis on bias management, rather than variance management. ...|$|R
40|$|Geographic <b>data</b> <b>set</b> {{integration}} {{is particularly important}} for update propagation, i. e. the reuse of updates from one <b>data</b> <b>set</b> in another <b>data</b> <b>set.</b> In this thesis geographic <b>data</b> <b>set</b> integration (also known as map integration) between two topographic <b>data</b> <b>sets,</b> GBKN and TOP 10 vector, is described. GBKN is a large-scale topographic <b>data</b> <b>set</b> and TOP 10 vector is a medium-scale topographic <b>data</b> <b>set.</b> Geographic <b>data</b> <b>set</b> integration (or map integration) is defined as ‘the process of establishing links between corresponding object instances in different, autonomously produced, geographic <b>data</b> <b>sets</b> of the same geographic space’. Corresponding object instances are semantically similar. Semantically similar means that corresponding object instances refer to the same terrain situation. In {{the first part of}} this thesis a general introduction to geographic <b>data</b> <b>set</b> {{integration is}} given. Relevant literature is reviewed. In the second part a conceptual framework for geographic <b>data</b> <b>set</b> integration is developed. Two important components of this framework are a domain ontology and a set of surveying rules. A domain ontology is important because it contains a set of shared concepts. It is this set of shared concepts of terrain situations that makes it possible to detect corresponding object instances...|$|R
40|$|A {{large number}} of <b>Data</b> <b>Sets</b> are available, but they are {{incomplete}} in nature so that they cannot be used for real applications. These incomplete <b>data</b> <b>sets</b> are produced due to various reasons like system failure, privacy of data, incomplete input, time delay in system, lack of available resources. As common examples Weather <b>data</b> <b>sets,</b> sensor image <b>data</b> <b>sets,</b> Measurement Reflection in Agriculture <b>Data</b> <b>Sets.</b> For these cases we use conceptual data reconstruction by using statistical models of multiple linear Regressions to Predict Missing values in <b>Data</b> <b>Sets.</b> </p...|$|R
30|$|A BYEC {{classifier}} {{composed of}} 25 SVM subclassifiers was trained by 70 % {{of the original}} NEU defect <b>data</b> <b>set,</b> and the remaining 30 % of the data were {{used to evaluate the}} accuracy of the BYEC classifier on the original <b>data</b> <b>set.</b> Then, we randomly sampled 10 % of the processed <b>data</b> <b>sets</b> to adjust our BYEC classifier. Finally, the accuracy of the adjusted BYEC classifier and the original classifier on the processed <b>data</b> <b>set</b> were tested by the remaining 90 % of the processed data. As for the BYEC classifier, 70 % of the original <b>data</b> <b>set</b> were used to train the KNN, BPNN, and SVM classifier and 30 % were used to evaluate the accuracy on the original <b>data</b> <b>set,</b> then the accuracy of these classifiers on the processed <b>data</b> <b>set</b> was tested by 90 % of the processed <b>data</b> <b>set.</b> The BPNN and KNN parameters were determined by cross-validation testing. The average accuracy of the classifiers on every <b>data</b> <b>set</b> was run 100 times, and the <b>data</b> <b>sets</b> were sampled individually.|$|R
40|$|Secondary {{analysis}} {{provides a}} useful method {{for the development of}} new knowledge. Larger samples can be constructed, and secondary analysis can be enhanced when <b>data</b> <b>sets</b> are com-bined. A standardized method for combining large <b>data</b> <b>sets</b> is crucial, yet literature on methods for combining large <b>data</b> <b>sets</b> for secondary analysis is lacking. The {{purpose of this article is}} to outline and explain the process of combining two or more large <b>data</b> <b>sets</b> (n = 276, n = 125) for secondary analysis by using these authors’previous work with large oncology and AIDS care-giver <b>data</b> <b>sets.</b> A single <b>data</b> <b>set</b> is commonly used for secondary analysis. Often, a pur-pose of secondary analysis is to examine a subset of the larger population studied (e. g. minorities,male caregivers). Unfortunately, a subsample of one <b>data</b> <b>set</b> is frequently inadequate due to the small number of participants. Larger samples can be constructed, and secondary analysis can be enhanced when <b>data</b> <b>sets</b> are combined. A standardized method for combining large <b>data</b> <b>sets</b> is crucial, yet literature onmethods for combining large <b>data</b> <b>sets</b> fo...|$|R
40|$|A {{characterisation}} {{of initial}} <b>data</b> <b>sets</b> for the Schwarzschild spacetime is provided. This characterisation is obtained by performing a 3 + 1 decomposition {{of a certain}} invariant characterisation of the Schwarzschild spacetime given in terms of concomitants of the Weyl tensor. This procedure renders a set of necessary conditions [...] which can be written {{in terms of the}} electric and magnetic parts of the Weyl tensor and their concomitants [...] for an initial <b>data</b> <b>set</b> to be a Schwarzschild initial <b>data</b> <b>set.</b> Our approach also provides a formula for a static Killing initial <b>data</b> <b>set</b> candidate [...] a KID candidate. Sufficient conditions for an initial <b>data</b> <b>set</b> to be a Schwarzschild initial <b>data</b> <b>set</b> are obtained by supplementing the necessary conditions with the requirement that the initial <b>data</b> <b>set</b> possesses a stationary Killing initial <b>data</b> <b>set</b> of the form given by our KID candidate. Thus, we obtain an algorithmic procedure of checking whether a given initial <b>data</b> <b>set</b> is Schwarzschildean or not. Comment: 16 page...|$|R
40|$|Modern machine {{learning}} {{systems such as}} image classifiers rely heavily on large scale <b>data</b> <b>sets</b> for training. Such <b>data</b> <b>sets</b> are costly to create, thus in practice {{a small number of}} freely available, open source <b>data</b> <b>sets</b> are widely used. We suggest that examining the geo-diversity of open <b>data</b> <b>sets</b> is critical before adopting a <b>data</b> <b>set</b> for use cases in the developing world. We analyze two large, publicly available image <b>data</b> <b>sets</b> to assess geo-diversity and find that these <b>data</b> <b>sets</b> appear to exhibit an observable amerocentric and eurocentric representation bias. Further, we analyze classifiers trained on these <b>data</b> <b>sets</b> {{to assess the impact of}} these training distributions and find strong differences in the relative performance on images from different locales. These results emphasize the need to ensure geo-representation when constructing <b>data</b> <b>sets</b> for use in the developing world. Comment: Presented at NIPS 2017 Workshop on Machine Learning for the Developing Worl...|$|R
40|$|The {{comparison}} of two <b>data</b> <b>sets</b> can reveal {{a great deal}} of information about the time-varying nature of an observed process. For example, suppose that the points in a <b>data</b> <b>set</b> represent a customer's activity by their location in n-dimensional space. A {{comparison of}} the distribution of points in two such <b>data</b> <b>sets</b> can indicate how the customer activity has changed between the observation periods. Other applications include data integrity checking. An unexpected change in a <b>data</b> <b>set</b> can indicate a problem in the data collection process. We propose a fast, inexpensive method for comparing massive high dimensional <b>data</b> <b>sets</b> that does not make any distributional assumptions. The method adapts the power of classical statistics for use on complex, high dimensional <b>data</b> <b>sets.</b> We generate a map of the <b>data</b> <b>set</b> (a DataSphere), and compare <b>data</b> <b>sets</b> by comparing their DataSpheres. The DataSphere can be generated in two passes over the <b>data</b> <b>set,</b> stored in a database, and aggregat [...] ...|$|R
40|$|The method {{involves}} manufacturing {{an object}} (1) {{based on the}} binary <b>data</b> <b>set</b> referred to as target <b>data</b> <b>set</b> by the generative manufacturing method. The actual space form of a partial region of the object is detected and the binary <b>data</b> <b>set</b> describing the actual space form of the partial region of the object is generated. The geometric deviation between the space form of the partial region of the object, which {{is provided by the}} target <b>data</b> <b>set</b> and the space form of the partial region of the object, at which the actual <b>data</b> <b>set</b> is generated, is determined. The binary <b>data</b> <b>set</b> provides the space form of the object. The target <b>data</b> <b>set</b> is changed {{on the basis of a}} correcting function, which is derived from the geometric deviations, for obtaining a binary correction <b>data</b> <b>set.</b> The object is generated based on the binary binary correction <b>data</b> <b>set</b> using the generative manufacturing process...|$|R
30|$|A {{few other}} <b>data</b> <b>sets</b> of note are: ISCX [31], MAWI [32], NSA Data Capture [33], and the Internet Storm Center [34] (which also hosts the dshield.org <b>data</b> <b>set).</b> However, these more recent <b>data</b> <b>sets</b> {{are not used}} as {{frequently}} as the DARPA and KDD <b>data</b> <b>sets,</b> even in recent studies.|$|R
30|$|We built three <b>data</b> <b>sets</b> to {{construct}} the DNA duplex stability priors: one positive single-strand binding <b>data</b> <b>set,</b> one positive double-strand binding <b>data</b> <b>set,</b> and one background <b>data</b> <b>set.</b> The positive <b>data</b> <b>sets</b> are constructed from 226 known binding sites in our test <b>data</b> <b>set</b> by splitting the known binding sites into single- and double-strand binding sets according to the binding preference of each TF. The background <b>data</b> <b>set</b> is generated as follows. For each verified binding site in our test set, we randomly select 20 genomic locations (from the same promoter sequence) with the average binding site of length 12, which results in a background set that is 20 {{times larger than the}} test set.|$|R
40|$|Most {{previous}} work on multiple models {{has been done}} on a few domains. We present a comparsion of three ways of learning multiple models on 29 <b>data</b> <b>sets</b> from the UCI repository. The methods are bagging, k-fold partition learning and stochastic search. By using 29 <b>data</b> <b>sets</b> of various kinds [...] artificial <b>data</b> <b>sets,</b> artificial <b>data</b> <b>sets</b> with noise, molecular-biology and real-world noisy <b>data</b> <b>sets</b> [...] we are able to draw robust experimental conclusions about the kinds of <b>data</b> <b>sets</b> for which each learning method works best. We also compare four evidence combination methods (Uniform Voting, Bayesian Combination, Distribution Summation and Likelihood Combination) and characterize the kinds of <b>data</b> <b>sets</b> for which each method works best...|$|R
30|$|In this subsection, we {{describe}} the <b>data</b> <b>sets</b> that we use for our benchmarking. We use <b>data</b> <b>sets</b> {{from a variety of}} different mathematical and scientific areas and applications. In each case, when possible, we use <b>data</b> <b>sets</b> that have already been studied using PH. Our list of <b>data</b> <b>sets</b> is far from complete; we view this list as an initial step towards building a comprehensive collection of benchmarking <b>data</b> <b>sets</b> for PH.|$|R
5000|$|Creating {{partitioned}} output <b>data</b> <b>set</b> from sequential input <b>data</b> <b>set.</b>|$|R
30|$|Table 1 {{shows the}} seeded faults {{and number of}} <b>data</b> <b>sets</b> {{recorded}} for each fault type. The details of each fault are described in [4, 20, 21]. There are total 248 labeled <b>data</b> <b>sets</b> that are recorded for each sensor position. In testing of this technique, the last three faults from Table 1 were used {{as part of the}} unlabeled <b>data</b> <b>set</b> and the rest were part of labeled <b>data</b> <b>set.</b> For testing, the labeled <b>data</b> <b>set</b> for each position of the sensor is divided into different ratios of training and testing <b>data</b> <b>set,</b> as shown in Table 2.|$|R
50|$|In statistics, econometrics, {{and related}} fields, multidimensional {{analysis}} is a data analysis process that groups data into two categories: data dimensions and measurements. For example, a <b>data</b> <b>set</b> {{consisting of the}} number of wins for a single football team at each of several years is a single-dimensional (in this case, longitudinal) <b>data</b> <b>set.</b> A <b>data</b> <b>set</b> consisting {{of the number of}} wins for several football teams in a single year is also a single-dimensional (in this case, cross-sectional) <b>data</b> <b>set.</b> A <b>data</b> <b>set</b> consisting of the number of wins for several football teams over several years is a two-dimensional <b>data</b> <b>set.</b>|$|R
5000|$|A {{simplified}} {{example of}} this process is shown below where <b>data</b> <b>set</b> [...] "α" [...] is fused with <b>data</b> <b>set</b> β to form the fused <b>data</b> <b>set</b> δ. <b>Data</b> points in <b>set</b> [...] "α" [...] have spatial coordinates X and Y and attributes A1 and A2. <b>Data</b> points in <b>set</b> β have spatial coordinates X and Y and attributes B1 and B2. The fused <b>data</b> <b>set</b> contains all points and attributes ...|$|R
30|$|The {{complete}} 20  cm cylinder <b>data</b> <b>set</b> {{consisted of}} 55 scans, 4 {{of which are}} with the TEM material. The complete liter bottle <b>data</b> <b>set</b> contained 17 scans, of which 14 are 1 -L bottle scans, 2 are 2 -L bottle scans, and 1 is a 4 -L bottle scan. The mean conversion factors for the full <b>data</b> <b>set,</b> cylinder <b>data</b> <b>set,</b> and the liter <b>data</b> <b>set</b> only vary by approximately 2 to 3  %, indicating stability.|$|R
40|$|We {{created a}} {{generator}} of pseudo artificial <b>data</b> <b>sets.</b> The generator takes a given <b>data</b> <b>set</b> as input, analyses it {{and creates a}} new <b>data</b> <b>set</b> based on learned properties. A key component of the generator is PRBF algorithm, because its structure is particularly suited for example generation. We tested the generator on multiple real <b>data</b> <b>sets</b> and measured its quality with the <b>data</b> <b>sets</b> properties. In the conclusion we propose possible enhancements to generator's abilities. ...|$|R
5000|$|In many disciplines, {{two-dimensional}} <b>data</b> <b>sets</b> {{are also}} called panel data. While, strictly speaking, two- and higher- dimensional <b>data</b> <b>sets</b> are [...] "multi-dimensional," [...] the term [...] "multidimensional" [...] {{tends to be}} applied only to <b>data</b> <b>sets</b> with three or more dimensions. For example, some forecast <b>data</b> <b>sets</b> provide forecasts for multiple target periods, conducted by multiple forecasters, and made at multiple horizons. The three dimensions provide more information than can be gleaned from two dimensional panel <b>data</b> <b>sets.</b>|$|R
