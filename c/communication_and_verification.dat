15|10000|Public
40|$|In {{these days}} in this global world mobile {{technologies}} are growing very fast and another way world has lots of security problems, that’s why everyone wants to move on biometrics side. Because {{this is the only}} secure system for <b>communication</b> <b>and</b> <b>verification</b> function. So in this paper we are using graphical passwords on the first phase then we are using biometric features like as Face, Iris, and palm, Vein acknowledgement for verification function and safe transaction for the Mobile Banking...|$|E
40|$|The {{aim of this}} baccalaureate {{work was}} assess {{selected}} manager functions - planning, organising, <b>communication</b> <b>and</b> <b>verification,</b> applied in the company TRUST-ts spol. s r. o. and proposition of appropriate general changes to effective improve the managing a business. The theoretical part was devoted to selected managerial functions. In the practical part was briefly introduced the company TRUST-ts spol. s r. o. and then was carried assessment, how the selected manager functions in this company function. Based on the facts found were suggested some changes that could contribute to improvement, and thus to making the entire corporate activity more efficient...|$|E
40|$|Abstract — SoCs {{will soon}} have to {{integrate}} tens of complex system functions, each with their own optimal balance of performance, flexibility, energy consumption, communication, and design time. The traditional model of a (configurable) general-purpose processor core {{with a number of}} hardware accelerators no longer suffices. Application-specific instruction-set processors (ASIPs) can offer the right balance for each system function, and thus form the basis of new generations of multi-core SoCs. This presentation introduces Chess/Checkers, a retargetable tool suite available from Target Compiler Technologies, enabling the design of ASIPs in multicore SoCs. Chess/Checkers offers fast architectural exploration, hardware synthesis, software compilation, inter-ASIP <b>communication,</b> <b>and</b> <b>verification.</b> The tools support a broad range of architectures, from small microprocessors, over DSP dominated cores, to VLIW and vector processors. I...|$|E
30|$|In addition, {{the paper}} {{shows that the}} {{behavioral}} simulations present an expressive reduction of the CPU usage time if compared with Spectre transistor level simulations. Moreover, behavioral simulations are flexible and allow the fast <b>communication</b> network setup <b>and</b> <b>verification</b> if compared with hardware prototypes.|$|R
40|$|In {{this paper}} we present an {{object-oriented}} method for the design, <b>verification</b> <b>and</b> implementation of embedded and distributed real-time systems. The method is called Concurrent Object Net (CON). The CON method {{is based upon}} extended statecharts which use specific message links for <b>communication.</b> For simulation <b>and</b> <b>verification</b> corresponding Petri nets are used. A platform abstraction framework for CON accesses hardware in optimized manner. An anti-slip-control of a rc-car is used to show further CON details...|$|R
40|$|This paper {{analyzes}} and researches {{the theories}} of the encryption communications and identities authentication over the Internet banking based on the PKI technology. Combined with the specific Internet banking system, this research tries to design Internet banking system <b>communication</b> encryption <b>and</b> identities <b>verification</b> <b>and</b> the workflow and main functions. Implementing application to enhance confidentiality of the information transferred by the network communication, this research solves the problems of encrypting Internet banking data and verifying the identities of users and Internet banking. Finally, this paper explores the key technology more in-depth in the designing...|$|R
40|$|An {{aggregate}} signature scheme can aggregate n signatures on n distinct {{messages from}} n distinct signers {{into a single}} signature ． ， ．Thus n verification equations {{can be reduced to}} one So the aggregate signature adapts to Mobile Ad hoc Network (MANET). In this paper, we propose an efficient ID-based aggregate signature scheme with constant pairing computations. Compared with the existing ID-based aggregate signature schemes ， this scheme greatly improves the efficiency of signature <b>communication</b> <b>and</b> <b>verification.</b> In addition, in this work, we apply our ID-based aggregate signature to authenticated routing protocol to present a secure routing scheme. Our scheme not only provides sound authentication and a secure routing protocol in ad hoc networks, but also meets the nature of MANET...|$|E
40|$|AUTomotive Open System ARchitecture (AUTOSAR) is {{a central}} {{standardization}} in the automotive industry which was developed to address the increasing complexity in the automotive embedded development. In this paper, we share results from an interview-based case study in which we explore the requirements engineering process of an AUTOSAR-Tier- 2 supplier. Our results confirm well-known challenges regarding requirements <b>communication,</b> <b>and</b> <b>verification</b> in the cross-organizational requirements engineering process. These challenges are related to those modules that handle complex sensors and actuators, where specific requirements from the car manufacturers are required beyond the AUTOSAR standard requirements. We report that using the AUTOSAR standard brings commodity advantages, however the presence of non-AUTOSAR requirements results in (partly well-known) requirements challenges in the automotive domain. We believe that approaching these challenges from a broad ecosystem perspective will allow future research to address these challenges with tailored requirements engineering processes that facilitate collaboration between all AUTOSAR ecosystem parties...|$|E
40|$|Abstract. We {{propose a}} non-interactive zero {{knowledge}} pairwise multiset sum equality test (PMSET) {{argument in the}} common reference string (CRS) model that allows a prover {{to show that the}} given committed multisets Aj for j ∈ { 1, 2, 3, 4 } satisfy A 1 ⊎ A 2 = A 3 ⊎ A 4, i. e., every element is contained in A 1 and A 2 exactly as many times as in A 3 and A 4. As a corollary to the PMSET argument, we present arguments that enable to efficiently verify the correctness of various (multi) set operations, for example, that one committed set is the intersection or union of two other committed sets. The new arguments have constant <b>communication</b> <b>and</b> <b>verification</b> complexity (in group elements and group operations, respectively), whereas the CRS length and the prover’s computational complexity are both proportional to the cardinality of the (multi) sets. We show that one can shorten the CRS length at the cost of a small increase of the communication and the verifier’s computation...|$|E
40|$|To help programmers of {{high-performance}} computing (HPC) systems avoid communication-related errors, we employ a formal process algebra, Communicating Sequential Processes (CSP), {{which has a}} strict semantics for interprocess <b>communication</b> <b>and</b> synchronization. <b>Verification</b> tools are available for CSP-specified programs to prove the absence of failures such as deadlock, and to explore potential multiprocess interactions. By introducing a CSP abstraction layer {{on top of the}} popular MPI message-passing primitives, we create a framework, called CSP 4 MPI, designed to largely hide the complexity of parallel programming for HPC. CSP 4 MPI is comprised of a C++ class library that provides a CSP-based process model, and a “cookbook ” of candidate solutions for HPC programmers not trained in CSP. Developers can prototype their systems using CSP, <b>and</b> use <b>verification</b> tools to examine possible points of failure before implementing via the CSP 4 MPI library. Alternatively, they may choose an existing, verified solution from a number of common parallel application archetypes. By using CSP 4 MPI, HPC developers leverage the benefits of formal specification <b>and</b> <b>verification</b> in their work, in addition to obtaining an alternate method to developing HPC applications...|$|R
40|$|Automated {{verification}} of distributed programs is a challenging problem. Since {{the behavior of}} a distributed program encompasses the behavior of the network, possible configurations of the network have to be investigated during verification. This leads to very large state spaces, <b>and</b> automated <b>verification</b> becomes infeasible. We present a framework that addresses this problem by decoupling the behavior of distributed programs from the behavior of the network. Our framework is based on a set of stub classes that replace native methods used in network <b>communication</b> <b>and</b> enables <b>verification</b> of distributed Java applications by isolating their behavior from the network. The framework supports two modes of verification: unit <b>verification</b> <b>and</b> integration <b>verification.</b> Integration verification checks multiple interacting distributed application components by running them in a single JVM and simulating the behavior of the network within the same JVM via stub classes. Unit verification targets a single component of a distributed application and requires that the user write an event generator class that utilizes the API exported by the framework. While unit verification only checks a single application component, it benefits from a greatly reduced state space compared do that of integration verification. 1...|$|R
40|$|Bachelor's {{thesis is}} focused on the issue of data {{communications}} over power lines. This technology is known as PLC (Power Line Communication). Wider deployment of this technology into practice at present prevents a number of shortcomings. Main problems are disturbance useful signal on power lines, radiation interference, small reach of useful signal and elements of energy network, which affect the transmission. In the analysis of sub-problems, it appears that it is appropriate to have a computer power line model, allowing a computer simulation of data communication over power line. It is necessary to combine two different, conflicting requirements. Power lines are not primarily intended for data <b>communication,</b> <b>and</b> data lines are not designed for the transmission of electricity. Both types of line are described the same primary parameters, but they operate in completely different modes. This thesis examines these modes and possible models of power and data lines in terms of data communication. The main part is devoted to the draft power line model for data <b>communication</b> <b>and</b> computer <b>verification</b> of the functionality of the new proposed model...|$|R
40|$|Abstract. Rational proofs, {{introduced}} by Azar and Micali (STOC 2012) are {{a variant of}} interactive proofs in which the prover is neither honest nor malicious, but rather rational. The advantage of rational proofs over their classical counterparts is that they allow for extremely low <b>communication</b> <b>and</b> <b>verification</b> time. In recent work, Guo et al. (ITCS 2014) demonstrated their relevance to delegation of computation by showing that, if the rational prover is additionally restricted to being computationally bounded, then every language in NC 1 admits a single-round delegation scheme that can be verified in sublinear time. We extend the Guo et al. result by constructing a single-round delegation scheme with sublinear verification for all languages in P. Our main contribution is the introduction of rational sumcheck protocols, which are a relaxation of classical sumchecks, a crucial building block for interactive proofs. Unlike their classical counterparts, rational sumchecks retain their (rational) soundness properties, even if the polynomial being verified is of high degree (in particular, they do not rely on the Schwartz-Zippel lemma). This enables us to bypass the main efficienc...|$|E
40|$|The {{experiments}} at the AGOR facility require intense {{heavy ion}} beams. Typical examples are 1013 pps of 20 Ne 6 + at 23. 3 MeV/A and ≥ 1012 pps 206 Pb 27 + at 8. 5 MeV/A. To prevent damage to components by the beam (power density up to 1 kW/mm 3 in unfavourable cases) a modular beam loss {{monitoring and control}} system has been developed for the cyclotron and high energy beam lines. The architecture {{of the system is}} described and the considerations for the major design choices discussed. The system uses the CAN-bus for <b>communication</b> <b>and</b> <b>verification</b> of system integrity. The injected beam is chopped at 1 kHz with a variable duty factor between 5 and 90 %. The beam intensity at injection and a number of locations in the high energy beam line is measured by inductive pick-ups. Furthermore, localized beam losses on slits and diaphragms are directly measured. When beam loss in any section exceeds the predefined maximum value, the duty factor of the beam is automatically reduced. IMP;Chinese Academy of Science...|$|E
40|$|Hash {{tables are}} {{fundamental}} data structures that optimally answer membership queries. Suppose a client stores n {{elements in a}} hash table that is outsourced at a remote server. Authenticating the hash table functionality, i. e., verifying the correctness of queries answered by the server and ensuring {{the integrity of the}} stored data, is crucial because the server, lying outside the administrative control of the client, can be malicious. We design efficient and secure protocols for optimally authenticating (non-) membership queries on hash tables, using cryptographic accumulators as our basic security primitive and applying them in a novel hierarchical way over the stored data. We provide the first construction for authenticating a hash table with constant query cost and sublinear update cost, strictly improving upon previous methods. Our first solution, based on the RSA accumulator, allows the server to provide a proof of integrity of the answer to a membership query in constant time and supports updates in O (n ǫ log n) time for any fixed constant 0 < ǫ < 1, yet keeping the <b>communication</b> <b>and</b> <b>verification</b> costs constant. It also lends itself to a scheme that achieves different trade-offs—namely...|$|E
40|$|Formal {{specifications}} are {{now being}} used as a basis for <b>communication,</b> design, testing <b>and</b> <b>verification</b> of a software product. For a formal specification to be used effectively, it must be valid, which means that it must be well-formed and reflects the user requirements. The normal technique for validating a formal specification is by using formal reasoning. However, the use of formal reasoning is extremely tedious and time consuming. In this paper we explore alternative techniques for validating a Z formal specification. In particular, we consider the concept of satisfiability as a weaker alternative to validity and discuss how testing can be used to check the satisfiability of a Z formal specification...|$|R
40|$|Abstract—A quantum {{communication}} scheme for blind signa-ture is proposed based on two-particle entangled quantum system {{to create a}} novel systemetrical quantum cryptosystem. All the messages are encrypted by the private key of the sender Alice during the <b>communication</b> <b>and</b> the authenticity <b>verification</b> of signatures <b>and</b> an arbitrator’s batch efficient proxy signature is applied. It demonstrates {{that a large number}} of blind signatures can be derived with the characteristics: impossibility of forgery, impossibility of disavowal by the signatory and impossibility of denial by the receiver. The security of our scheme depends on the two-particle entangled system which cannot be deterministically intercepted. Index Terms—Quantum communication, Blind signature, Proxy signature, Quantum signature, Quantum cryptography...|$|R
40|$|In {{this paper}} we present an {{object-oriented}} method for the design, <b>verification</b> <b>and</b> implementation of embedded and distributed systems. The method is called Concurrent Object Net (CON). The CON method {{is based upon}} extended statecharts which use specific message links for <b>communication.</b> For simulation <b>and</b> <b>verification</b> corresponding Petri nets are used. A platform abstraction framework for CON accesses hardware in optimized manner. An application example from the automotive domain is used to show further CON details. 1 Introduction The design and implementation of embedded and distributed real-time systems differs hardly from traditional software design known from office applications. Despite all known differences we found object-oriented paradigm suitable for embedded systems as well. But using the object-oriented design techniques for embedded systems means to deal with two contradictory intentions. On one side the object-orientation tries to assist the designer handling huge softw [...] ...|$|R
40|$|Rational proofs, {{recently}} introduced by Azar and Micali (STOC 2012), are {{a variant of}} interactive proofs in which the prover is neither honest nor malicious, but rather rational. The advantage of rational proofs over their classical counterparts is that they allow for extremely low <b>communication</b> <b>and</b> <b>verification</b> time. Azar and Micali demonstrated their potential by giving a one message rational proof for #SAT, in which the verifier runs in time O(n), where n denotes the instance size. In a follow-up work (EC 2013), Azar and Micali proposed “super-efficient ” and interactive versions of rational proofs and argued that they capture precisely the class TC 0 of constant-depth, polynomial-size circuits with threshold gates. In this paper, we show that by considering rational arguments, in which the prover is additionally restricted to be computationally bounded, the class NC 1, of search problems computable by log-space uniform circuits of O(logn) -depth, admits rational protocols that are simultaneously one-round and polylog(n) time verifiable. This demonstrates the potential of rational arguments {{as a way to}} extend the notion of “super-efficient " rational proofs beyond the class TC 0. The low interaction nature of our protocols, along with their sub-linear verification time, make them well suited for delegation of computation. While they provide a weaker (yet arguably meaningful...|$|E
40|$|The authors {{investigated}} {{primary school}} pupils’ experiences using an electronic portfolio in their craft education for a three year period, from the 3 rd grade {{until the end}} of the 5 th grade. This article emphasizes the functions and the benefits of the ePortfolio method and outlines general user experiences based on pupil interviews (N= 38), which recounted user experiences from the start of subject teaching in the 3 rd grade. Data-driven content analysis with a summative approach was used to analyse these interviews. The results indicate that an ePortfolio (realized through the iPad application, Book Creator) is a workable method in craft education. When the use begins during the early school years, it is experienced as a natural part of the work process. The identified key functions were collection and management of information, <b>communication</b> <b>and</b> <b>verification</b> of development. The experienced benefits were related to supporting the working process; activities documented by the ePortfolio appeared to operate as stimuli to memory and elicited rehearsing of concepts in a way that deepened understanding of the past experiences. The ePortfolio method offers a balancing opportunity to regard design and making process in assessment. Despite being based on the pupils’ experiences, the results are relevant and useful for teachers when improving their pedagogical practices...|$|E
40|$|We {{study the}} {{verification}} of set {{operations in the}} model of authenticated data structures, namely the problem of cryptographically checking the correctness of outsourced set operations performed by an untrusted server over a dynamic collection of sets that are owned (and updated) by a trusted source. We present a new authenticated data structure scheme that allows any entity to publicly verify the correctness of primitive sets operations such as intersection, union, subset and set difference. Based on a novel extension of the security properties of bilinear-map accumulators {{as well as on}} a primitive called accumulation tree, our authenticated data structure is the first to achieve optimal verification and proof complexity (i. e., only proportional {{to the size of the}} query parameters and the answer), as well as optimal update complexity (i. e., constant), and without bearing any extra asymptotic space overhead. Queries (i. e., constructing the proof) are also efficient, adding a logarithmic overhead to the complexity needed to compute the actual answer. In contrast, existing schemes entail high <b>communication</b> <b>and</b> <b>verification</b> costs or high storage costs as they recompute the query over authentic data or precompute answers to all possible queries. Applications of interest include efficient verification of keyword search and database queries. We base the security of our constructions on the bilinear q-strong Diffie-Hellman assumption...|$|E
40|$|Query {{execution}} assurance is {{an important}} concept in defeating lazy servers in the database as a service model. We show that extending query execution assurance to outsourced databases with multiple data owners is highly inefficient. To cope with lazy servers in the distributed setting, we propose query access assurance (QAA) that focuses on IO-bound queries. The goal in QAA is to enable clients to verify that the server has honestly accessed all records {{that are necessary to}} compute the correct query answer, thus eliminating the incentives for the server to be lazy if the query cost is dominated by the IO cost in accessing these records. We formalize this concept for distributed databases, and present two efficient schemes that achieve QAA with high success probabilities. The first scheme is simple to implement and deploy, but may incur excessive server to client <b>communication</b> cost <b>and</b> <b>verification</b> cost at the client side, when the query selectivity or the database size increases. The second scheme is more involved, but successfully addresses the limitation of the first scheme. Our design employs a few number theory techniques. Extensive experiments demonstrate the efficiency, effectiveness and usefulness of our schemes...|$|R
30|$|Generally, how to {{configure}} a SSCD {{in the power}} grid should take control range and functions to be achieved of the device and power system planning into consideration. In order to achieve large-scale stability control, SSCDs of multiple plants are usually configured as a SSCS via <b>communication</b> channel <b>and</b> <b>communication</b> interface equipment, <b>and</b> work cooperatively, to achieve the regional security and stability control. However, in the communication progress between SSCDs, the reception and execution of commands in the SSCS are affected by error codes, unstable transmission in communication channel, not-in-time information transfer. It brings about that the strategy cannot be executed at right time or security & control devices cannot act properly, resulting in more serious accidents. On July 30, 2004, the security and stability control subsystem of Zengcheng, Guangdong mal-operated with 714  MW load shedding {{for the reason that}} communication codes were error and there were loopholes in the <b>communication</b> module <b>and</b> the <b>verification</b> links.|$|R
40|$|The SSL/TLS {{protocols}} over HTTPs {{main tasks}} are to encrypt <b>communication</b> <b>and</b> provide <b>verification</b> {{to the user}} that the website is the one it is claiming to be. With an increase in egovernment and agencies using e-services where sensitive information can travel over the Internet the need for SSL/TLS has increased {{and will continue to}} increase. This study therefore aims to provide answers to how the Swedish agencies have configured their websites in terms of SSL/TLS and why they are at their current level of security in regards to SSL/TLS. A technical survey using the tool Qualys SSL Server Test was used in order to collect the configurations. Follow up interviews with a semi-structured qualitative approach was then used to answer the second research question of what factors affect why they had their current security level. 48, 77 % of agencies had some sort of implementation but the majority did not use SSL/TLS. The ten most common factors which affected agencies security levels was “Projects”, “Availability”, “Attitude towards security”, “Perceived sensitivity of data”, “Consultants”, “Resources”, “Knowledge of SSL”, “Security responsibility”, “Eservice”, and “Laws or other externa influence”...|$|R
40|$|Policies {{for energy}} {{efficiency}} requirements in buildings have become more stringent according to EU 2020 goals. Despite policy regulations, requirements for energy efficiency are not met in many new buildings. Some {{of the reasons for}} this energy performance gap are related to the building process. The aim with this paper is to describe a purposed method for quality assurance of sustainable buildings according to energy efficiency. The proposed method is called ByggaE, where ‘Bygga’ is the Swedish word for ‘build’ and E is the first letter in ‘energy efficient’. It is a tool intended to lower the energy performance gap related to the building process by guiding the client and providers through the process to fulfill goals. The essence of ByggaE is the formulation of requirements by the client and the working process of identifying, handling and following up critical constructions and key issues. This working process involves all participants in the building project by using appropriate quality guidelines and checklists for documentation, <b>communication</b> <b>and</b> <b>verification.</b> ByggaE is a step forward ensuring that the building fulfills the defined functions and that conscious decisions are taken when goals have to be changed during the building project. The next steps are to ensure the usefulness of the method in practice by more testing and to spread knowledge about the method...|$|E
40|$|Hash {{tables are}} {{fundamental}} data structures that optimally answer membership queries. Suppose a client stores n {{elements in a}} hash table that is outsourced at a remote server so that the client can save space or achieve load balancing. Authenticating the hash table functionality, i. e., verifying the correctness of queries answered by the server and ensuring {{the integrity of the}} stored data, is crucial because the server, lying outside the administrative control of the client, can be malicious. We design efficient and secure protocols for optimally authenticating membership queries on hash tables: for any fixed constants 0 1 /ǫ, the server can provide a proof of integrity of the answer to a (non-) membership query in constant time, requiring O (n ǫ / log κǫ− 1 n) time to treat updates, yet keeping the <b>communication</b> <b>and</b> <b>verification</b> costs constant. This is the first construction for authenticating a hash table with constant query cost and sublinear update cost. Our solution employs the RSA accumulator in a nested way over the stored data, strictly improving upon previous accumulator-based solutions. Our construction applies to two concrete data authentication models and lends itself to a scheme that achieves different trade-offs—namely, constant update time and O(n ǫ / log κǫ n) query time for fixed ǫ> 0 and κ> 0. An experimental evaluation of our solution shows very good scalability...|$|E
40|$|E-commerce {{is rapidly}} {{transforming}} the way accounting and auditing functions are performed, posing new {{challenges to the}} accounting profession. A few guidelines exist for meeting some of these new challenges, however, sheer magnitude and diversity of problems render these guidelines grossly inadequate to comprehend their intricacies. Most of the studies on e-commerce have examined the extent of its activities in the advanced countries. However, no attempt {{seems to have been}} made to examine the accounting and auditing aspects of e-commerce. The present work, therefore, bridges a research gap: it aims at identifying critical issues related to e-accounting and e-auditing that the professional community is presently confronted with. Two sets of pre-structured questionnaires have been used to collect information from 75 accounting professionals and 75 auditors who are members of the Institute of Chartered Accountants of India. The research has underlined the need for standards related to measurement, recording and disclosure of certain e-transactions. E-environment has also changed the manner of audit and the nature of services required from an auditor. It is hoped that the present work would be useful for professionals, policy makers, regulatory bodies and managers in the development of strategies for better handling of e-commerce environment. It would provide some insight into identification, measurement, <b>communication</b> <b>and</b> <b>verification</b> of economic information about e-commerce transactions...|$|E
40|$|System-on-Chip (SoC) is a {{promising}} paradigm to implement safety-critical embedded systems, but it poses significant challenges from a design <b>and</b> <b>verification</b> point of view. In particular, in a mixed-criticality system, low criticality applications must be prevented from interfering with high criticality ones. In this paper, we {{introduce a new}} design methodology for SoC that provides strong isolation guarantees to applications with different criticalities. A set of certificates describing the assumed application behavior is extracted from a functional Architectural Analysis and Design Language (AADL) specification. Our tools then automatically generate hardware wrappers that enforce at run-time the behavior described by the certificates. In particular, we employ run-time monitoring to formally check all data communication in the system, and we enforce timing reservations for both computation <b>and</b> <b>communication</b> resources. <b>Verification</b> is greatly simplified because certificates are much simpler than the components used to implement low-criticality applications. The effectiveness of our methodology is proven on a case study consisting of a medical pacemaker...|$|R
40|$|Two one-millipound-thrust cesium-bombardment ion {{thrusters}} {{have been}} developed and integrated on the ATS-F spacecraft {{for the purpose of}} demonstrating compatible north-south stationkeeping of a synchronous communication satellite. In addition to the two flight units, an identical system is undergoing extended testing on the ground to demonstrate operating lifetime. On July 17, 1974, approximately 50 days after launch, preliminary operation of one of the thrusters was begun. Completely successful operation was achieved on the first run. In addition to verifying operation, the principal accomplishments were the demonstration of a total absence of interference with the complete array of spacecraft-to-ground <b>communication</b> links <b>and</b> the <b>verification</b> of the predicted spacecraft operating potential of approximately - 10 volts. Subsequent attempts to operate the same thruster have not been successful and an investigation of possible explanations is underway...|$|R
40|$|There {{is a great}} {{diversity}} in the transmission technologies in current data networks. Individual technologies are in most cases incompatible at physical and partially also at the link layer of the reference ISO/OSI model. Network compatibility, {{as the ability to}} transmit data, is realizable through the third layer, which is able to guarantee the operation of the different devices across their technological differences. The proposed inverse packet multiplexer addresses increase of the speed and reliability of packet transmission to the third layer, {{and at the same time}} it increases the stability of the data communication by the regulation of the delay value during the transmission. This article presents implementation of a <b>communication</b> system <b>and</b> its <b>verification</b> in real conditions. The conclusion compares the strengths and weaknesses of the proposed control system...|$|R
30|$|Traditional {{asymmetric}} schemes such as public-key {{techniques are}} {{not suitable for}} the resource-constrained sensor nodes, which are characterized by limited memory, computation, <b>communication,</b> <b>and</b> power. There are many variations of symmetric key schemes[9 – 11] used in the certificate authentication, <b>and</b> <b>verification</b> of a broadcast message. These variations are suitable for sensor nodes because they use the delay disclosure key that is actually used in a symmetric scheme for authentication <b>and</b> <b>verification.</b>|$|R
40|$|Title: The {{basics of}} self-defense for helping professions Goals: The {{goal of this}} work is to define risks arising from {{the nature of the}} helping professions and to find out {{possibilities}} leading to minimization of these risks. The main goal of this work is to develop a training program for clients who face attacks not only on the performance of their profession but also on their life or health. Methods: The identifying of risks and oportunities for target group training was performed using analysis of available resources and using exploratory investigation carried out with target group representatives. Selecting the most appropriate tactical and strategic solutions, defensive techniques, training methods and pedagogical procedures has been conducted by facts research in accessible resources in dealing with risky situation, self-defence, psychology, law, social <b>communication</b> <b>and</b> education. The <b>verification</b> <b>and</b> the modification of the training programme ran continuously during the test training with target groups and through consultations with experts in certain field. The final verification of the effectiveness of the created system was solved partly by the accreditation of relevant ministry to which competency the target groups belong and especially by the critical evaluation of invited consultants [...] . ...|$|R
40|$|Simulation {{is widely}} used for {{modeling}} engineering artifacts and natural phenomena to gain insight into the operation of those systems. Formal verification is concerned with proving or disproving the correctness of a system {{with respect to a}} certain property. Despite of these different objectives, the fields of simulation <b>and</b> <b>verification</b> address similar research challenges. Particularly, in the application area systems biology simulation <b>and</b> <b>verification</b> are moving together. The Dagstuhl Seminar was dedicated to intensifying this dialogue, and stimulating the exchange of ideas. Three working groups discussed questions: Why are biological systems difficult to model?, What role does refinement and abstraction play in combining simulation <b>and</b> <b>verification?,</b> What is the role of <b>communication</b> <b>and</b> composition in simulating and analysing dynamic systems? The results of the working groups {{can be found in the}} working groups 2 ̆ 7 report...|$|R
30|$|In {{traditional}} searchable encryption schemes, servers {{are generally}} considered to be half trustworthy. In this model, the server is faithful and inquisitive, and it enforces the protocol strictly, but tries to find as much secret information as possible from the resources it has, given that the medical server may be selfish in addition to “curiosity” in order to save computing and download bandwidth. And the proposed hash algorithm, the MTH algorithm, is proposed in order to verify the search result (MTH is a continuation algorithm of the hash algorithm, which provides an effective method for dealing with the gap between theory and reality) by generating each tuple MTH, which reduces <b>communication</b> costs <b>and</b> improves <b>verification</b> efficiency. In addition, Chaff and others have proposed a new, stronger server model, called Servers that are half believable and curious. Under this model, the medical server may perform only partial search operations and return partial search results.|$|R
40|$|This paper investigates how formal {{techniques}} {{can be used}} for the analysis <b>and</b> <b>verification</b> of hybrid systems [1, 5, 7, 16] [...] - systems involving both discrete and continuous behavior. The motivation behind such research lies in the inherent similarity of the hierarchical and decentralized control strategies of hybrid systems <b>and</b> the <b>communication</b> <b>and</b> operation protocols used for distributed systems in computer science...|$|R
40|$|The use of {{two-dimensional}} (2 D) and three-dimensional (3 D) visual displays for discrete-event simulation (DES) {{has become}} very popular within the simulation community in recent years. This paper presents results from surveys of users of 3 D and 2 D simulation applications regarding their views of the impact, benefits and drawbacks of 3 D displays. The results indicate that 3 D displays can often {{be more effective than}} 2 D displays in <b>communication,</b> <b>verification</b> <b>and</b> validation, and experimentation. This can lead to a better project outcome with an improved understanding of the real system and a better solution for the decision maker. The main drawback is the additional cost, time and complexity of building the model. Potential implications for modeling in general are the importance of being able to relate the model to the real system and of involving the decision maker in the modeling process...|$|R
