4449|822|Public
25|$|Various DoS-causing {{exploits}} such as {{buffer overflow}} can cause server-running software to get confused {{and fill the}} disk space or consume all available memory or <b>CPU</b> <b>time.</b>|$|E
25|$|The {{development}} of time-sharing systems {{led to a}} number of problems. One was that users, particularly at universities where the systems were being developed, seemed to want to hack the system to get more <b>CPU</b> <b>time.</b> For this reason, security and access control became a major focus of the Multics project in 1965. Another ongoing issue was properly handling computing resources: users spent most of their time staring at the terminal and thinking about what to input instead of actually using the resources of the computer, and a time-sharing system should give the <b>CPU</b> <b>time</b> to an active user during these periods. Finally, the systems typically offered a memory hierarchy several layers deep, and partitioning this expensive resource led to major developments in virtual memory systems.|$|E
25|$|Handles {{process and}} thread {{creation}} and termination, and it implements {{the concept of}} Job, a group of processes that can be terminated as a whole, or be placed under shared restrictions (such a total maximum of allocated memory, or <b>CPU</b> <b>time).</b> Job objects were introduced in Windows 2000.|$|E
30|$|The used <b>CPU</b> <b>times</b> of full {{system are}} 7  s. For Galerkin and least-square finite element methods, {{this time is}} equal.|$|R
5000|$|Work unit <b>CPU</b> <b>times</b> vary widely: {{some work}} units {{can be very}} fast (10 minutes) and some can be very slow (75 hours).|$|R
30|$|From Table  2, the {{iteration}} {{numbers and}} <b>CPU</b> <b>times</b> of the nonlinear MHSS-like method for solving the AVE (1.1) {{are less than}} that of the nonlinear HSS-like method. The presented results in Table  2 show that in all cases the nonlinear MHSS-like method is superior to the nonlinear HSS-like method in terms of the iteration numbers and <b>CPU</b> <b>times.</b> Comparing with the nonlinear HSS-like method, the nonlinear MHSS-like method for solving the AVE (1.1) may be given priority under certain conditions.|$|R
25|$|The Virtual Machine console is {{replaced}} by an integrated Virtual Machines shell folder. Several options from the console have been removed such as Restore at start, <b>CPU</b> <b>time</b> performance settings, muting sound in inactive virtual machines, full-screen resolution related options, configuring the host key, mouse capture options and settings for requiring administrator permissions.|$|E
25|$|LPAR (Logical PARtitioning), {{a feature}} {{introduced}} from IBM's mainframe computers, facilitates running multiple operating systems simultaneously on one IBM System i unit. A system configured with LPAR can run various operating systems on separate partitions while ensuring that one OS cannot run over the memory or resources of another. Each LPAR {{is given a}} portion of system resources (memory, hard disk space, and <b>CPU</b> <b>time)</b> via a system of weights that determines where unused resources are allocated at any given time. The operating systems supported (and commonly used) under the LPAR scheme are IBM i, AIX, and Linux.|$|E
25|$|Another {{factor that}} impacts total <b>CPU</b> <b>time</b> needed by a {{simulation}} {{is the size}} of the integration timestep. This is the time length between evaluations of the potential. The timestep must be chosen small enough to avoid discretization errors (i.e., smaller than the fastest vibrational frequency in the system). Typical timesteps for classical MD are in the order of 1femtosecond (10−15 s). This value may be extended by using algorithms such as the SHAKE constraint algorithm, which fix the vibrations of the fastest atoms (e.g., hydrogens) into place. Multiple time scale methods have also been developed, which allow extended times between updates of slower long-range forces.|$|E
40|$|Abstract: In this study, {{numerical}} {{simulations of}} acoustic propagation using a 3 -D parabolic equation based model are presented. The calculations are performed on a {{massively parallel computer}} with two parallelization levels {{in order to reduce}} <b>CPU</b> <b>times</b> for broadband and CW signal propagation. The parallelization procedure is de-scribed in detail. Both speedup and efficiency are investigated. <b>CPU</b> <b>times</b> are given for the 3 -D ASA wedge benchmark. The parallelized code is then applied to a realistic environment problem involving both sound speed profiles and bathymetry data sets...|$|R
40|$|A parallelized {{algorithm}} {{based on}} an existing 3 D wide-angle parabolic equation model is devel-oped to perform numerical simulations of underwater acoustic propagation on a massively parallel computer. The parallelization method used is a suitable two-level procedure: A frequency decom-position and a spatial decomposition of the calculations, which are respectively dedicated to reduce <b>CPU</b> <b>times</b> for broadband and cw signal propagation. The high-performance of the parallelized algorithm is examined for the 3 D extension of the classical ASA wedge benchmark. <b>CPU</b> <b>times</b> are reported and both speedup and efficiency are analyzed. An investigation of significant 3 D effects at higher frequencies and at longer propagation ranges than in earlier works [F. Sturm, J. Acoust. Soc. Am. 117 (3) (2005) 1058 – 1079] is performed with reasonable <b>CPU</b> <b>times</b> by using the new parallel algorithm. Further, the feasability of the procedure applied to a realistic oceanic environment problem involving both real sound speed profiles and bathymetry data sets is also illustrated...|$|R
30|$|The results {{shown in}} Table 3 {{demonstrate}} the <b>CPU</b> usage <b>time</b> variation {{according to the}} chosen architecture. It is also possible to verify that <b>CPU</b> usage <b>time</b> variation {{is strongly dependent on}} the test case characteristics.|$|R
25|$|The {{computational}} {{cost of a}} water simulation {{increases with}} the number of interaction sites in the water model. The <b>CPU</b> <b>time</b> is approximately proportional to the number of interatomic distances that need to be computed. For the 3-site model, 9 distances are required for each pair of water molecules (every atom of one molecule against every atom of the other molecule, or 3 × 3). For the 4-site model, 10 distances are required (every charged site with every charged site, plus the O–O interaction, or 3 × 3 + 1). For the 5-site model, 17 distances are required (4 × 4 + 1). Finally, for the 6-site model, 26 distances are required (5 × 5 + 1).|$|E
25|$|A single-tasking {{system can}} only run one {{program at a}} time, while a multi-tasking {{operating}} system allows more than one program to be running in concurrency. This is achieved by time-sharing, dividing the available processor time between multiple processes that are each interrupted repeatedly in time slices by a task-scheduling subsystem of the operating system. Multi-tasking may be characterized in preemptive and co-operative types. In preemptive multitasking, the operating system slices the <b>CPU</b> <b>time</b> and dedicates a slot {{to each of the}} programs. Unix-like operating systems, e.g., Solaris, Linux, as well as AmigaOS support preemptive multitasking. Cooperative multitasking is achieved by relying on each process to provide time to the other processes in a defined manner. 16-bit versions of Microsoft Windows used cooperative multi-tasking. 32-bit versions of both Windows NT and Win9x, used preemptive multi-tasking.|$|E
500|$|The {{graphics}} and physics code increased the game's system requirements, {{and the team}} worked to optimize performance during development. They struggled to improve the game's memory usage: the process consumed nearly {{as much time as}} the creation of the physics model, according to Church. Programmer Eric Twietmeyer ran weekly tests of the game's performance by disabling certain parts of the code—such as the physics calculations—to isolate which parts used the most memory. By 1994, Blackley's physics code took up only 1% of <b>CPU</b> <b>time,</b> with the rest allocated to the terrain renderer. Blackley optimized his code by converting the mathematical calculations of air from the 3D game world into a [...] "math-friendly space", during which time the Navier-Stokes equations are applied. Afterwards, the data is returned to 3D space. According to Computer Gaming World, this method increased speed by [...] "a factor of 100, with almost no loss in precision." [...] The team had trouble with complex memory-related glitches during development. Church called them [...] "crazy", and programmer Greg Travis noted that debugging the terrain cache system was a [...] "nightmare".|$|E
30|$|In {{terms of}} {{computational}} efficiency, the <b>CPU</b> <b>times</b> of the ASC-N have big advantage {{compared with the}} ID scheme. From Tables  2 and 3, {{we know that the}} parallel computing advantages of the ASC-N scheme will be more obvious with the increase of the number of time layers or space lattice points. Comparing with the ID scheme the <b>CPU</b> <b>times</b> of the ASC-N scheme can save near 90 %. Comprehensively considering the computational efficiency and computational accuracy, the ASC-N scheme can be more effective to solve the time fractional sub-diffusion equation. When the long time course is calculated, the parallel computing advantages of the ASC-N scheme will be more evident.|$|R
30|$|We compare {{different}} algorithms {{from the}} point of view of iteration numbers and <b>cpu</b> <b>times</b> (seconds). Here, we consider three algorithms: Algorithm 3.1, denoted by (AL); the semismooth equation approach proposed in [21], denoted by SSN; and primal-dual algorithm proposed in [22], denoted by PDA.|$|R
30|$|From Tables  1 - 4, we see that, {{under the}} same conditions, both the number of {{iterative}} steps and the <b>CPU</b> <b>times</b> of our algorithm are less than Byrne’s. So to some extent, the numerical results indicate that our algorithm is better than Byrne’s.|$|R
2500|$|The {{solution}} of these equations in principle defines a three-dimensional and transient {{field of the}} relevant variables such as temperature or species. However, the application of these conservation principles to {{a large number of}} particles usually restricts the resolution to at most one representative dimension and time due to <b>CPU</b> <b>time</b> consumption. Experimental evidence ...|$|E
2500|$|Important {{merit of}} the matrix [...] {{is the fact that}} it allows to {{directly}} compute the frequency response of the equivalent network having the inductively coupled resonant circuits,. Therefore it is convenient to use this matrix when designing the cross-coupled filters. The coupling matrices , in particular, are used as coarse models of filters. Utilization of a coarse model allows to quicken filter optimization manyfold because of computation of the frequency response for the coarse model does not consume <b>CPU</b> <b>time</b> with respect to computation for the real filter.|$|E
2500|$|Decoding, on {{the other}} hand, is {{carefully}} defined in the standard. Most decoders are [...] "bitstream compliant", {{which means that the}} decompressed output that they produce from a given MP3 file will be the same, within a specified degree of rounding tolerance, as the output specified mathematically in the ISO/IEC high standard document (ISO/IEC 11172-3). Therefore, comparison of decoders is usually based on how computationally efficient they are (i.e., how much memory or <b>CPU</b> <b>time</b> they use in the decoding process). Over time this concern has become less of an issue as CPU speeds transitioned from MHz to GHz. Encoder/decoder overall delay is not defined, which means there is no official provision for gapless playback. However, some encoders such as LAME can attach additional metadata that will allow players that can handle it to deliver seamless playback.|$|E
40|$|Abstract: The paper reviews {{different}} methods of orthogonalization of constraints adopted in multibody dynamics formulations. A new approach, based on Schur decomposition, has been herein proposed. All the {{methods have been}} implemented in the MATLAB computing environnement and <b>CPU</b> <b>times</b> required for a simple simulation have been recorded. 1...|$|R
40|$|SLS) {{algorithms}} {{for training}} neural networks with threshold activation functions. and proposes a novel technique, called Bi nary Learning Machine (BLM). BLM acts by changing individual bits in the binary representation of each weight and picking improving moves. While brute-force implementations of SLS lead to enormous <b>CPU</b> <b>times,</b> {{due to the}} limited extent of each move, the use of incremental neighborhood evaluation, first-improving strategies, and Gray encoding accelerates SLS by a huge factor, opening the way to affordable <b>CPU</b> <b>times</b> {{for a wide range}} of problems. Comparisons with alternative methods demonstrate the effec tiveness of the approach. In details, BLM outperforms state of-the-art techniques either by achieving better generalization properties, or by allowing for more compact networks, suitable for the type of applications for which threshold neural networks were originally introduced. I...|$|R
40|$|Due to {{copyright}} restrictions, {{the access}} to {{the full text of}} this article is only available via subscription. We investigate the one-dimensional variable-sized bin-packing problem. This problem requires packing a set of items into a minimum-cost set of bins of unequal sizes and costs. Six optimization-based heuristics for this problem are presented and compared. We analyze their empirical performance on a large set of randomly generated test instances with up to 2000 items and seven bin types. The first contribution {{of this paper is to}} provide evidence that a set covering heuristic proves to be highly effective and capable of delivering very-high quality solutions within short <b>CPU</b> <b>times.</b> In addition, we found that a simple subset-sum problem-based heuristic consistently outperforms heuristics from the literature while requir- ing extremely short <b>CPU</b> <b>times...</b>|$|R
2500|$|Most {{variants}} {{solved by}} Geoffrey Irving, Jeroen Donkers and Jos Uiterwijk (2000) except Kalah (6/6). The (6/6) variant was solved by Anders Carstensen (2011). Strong first-player advantage was proven in most cases. [...] Mark Rawlings, of Gaithersburg, MD, has quantified {{the magnitude of}} the first player win in the (6/6) variant (2015). [...] After creation of 39 GB of endgame databases, searches totaling 106 days of <b>CPU</b> <b>time</b> and over 55 trillion nodes, it was proven that, with perfect play, the first player wins by 2. Note that all these results refer to the Empty-pit Capture variant and therefore are of very limited interest for the standard game. [...] Analysis of the standard rule game has now been posted for Kalah(6,4), which is a win by 8 for the first player, and Kalah(6,5), which is a win by 10 for the first player. Analysis of Kalah(6,6) with the standard rules is on-going, however, it has been proven that it is a win by at least 4 for the first player.|$|E
5000|$|The total <b>CPU</b> <b>time</b> is the {{combination}} of the amount of time the CPU or CPUs spent performing some action for a program and the amount of time they spent performing system calls for the kernel on the program's behalf. When a program loops through an array, it is accumulating user <b>CPU</b> <b>time.</b> Conversely, when a program executes a system call such as [...] or , it is accumulating system <b>CPU</b> <b>time.</b>|$|E
50|$|<b>CPU</b> <b>time</b> (or process time) is {{the amount}} of time for which a central {{processing}} unit (CPU) was used for processing instructions of a computer program or operating system, as opposed to, for example, waiting for input/output (I/O) operations or entering low-power (idle) mode. The <b>CPU</b> <b>time</b> is measured in clock ticks or seconds. Often, it is useful to measure <b>CPU</b> <b>time</b> {{as a percentage of the}} CPU's capacity, which is called the CPU usage.|$|E
5000|$|The VAX-11/785, code-named [...] "superstar", was {{introduced}} in April 1984. It is essentially a faster VAX-11/780, with a <b>CPU</b> cycle <b>time</b> of 133 ns (7.52 MHz) versus the 200 ns (5 MHz) <b>CPU</b> cycle <b>time</b> of the VAX-11/780. The memory subsystem was also upgraded to support higher capacity memory boards.|$|R
30|$|This {{approach}} is tested on different artificial and real images for additive noise, {{and the results}} are compared with the existing methods. Our experimental results have shown that the quality of the restoration of images, the number of iterations, and the <b>CPU</b> <b>times</b> {{with the use of the}} proposed method are quite good, and the proposed algorithm is quite efficient. We have also noticed that the performance of our proposed method is far better than that of the existing methods regarding restoration quality (PSNR), the number of iterations, and <b>CPU</b> <b>times</b> because of the mesh-free properties of RBF used in our algorithm. The choice of shape parameter c also plays a significant role in this algorithm, which affects the image restoration. The shape parameter analysis has also been discussed here. A comparison with another method in this field is provided as well.|$|R
3000|$|We compare {{different}} algorithms {{from the}} point of view of iteration numbers and <b>CPU</b> <b>times.</b> Here, we consider three algorithms: classical additive Schwarz algorithm (i.e., Algorithm 2.1, denoted by AS), Newton’s method proposed in [9] (denoted by SSN), and Algorithm 2.2 (denoted by TLDD). In the AS, we decompose N into two equal parts with the overlapping size [...]...|$|R
5000|$|Because of {{the idle}} process's function, its <b>CPU</b> <b>time</b> {{measurement}} (visible through, for example, Windows Task Manager) {{may make it}} appear to users that the idle process is monopolizing the CPU. However, the idle process does not use up computer resources (even when stated to be running at a high percent). Its <b>CPU</b> <b>time</b> [...] "usage" [...] {{is a measure of}} how much <b>CPU</b> <b>time</b> is not being used by other threads.|$|E
50|$|<b>CPU</b> <b>time</b> is a {{necessary}} yet undesirable factor. For a highly refined mesh, where the number of cells per unit area is maximum, the <b>CPU</b> <b>time</b> required will be relatively large. Time will generally be proportional {{to the number of}} elements.|$|E
50|$|Elapsed {{real time}} is always {{greater than or}} equal to the <b>CPU</b> <b>time</b> for {{computer}} programs which use only one CPU for processing. If no wait is involved for I/O or other resources, elapsed real time and <b>CPU</b> <b>time</b> are very similar.|$|E
30|$|Firstly, the {{proposed}} algorithms, mDEOB/best/ 1 and mDEOB/cur_to_best/ 1, {{can find the}} minimal objective value obtained in literatures. The best results obtained by other researchers in Table  4 is 2.65856. mDEOB/best/ 1 and mDEOB/cur_to_best/ 1 can also find the optimal solution with the average <b>CPU</b> <b>times</b> of 0.02578 s and 0.02322 s in 100 independent runs.|$|R
30|$|A Seventh-Order Linear Multistep Method (SOLMM) is {{developed}} and implemented in both predictor-corrector mode and block mode. The two approaches are compared by measuring their {{total number of}} function evaluations and <b>CPU</b> <b>times.</b> The stability property of the method is examined. This SOLMM is also compared with existing methods in the literature using standard numerical examples.|$|R
40|$|Today wave-body {{interaction}} {{problems can}} be solved numerically by using softwares based on Reynolds Averaged Navier-Stokes Equations (RANSE). So {{it is possible to}} take into account vorticity and viscosity effects which can influence hydrodynamic loads and structure of the flows. However numerical simulations under viscous flow theory still lead generally to large <b>CPU</b> <b>times</b> because of gri...|$|R
