0|256|Public
40|$|A {{method for}} {{identification}} of the primary (main clause) functional <b>constituents</b> of Swedish <b>sentences</b> is outlined. The method gives a robust analysis of the unbounded constituents (phrases which {{do not have an}} upper bound on their length: subjects, objects/predicatives and adverbials) by first identifying bounded <b>constituents.</b> Diderichsen’s <b>sentence</b> schema, chunking, syntactic valency data and heuristics are used for the delimitation of the constituents and labelling with grammatical functions. ...|$|R
50|$|The {{nomenclature}} {{used for}} the <b>constituents</b> of <b>sentences</b> such as {{this is still a}} matter of some dispute, but there might be the subject, are the copula, and ten desks a predicate nominal.|$|R
50|$|Interrogative {{words are}} always the first <b>constituents</b> of a <b>sentence</b> or phrase.|$|R
5000|$|Focus rule: Whichever <b>constituent</b> of a <b>sentence</b> is {{in focus}} {{immediately}} precedes the verb.|$|R
40|$|The {{nature of}} the content {{structure}} and the hierarchical structure of a narrative, {{and the relationship between}} the two, are important questions for the analysis and formation of texts. In Guarani, some light is shed on these matters by the <b>sentence</b> <b>constituents</b> that occur <b>sentence</b> initially and precede the independent clause. 1 Such a sentence initial element serves one or bot...|$|R
5000|$|... {{describes}} the basic order of <b>sentence</b> <b>constituents</b> in main clauses as comprising the following 8 positions: ...|$|R
30|$|As {{exemplified}} in 2 a-d, the basic word order of <b>constituents</b> in a <b>sentence</b> is AUX[*]>[*]VERB[*]>[*]ERG[*]>[*]OBL[*]>[*]ABS, where the sign ‘>’ means ‘precede’.|$|R
2500|$|Constituency {{tests are}} {{diagnostics}} {{used to identify}} the <b>constituent</b> structure of <b>sentences.</b> There are numerous constituency tests applied to English sentences, such as: ...|$|R
50|$|In head-initial structures, which {{includes}} example SVO and VSO word order, the speaker's {{goal is to}} order the <b>sentence</b> <b>constituents</b> from least to most complex.|$|R
40|$|Natural {{language}} processing systems which deal with real-world documents require several low-level {{tasks such as}} splitting a text into its <b>constituent</b> <b>sentences,</b> and splitting each <b>sentence</b> into its <b>constituent</b> tokens. These basic text segmentation services are usually supplied by some preprocessor prior to linguistic analysis. While this task is often considered as unsophisticated clerical work, in the natural sciences and engineering domains it poses particular problems due to complex naming conventions. In this paper, we first introduce an annotation framework for sentence and token splitting underlying a newly constructed sentence- and token-tagged biomedical text corpus. This corpus serves as a training environment and test bed for our machine-learning based sentence and token splitters using Conditional Random Fields (CRFs). Our evaluation experiments reveal that CRFs with a rich feature set substantially increase the performance for sentence and token segmentation in scientific documents, viz. from the biomedical domain. ...|$|R
50|$|Aggregation is a subtask {{of natural}} {{language}} generation, which involves merging syntactic <b>constituents</b> (such as <b>sentences</b> and phrases) together. Sometimes aggregation {{can be done}} at a conceptual level.|$|R
50|$|The syntax of each lexical {{category}} {{and its associated}} phrase (i.e., the syntactic <b>constituents</b> below the <b>sentence</b> level) is detailed below. Attention is paid to both form and function.|$|R
40|$|Meaning {{extraction}} from text documents {{is a form}} {{of information}} management. The approach suggested in this paper is based on Peirce's semiotic which, by virtue of its deeper foundation, provides us with an adequate modelling of the information content of language. We exemplify the potential of the Peircean approach by extracting the meaning of a sample English text. Keywords language analysis, C. S. Peirce, semiotic, meaning extraction. 1 Introduction We consider the problem of determining the structure of the information content of a text document. Such a structure will be referred to as its meaning. We will present a partial solution to this problem which is based on syntactic analysis, and we will argue that the meaning of a text (in the above sense) can be derived from the syntactic structure of its <b>constituent</b> <b>sentences.</b> Preliminary research using conceptual lattices is presented in [Sarbo and Farkas, 1995], [Sarbo, 1997] and [Sarbo, 1999]. Related research on text summarisa [...] ...|$|R
5000|$|Here {{the element}} var (the past tense third person {{singular}} {{form of the}} verb vera, ‘to be’, i.e. ‘was’) is the second <b>constituent</b> of the <b>sentence.</b> If we change the sentence, however: ...|$|R
50|$|Since {{there is}} {{disagreement}} concerning {{the status of}} finite VPs (whether they are constituents or not), empirical considerations are needed. Grammarians can (again) employ constituency tests {{to shed light on}} the controversy. Constituency tests are diagnostics for identifying the <b>constituents</b> of <b>sentences</b> and they are thus essential for identifying phrases. The results of most constituency tests do not support the existence of a finite VP constituent.|$|R
40|$|Basque {{is both a}} {{minority}} and a highly inflected language with free order of <b>sentence</b> <b>constituents.</b> Machine Translation of Basque is thus both a real need and a test bed for MT techniques. In this paper, we present a modular Data-Driven MT system which includes different chunkers as well as chunk aligners which {{can deal with the}} free order of <b>sentence</b> <b>constituents</b> of Basque. We conducted Basque to English translation experiments, evaluated on a large corpus (270, 000 sentence pairs). The experimental results show that our system significantly outperforms state-of-the-art approaches according to several common automatic evaluation metrics. ...|$|R
5000|$|When {{there is}} no pragmatically marked <b>constituents</b> in the <b>sentence</b> to take the preverbal slot (for example when all the {{information}} is new), the slot has to take a dummy subject [...] "der".|$|R
50|$|In this example, the {{computer}} program has specified the linguistic <b>constituents</b> of the <b>sentence</b> (verb, subject), and also linguistic features (plural subject, negated), and from this information the realiser has constructed the actual sentence.|$|R
5000|$|Evidentials in the Quechuan {{languages}} are [...] "second position enclitics", which usually {{attach to the}} first <b>constituent</b> in the <b>sentence,</b> as shown in this example.Once, there were an old man and an old woman.|$|R
40|$|We {{investigated}} the online {{sensitivity of the}} semantic integration system to the different roles played by <b>sentence</b> <b>constituents</b> that are necessary (verbs and nouns) or optional (adjectives) for argument completion. We compared the effect of semantic incongruities introduced in both types of words on the N 400 ERP component. Participants read sentences formeaning, half ofwhichwere rendered anomalous by an incongruent verb, noun, or an early/late adjective. Incongruent adjectives led to smaller N 400 effects than did incongruent nouns and verbs, and the congruity effect for sentence-final adjectives was not significant. All incongruities are therefore not created equal: Incongruent optional <b>sentence</b> <b>constituents</b> create less of an integrative burden than incongruent mandatory <b>sentence</b> <b>constituents,</b> sug-gesting that online sentence integration processes {{are sensitive to the}} distinct roles played by different words in shaping sentence meaning. Descriptors: N 400, ERP, Semantic integration, Sentence comprehension, Word class The current study was designed to compare the relative impor-tance of verbs, nouns, and adjectives in shaping the meaning of simple transitive sentences. We gauged the online sensitivity of the semantic integration system to the different roles played i...|$|R
40|$|Abstract The proviso problem arises for {{theories}} of presupposition whose projection component fails to derive certain presuppositions that are contributed by their <b>constituent</b> <b>sentences.</b> Mismatch-based satisfaction theories {{respond to the}} difficulty by tying {{the emergence of the}} proviso problem to presupposition accommodation. Consequently, when the context entails the projected presupposition and no accommodation is required, mismatchbased satisfaction theories predict that the relevant inferences will be absent. Evidence for this predicted connection between the proviso problem and the need for accommodation is provided by Heim (1992, 2006). Against this conclusion, Geurts (1996) has provided evidence that the proviso problem arises even when the context does entail the sentence’s projected presupposition. We will argue that there are confounds in both arguments. The goal of our note is to identify the relevant confounds and to characterize the data that can overcome them. Our attempts to construct examples that control for the confounds and obtain the crucial judgments will prove unsuccessful, leaving the debate unsettled and raising new challenges to constructing the right kinds of data...|$|R
40|$|In {{order to}} be plain, {{language}} should be well-structured. This is the theory upon which the PLAIN LANGUAGE Game is based. It provides those who aspire to be legal drafters with practice in constructing well-structured statements - a useful skill for expressing clear legal norms. You have already encountered the underlining of part of the term 2 ̆ 7 well-structured 2 ̆ 7, 2 ̆ 7 PLAIN LANGUAGE 2 ̆ 7 and 2 ̆ 7 norm 2 ̆ 7, and may be wondering about it The underlining {{of the first two}} letters of each word of a term indicates that the term is a defined term and that it is being used in its defined sense. Thus, a well-structured statement has the property of being well-structured in a defined sense of the term 2 ̆ 7 well-structured 2 ̆ 7. The term is defined by the following contextual definition. A statement is well-structured, IF AND ONLY IF the relationships between its <b>constituent</b> <b>sentences</b> are expressed by defined structural terms...|$|R
40|$|The proviso problem arises for {{theories}} of presupposition whose projection component fails to derive certain presuppositions that are contributed by their <b>constituent</b> <b>sentences.</b> Mismatch-based satisfaction theories {{respond to the}} difficulty by tying {{the emergence of the}} proviso problem to presupposition accommodation. Consequently, when the context entails the projected presupposition and no accommodation is required, mismatch-based satisfaction theories predict that the relevant inferences will be absent. Evidence for this predicted connection between the proviso problem and the need for accommodation is provided by Heim (1992, 2006). Against this conclusion, Geurts (1996) has provided evidence that the proviso problem arises even when the context does entail the sentence's projected presupposition. We will argue that there are confounds in both arguments. The goal of our note is to identify the relevant confounds and to characterize the data that can overcome them. Our attempts to construct examples that control for the confounds and obtain the crucial judgments will prove unsuccessful, leaving the debate unsettled and raising new challenges to constructing the right kinds of data. [URL] BibTeX info</a...|$|R
5000|$|Some of the cruces in {{descriptive}} linguistics {{have been}} due to the search for a <b>constituent</b> analysis in <b>sentence</b> types where this does not exist because the sentences are transformationally derived from each other added ...|$|R
50|$|Merge is {{commonly}} seen as merging smaller constituents to greater constituents until the greatest <b>constituent,</b> the <b>sentence,</b> is reached. This bottom-up view of structure generation is rejected by representational (non-derivational) theories (e.g. Generalized Phrase Structure Grammar, Head-Driven Phrase Structure Grammar, Lexical Functional Grammar, most dependency grammars, etc.), {{and it is}} contrary to early work in Transformational Grammar. The phrase structure rules of context free grammar, for instance, were generating sentence structure top down.|$|R
2500|$|Question words may be {{preceded by}} another <b>sentence</b> <b>constituent</b> as topic, e.g. Tiiskama taa yutaaku? [...] "Who took the child?" [...] (child who took), Maa, ngarangki aakar? [...] "And you, {{where do you}} live?" [...] (you, where live).|$|R
50|$|Here {{the subject}} and verb have been {{inverted}} to form a question, meaning the verb is the first <b>constituent</b> in the <b>sentence</b> {{as opposed to the}} second. This method of forming questions is used in many languages, including English.|$|R
40|$|A strict {{distinction}} between semantics and syntax {{is difficult to}} maintain since the significance of a sentence is contained in and expressed by the elements occurring in it. In the majority of languages a verb is necessary as {{the core of the}} most frequent type of sentence structure. The chosen verb determines the basic structure of the sentence involved, often not so much in the order of elements as in the number and nature of the elements occurring in the sentence. The core lexical meaning of a verb is made visible in the elements with which it occurs; specific satellites modify the significance by reducing or expanding the valence or by adding other types of information. The differences between biblical Hebrew verbs as projected onto syntax are brought together in a flow chart. The presence or absence of specific <b>sentence</b> <b>constituents</b> is charted through a set of choices. In this way differences between verbs are traceable and comparable. It is possible to compare the specific contribution a particular type of <b>sentence</b> <b>constituent</b> makes to the significance of a verb with the contribution of the same <b>constituent</b> to <b>sentences</b> with other verbs. The elements contributing to patterns occurring with different types of verbs, for example, a transitive verb, an intransitive verb, or a verb of movement, are made visible...|$|R
2500|$|With {{regards to}} word order, Hindustani is an SOV language. In terms of branching, {{it is neither}} purely left- or right-branching, and {{phenomena}} of both types can be found. The order of <b>constituents</b> in <b>sentences</b> as a whole lacks governing [...] "hard and fast rules", and frequent deviations can be found from normative word position, describable {{in terms of a}} small number of rules, accounting for facts beyond the pale of the label of [...] "SOV".|$|R
40|$|We {{present a}} {{classifier}} to predict contextual polarity of subjective phrases in a sentence. Our approach features lexical scoring {{derived from the}} Dictionary of Affect in Language (DAL) and extended through WordNet, allowing us to automatically score {{the vast majority of}} words in our input avoiding the need for manual labeling. We augment lexical scoring with n-gram analysis to capture the effect of context. We combine DAL scores with syntactic constituents and then extract n-grams of <b>constituents</b> from all <b>sentences.</b> We also use the polarity of all syntactic <b>constituents</b> within the <b>sentence</b> as features. Our results show significant improvement over a majority class baseline as well as a more difficult baseline consisting of lexical n-grams. ...|$|R
40|$|Abstract—The {{rapid growth}} of the data in the Internet has {{overloaded}} the user with enormous amounts of information which {{is more difficult to}} access huge volumes of documents. Automatic text summarization technique is an important activity in the analysis of high volume text documents. Text Summarization is condensing the source text into a shorter version preserving its information content and overall meaning. In this paper a frequent term based text summarization technique with HMM tagger is designed and implemented in java. The proposed system generates a summary for a given input document based on identification and extraction of important sentences in the document. The model consists of four stages. In first stage, the system decomposes the given text into its <b>constituent</b> <b>sentences,</b> assigning the POS (tag) for each word in the text and stores the result in a table. The second stage removes the stop words, stemming the text and applying lemmatization. Feature term identification is done in third stage. Finally each sentence is ranked depending on feature terms. This stage reduced the amount of the sentences in the summary in order to produce a qualitative summary...|$|R
40|$|This paper elaborates on {{the design}} of a machine {{translation}} evaluation method that aims to determine to what degree the meaning of an original text is preserved in translation, without looking into the grammatical correctness of its <b>constituent</b> <b>sentences.</b> The basic idea is to have a human evaluator take the sentences of the translated text and, for each of these sentences, determine the semantic relationship that exists between it and the sentence immediately preceding it. In order to minimise evaluator dependence, relations between sentences are expressed in terms of the conjuncts that can connect them, rather than through explicit categories. For an n-sentence text this results in a list of n − 1 sentence-to-sentence relationships, which we call the text’s connectivity profile. This can then be compared to the connectivity profile of the original text, and the degree of correspondence between the two would be a measure for the quality of the translation. A set of “essential ” conjuncts was extracted for English and Japanese, and a computer interface was designed to support the task of inserting the most fitting conjuncts between sentence pairs. With these in place, several sets of experiments were performed. ...|$|R
40|$|The {{rapid growth}} of the data in the Internet has {{overloaded}} the user with enormous amounts of information which {{is more difficult to}} access huge volumes of documents. Automatic text summarization technique is an important activity in the analysis of high volume text documents. Text Summarization is condensing the source text into a shorter version preserving its information content and overall meaning. In this paper a frequent term based text summarization technique with HMM tagger is designed and implemented in java. The proposed system generates a summary for a given input document based on identification and extraction of important sentences in the document. The model consists of four stages. In first stage, the system decomposes the given text into its <b>constituent</b> <b>sentences,</b> assigning the POS (tag) for each word in the text and stores the result in a table. The second stage removes the stop words, stemming the text and applying lemmatization. Feature term identification is done in third stage. Finally each sentence is ranked depending on feature terms. This stage reduced the amount of the sentences in the summary in order to produce a qualitative summary...|$|R
40|$|A crucial {{step toward}} the goal of {{automatic}} extraction of propositional information from natural language text is the identification of semantic relations between <b>constituents</b> in <b>sentences.</b> We examine the problem of distinguishing among seven relation types that can occur between the entities “treatment ” and “disease ” in bioscience text, {{and the problem of}} identifying such entities. We compare five generative graphical models and a neural network, using lexical, syntactic, and semantic features, finding that the latter help achieve high classification accuracy. ...|$|R
40|$|Plagiarism of {{material}} from the Internet is a widespread and growing problem. Several methods used to detect the plagiarism and similarity between the source document and suspected documents such as fingerprint based on character or n-gram. In this paper, we discussed a new method to detect the plagiarism based on graph representation; however, Preprocessing for each document is required such as breaking down the document into its <b>constituent</b> <b>sentences.</b> Segmentation of each sentence into separated terms and stop word removal. We build the graph by grouping each sentence terms in one node, the resulted nodes are connected to each other based on order of sentence within the document, all nodes in graph are also connected to top level node "Topic Signature". Topic signature node is formed by extracting the concepts of each sentence terms and grouping them in such node. The main advantage of the proposed method is the topic signature which is main entry for the graph is used as quick guide to the relevant nodes. which should be considered for the comparison between source documents and suspected one. We believe the proposed method can achieve a good performance in terms of effectiveness and efficiency. Comment: Journal of Computing online at [URL]...|$|R
50|$|Agreement {{generally}} involves {{matching the}} value of some grammatical category between different <b>constituents</b> of a <b>sentence</b> (or sometimes between sentences, as in some cases where a pronoun is required to agree with its antecedent or referent). Some categories that commonly trigger grammatical agreement are noted below.|$|R
