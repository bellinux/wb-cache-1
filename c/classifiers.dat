10000|10000|Public
5|$|Many other {{languages}} of the Mainland Southeast Asia linguistic area exhibit similar classifier systems, leading to {{speculation about the}} origins of the Chinese system. Ancient classifier-like constructions, which used a repeated noun rather than a special classifier, are attested in Old Chinese as early as 1400 BCE, but true <b>classifiers</b> did not appear in these phrases until much later. Originally, <b>classifiers</b> and numbers came after the noun rather than before, and probably moved before the noun sometime after 500 BCE. The use of <b>classifiers</b> did not become a mandatory part of Old Chinese grammar until around 1100 CE. Some nouns became associated with specific <b>classifiers</b> earlier than others, the earliest probably being nouns that signified culturally valued items such as horses and poems. Many words that are <b>classifiers</b> today started out as full nouns; in some cases their meanings have been gradually bleached away so that they are now used only as <b>classifiers.</b>|$|E
5|$|By {{the middle}} of the 18th century the botanical booty {{resulting}} from the era of exploration was accumulating in gardens and herbaria – and it needed to be systematically catalogued. This was the task of the taxonomists, the plant <b>classifiers.</b>|$|E
5|$|Classifying {{particles}} are used after numerals, {{but are not}} always obligatory {{as they are in}} Thai or Chinese, for example, and are often dropped in colloquial speech. Khmer nouns are divided into two groups: mass nouns, those which take <b>classifiers,</b> and specific nouns, which do not. The overwhelming majority are mass nouns.|$|E
50|$|<b>Classifier</b> handshapes {{can further}} {{be divided into}} entity <b>classifier</b> handshapes and {{handling}} <b>classifier</b> handshapesAs noted by Morgan (2005, 2009), incorporation of <b>classifier</b> handshapes generally follows an ergative-absolutive patterning, with the <b>classifier</b> representing the subject incorporated of intransitive verbs and the object incorporated for transitive verbs. A distinction, however, can be made between the type of <b>classifier</b> handshape incorporated {{in each of these}} instances. thus, for subjects of intransitive verbs the <b>classifier</b> is typically and entity <b>classifier,</b> while for the object of intransitive verbs the <b>classifier</b> is typically a handling <b>classifier.</b>|$|R
50|$|Another way {{of forming}} {{relative}} clauses in Jambi Teochew is through using the <b>classifier.</b> The main {{difference between the}} kai and <b>classifier</b> relative clause {{is that there is}} the presence of a <b>classifier</b> in the <b>classifier</b> relative clause. The <b>classifier</b> in <b>classifier</b> relative clauses can only appear head-initially. The <b>classifier</b> agrees with the head noun type and is in the place of the relativizer kai.|$|R
40|$|We {{consider}} a popular approach to multi-category classification tasks: a Two-Stage {{system based on}} a first (global) <b>classifier</b> with rejection followed by a (local) Nearest Neighbor <b>classifier.</b> Patterns which are not rejected by the first <b>classifier</b> are classified according to its output. Rejected patterns are passed to the Nearest Neighbor <b>classifier</b> together with the top-h ranking classes returned by the first <b>classifier.</b> The Nearest Neighbor <b>classifier,</b> looking at patterns in the top-h classes, classifies the rejected pattern. An editing strategy for the Nearest Neighbor reference database, controlled by the first <b>classifier,</b> is also considered...|$|R
5|$|Gallagher was {{selected}} to compete at the 2008 Beijing Paralympics in long jump, 100m, shot put and discus, but she failed her classification test because the <b>classifiers</b> said the eyesight in her right eye was 0.01% better than it needed to be. In November 2009, her classification was revisited due to deterioration of her vision and she was deemed eligible to compete.|$|E
5|$|Khmer is {{primarily}} an analytic, isolating language. There are no inflections, conjugations or case endings. Instead, particles and auxiliary words {{are used to}} indicate grammatical relationships. General word order is subject–verb–object, and modifiers follow the word they modify. <b>Classifiers</b> appear after numbers when used to count nouns, though not always so consistently as in languages like Chinese. In spoken Khmer, topic-comment structure is common and the perceived social relation between participants determines which sets of vocabulary, such as pronouns and honorifics, are proper.|$|E
5|$|Northern dialects tend to {{have fewer}} <b>classifiers</b> than {{southern}} ones. 個 ge is the only classifier found in the Dungan language. All nouns could have just one classifier in some dialects, such as Shanghainese (Wu), the Mandarin dialect of Shanxi, and Shandong dialects. Some dialects such as Northern Min, certain Xiang dialects, Hakka dialects, and some Yue dialects use 隻 for the noun referring to people, rather than 個.|$|E
40|$|This article {{describes}} {{the construction of the}} local logistic <b>classifier,</b> a highly scalable and parallelizable <b>classifier</b> combining local learning with a logistic <b>classifier.</b> To speed up the learning of each local logistic <b>classifier,</b> a novel fixed point algorithm is introduced. The practicality and performance of the <b>classifier</b> is demonstrated on a 10 million character training set derived from the UW 3 dataset, and the <b>classifier</b> is demonstrated as part of an OCR system. The <b>classifier</b> is also compared to another proposed large scale classification method, GURLS [7], in experiments on ImageNet dataset. ...|$|R
40|$|The common {{operation}} mechanism of multiple <b>classifier</b> systems {{is the combination}} of <b>classifier</b> outputs. Some researchers have pointed out the potentialities of “dynamic <b>classifier</b> selection” as an alternative operation mechanism. However, such potentialities have been motivated so far by experimental results and qualitative arguments. This paper provides a theoretical framework for dynamic <b>classifier</b> selection. To this end, dynamic <b>classifier</b> selection {{is placed in the}} general framework of statistical decision theory and it is showed that, under some assumptions, the optimal Bayes <b>classifier</b> can be obtained by the selection of non-optimal <b>classifier...</b>|$|R
5000|$|AdaBoost {{refers to}} a {{particular}} method of training a boosted <b>classifier.</b> A boost <b>classifier</b> is a <b>classifier</b> in the form ...|$|R
25|$|The {{construction}} of possessive <b>classifiers</b> depends on ownership, temporality, degrees of control, locative associations, and status. In addition to status-rising and status-lowering possessive <b>classifiers,</b> {{there are also}} common (non-status marked) possessive <b>classifiers.</b> Status-rising and status-lowering possessive <b>classifiers</b> have different properties of control and temporality. Common possessive <b>classifiers</b> are divided into three main categories – relatives, personal items, and food/drink.|$|E
25|$|One way {{in which}} many sign {{languages}} {{take advantage of the}} spatial nature of the language is through the use of <b>classifiers.</b> <b>Classifiers</b> allow a signer to spatially show a referent's type, size, shape, movement, or extent.|$|E
25|$|Below {{are some}} of the most {{commonly}} used <b>classifiers</b> in Burmese.|$|E
30|$|We {{introduced}} the <b>classifier</b> pool in this paper. By saving the <b>classifier</b> which corresponds to different concepts, the system model {{can choose the}} appropriate <b>classifier</b> from the <b>classifier</b> pool directly and has no need to retrain when recurring concept appears again.|$|R
30|$|To {{test the}} <b>classifier,</b> {{and the overall}} system, we ran {{the items in the}} test set through the <b>classifier,</b> and {{compared}} the decision tree’s classification to the expected classification from the human judgements. This conforms to standard test approaches to machine learning applications (Bishop 2006). From the test runs, we created a confusion matrix for the <b>classifier</b> and in turn determined the <b>classifier’s</b> accuracy. We also calculated the standard error of the <b>classifier,</b> and thus gave a confidence interval for the <b>classifier’s</b> accuracy.|$|R
5000|$|Classification: Perceptron, SGD <b>classifier,</b> Naive bayes <b>classifier.</b>|$|R
25|$|SVMs {{belong to}} a family of {{generalized}} linear <b>classifiers</b> and {{can be interpreted as}} an extension of the perceptron. They can also be considered a special case of Tikhonov regularization. A special property is that they simultaneously minimize the empirical classification error and maximize the geometric margin; hence they are also known as maximum margin <b>classifiers.</b>|$|E
25|$|Finite state {{machines}} can be subdivided into transducers, acceptors, <b>classifiers</b> and sequencers.|$|E
25|$|The <b>classifiers</b> can {{sometimes}} be used {{in place of the}} nouns they group in context.|$|E
40|$|This report {{describes}} the fuzzy <b>classifier</b> {{system and a}} new payoff distribution scheme that performs true reinforcement learning. The fuzzy <b>classifier</b> system is a crossover between learning <b>classifier</b> systems and fuzzy logic controllers. By the use of fuzzy logic, the fuzzy <b>classifier</b> system allows for variables to take continuous values, and thus, {{could be applied to}} the identification and control of continuous dynamic systems. The fuzzy <b>classifier</b> system adapt the mechanics of learning <b>classifier</b> system to fuzzy logic to evolve sets of coadapted fuzzy rules. The payoff distribution scheme presented here opens the way {{for the use of the}} fuzzy <b>classifier</b> system in control tasks. Additionally, other mechanisms that improve learning speed are presented. Keywords Learning <b>classifier</b> systems, fuzzy logic, fuzzy logic control, reinforcement learning. Reinforcement Learning in the Fuzzy <b>Classifier</b> System January, 1997 1 Introduction Automatic control is a mature field with a vast litera [...] ...|$|R
30|$|In this article, {{a hybrid}} {{recognition}} method {{has been proposed}} to recognize radar emitter signals. The hybrid <b>classifier</b> consists of a rough k-means <b>classifier</b> (linear <b>classifier)</b> and a SVM (nonlinear <b>classifier).</b> Based on the linear separability of the classifying sample, the sample is classified by the suitable <b>classifier.</b> Thus for the radar emitter sample set containing both linearly separable samples and linearly inseparable samples, the approach can achieve a higher accuracy.|$|R
40|$|A {{strategy}} to evaluate an expert system is formulated. The strategy proposed {{is based on}} finding an equivalent <b>classifier</b> to an expert system and evaluate that <b>classifier</b> with respect to an optimal <b>classifier,</b> a Bayes <b>classifier.</b> Here it is shown that for the rules considered an equivalent <b>classifier</b> exists. Also, a brief consideration of meta and meta-meta rules is included. Also, a taxonomy of expert systems is presented and an assertion made that an equivalent <b>classifier</b> exists {{for each type of}} expert system in the taxonomy with associated sets of underlying assumptions...|$|R
25|$|A {{comparison}} of the SVM to other <b>classifiers</b> has been made by Meyer, Leisch and Hornik.|$|E
25|$|Support vector {{machines}} and other much simpler methods, such as linear <b>classifiers,</b> gradually overtook neural networks in machine learning popularity.|$|E
25|$|Lin, H., Lee, S., Bui, N. and Honavar, V. (2013). Learning <b>Classifiers</b> from Distributional Data. In: IEEE Big Data Congress.|$|E
40|$|In Slovenian Sign Language (SZJ), <b>classifier</b> {{predicate}} {{cannot be}} negated {{and thus it}} does not qualify {{as the head of}} a verb phrase. Such a conclusion does not {{rule out the possibility that}} SZJ <b>classifier</b> predicate projects a reduced clausal structure. I analyze these SZJ <b>classifier</b> predicates as non-verbal predicates that form a small-clause structure assuming that <b>classifier</b> small clause is selected by an overt (HAVE) or a covert verbal head. This proposal explains the complexity of <b>classifier</b> predicates. Being a non-verbal projection, <b>classifier</b> predicate fails to move with a verbal V-to-T movement and stays in situ. For SVO languages such as SZJ, this analysis correctly predicts the change from the basic SVO to the non-basic SOV for transitive <b>classifier</b> predicates and from the basic SVOdOi to the non-basic SOdVOi for ditransitive <b>classifier</b> predicates...|$|R
40|$|The nearest {{shrunken}} centroid <b>classifier</b> uses shrunken centroids as prototypes {{for each}} class and test samples are classified {{to belong to}} the class whose shrunken centroid is nearest to it. In our study, the nearest shrunken centroid <b>classifier</b> was used simply to select important genes prior to classification. Random Forest, a decision tree based classification algorithm, is chosen as a <b>classifier</b> to seven cancer microarray data for correct diagnosis. Classification was also performed using the nearest shrunken centroid <b>classifier</b> and its results are compared to those from random Forest. Our study demonstrates that the nearest shrunken centroid <b>classifier</b> is simple, yet efficient in selecting important genes, but does not perform well as a <b>classifier.</b> We report that performance of Random Forest as a <b>classifier</b> is far superior to that of Shrunken centroid <b>classifier...</b>|$|R
30|$|The best {{performance}} of the logistic regression <b>classifier</b> is achieved with review data set of kitchen domain (88.47) when the combined (IG, CHI, GI) method is being used. Naïve Bayes <b>classifier</b> is quite convenient for small datasets as it is effortless to implement and very swift to train, but the <b>classifier</b> LR and SVM produce overall better performance for large dataset. In particular, unlike the NB <b>classifier</b> LR is capable of handling dependent features, while NB <b>classifier</b> {{is based on the}} independent assumption. In this case, LR underperformed SVM and MNB <b>classifier</b> for the datasets, such as: Movie, Electronics and outperformed RF <b>classifier</b> for most of the review datasets with combined feature selection method.|$|R
25|$|Lin, H. and Honavar, V. (2013). Learning <b>Classifiers</b> from Chains of Multiple Interlinked RDF Data Stores. In: IEEE Big Data Congress. Best Student Paper Award.|$|E
25|$|El-Manzalawy, Y., Dobbs, D., and Honavar, V. (2012). Predicting {{protective}} bacterial antigens using {{random forest}} <b>classifiers..</b> ACM Conference on Bioinformatics and Computational Biology pp.426–433, 2012.|$|E
25|$|Zhang, J. and Honavar, V. (2003). Learning Decision Tree <b>Classifiers</b> from Attribute Value Taxonomies and Partially Specified Data. In: Proceedings of the International Conference on Machine Learning (ICML-03).|$|E
50|$|<b>Classifier</b> {{performance}} depends greatly on {{the characteristics}} of the data to be classified. There is no single <b>classifier</b> that works best on all given problems (a phenomenon that may be explained by the no-free-lunch theorem). Various empirical tests have been performed to compare <b>classifier</b> performance and to find the characteristics of data that determine <b>classifier</b> performance. Determining a suitable <b>classifier</b> for a given problem is however still more an art than a science.|$|R
5000|$|The linear <b>classifier</b> {{for this}} {{support vector machine}} <b>classifier</b> is, ...|$|R
5000|$|Khasi has a <b>classifier</b> system, {{apparently}} {{used only}} with numerals. Between the numeral and noun, the <b>classifier</b> tylli {{is used for}} non-humans, and the <b>classifier</b> ngut is used for humans, e.g.|$|R
