0|4762|Public
5000|$|A {{small number}} of <b>common</b> <b>function</b> <b>words</b> (the Middle English {{anomalies}} mentioned below) begin with [...] The words in this group are: ...|$|R
5000|$|Many content {{words would}} be homographs of <b>common</b> <b>function</b> <b>words</b> {{if not for}} the latters [...] "redundant" [...] letters: e.g. be/bee, in/inn, I/eye, to/two.Otto Jespersen, {{describing}} the phenomenon in 1909, suggested the short spelling was a marker of reduced stress. Content words always have at least one stressed syllable, whereas <b>function</b> <b>words</b> are often completely unstressed; shorter spellings help to reflect this. (Interjections such as ah, eh, lo, yo are always stressed. Punctuation serves to isolate these elements.) ...|$|R
5000|$|In early Middle English times, a {{group of}} very <b>common</b> <b>function</b> <b>words</b> {{beginning}} with [...] (the, they, there, etc.) came to be pronounced with [...] instead of [...] Possibly this was a sandhi development; as these words are frequently found in unstressed positions, they can sometimes appear to run on from the preceding word, which may {{have resulted in the}} dental fricative being treated as though it were word-internal.|$|R
40|$|The missing-letter effect {{refers to}} the {{phenomenon}} that letters {{are more difficult to}} detect in <b>common</b> <b>function</b> <b>words</b> (such as the) than in content words. Assuming that the missing-letter effect is diagnostic of the extraction of text structure, we exploited a special feature of German [...] the convention to capitalize the initial letter of nouns. Given the great flexibility of word order in German, it was proposed that this convention might help readers specify the structure of the sentence. Therefore orthographic variations that violate the capitalization rules should disrupt structure extraction and should result in a reduced missing-letter effect. The results indicated that: 1) capitalization of <b>function</b> <b>words</b> eliminated the missing-letter effect, but not {{at the beginning of a}} sentence; 2) A missing-letter effect occurred when the capitalization of the first letter was correct, but was followed by typecase alternation, and also when the size of the initial letters was relatively large for <b>function</b> <b>words,</b> but relatively small for content words. The results were discussed with respect to the possible contributions of visual familiarity, structural role, and processing time to the missing-letter effect, taking into account that a capitalized initial letter conveys significant information about the word class for German readers. Thus, the present results indicate that readers take advantage not only of <b>function</b> <b>words</b> but of any other information (here the capitalization of nouns) that helps to extract the structure of a sentence...|$|R
5000|$|Any {{group of}} words can be {{chosen as the}} stop words for a given purpose. For some search engines, {{these are some of}} the most <b>common,</b> short <b>function</b> <b>words,</b> such as the, is, at, which, and on. In this case, stop words can cause {{problems}} when searching for phrases that include them, particularly in names such as [...] "The Who", [...] "The The", or [...] "Take That". Other search engines remove some of the most common words—including lexical words, such as [...] "want"—from a query in order to improve performance.|$|R
40|$|In this work, {{we suggest}} a {{parameterized}} statistical model (the gamma distribution) for {{the frequency of}} word occurrences in long strings of English text and use this model to build a corresponding thermodynamic picture by constructing the partition function. We then use our partition function to compute thermodynamic quantities such as the free energy and the specific heat. In this approach, {{the parameters of the}} word frequency model vary from word to word so that each word has a different corresponding thermodynamics and we suggest that differences in the specific heat reflect differences in how the words are used in language, differentiating keywords from <b>common</b> and <b>function</b> <b>words.</b> Finally, we apply our thermodynamic picture to the problem of retrieval of texts based on keywords and suggest some advantages over traditional information retrieval methods. Comment: 12 pages, 7 figure...|$|R
5000|$|Since {{there is}} no need to mark the {{stressed}} syllable of a monosyllabic word, most of them do not have an accent. Exceptions to this are those with a diacritical accent that differentiates some cases of words that would otherwise be homographic. Example: es [...] (it impersonal) vs és [...] (is), te [...] (you clitic) vs té [...] (s/he has), mes [...] (month) vs més [...] (more), dona [...] (woman) vs dóna [...] (s/he gives). In most cases, the word bearing no accent is either unstressed (as in the case of es and te), or the word without the accent is more <b>common,</b> usually a <b>function</b> <b>word.</b>|$|R
40|$|Abstract. We explore {{some of the}} {{properties}} of a subposet of the Tamari lattice introduced by Pallo, which we call the comb poset. We show that three binary functions that are not well-behaved in the Tamari lattice are remarkably well-behaved within an interval of the comb poset: rotation distance, meets and joins, and the <b>common</b> parse <b>words</b> <b>function</b> {{for a pair of}} trees. We relate this poset to a partial order on the symmetric group studied by Edelman. 1...|$|R
40|$|We explore {{some of the}} {{properties}} of a subposet of the Tamari lattice introduced by Pallo, which we call the comb poset. We show {{that a number of}} binary functions that are not well-behaved in the Tamari lattice are remarkably well-behaved within an interval of the comb poset: rotation distance, meets and joins, and the <b>common</b> parse <b>words</b> <b>function</b> for a pair of trees. We relate this poset to a partial order on the symmetric group studied by Edelman. National Institutes of Health (U. S.) (NSF grant DMS- 1001933...|$|R
40|$|The {{success of}} support vector {{machines}} (SVMs) for classification problems is often dependent on an appropriate normalization of the input feature space. This {{is particularly true}} in topic identification, where the relative contribution of the <b>common</b> but uninformative <b>function</b> <b>words</b> can overpower {{the contribution of the}} rare but informative content words in the SVM kernel function score if the feature space is not normalized properly. In this paper we apply the discriminative minimum classification error (MCE) training approach to the problem of learning an appropriate feature space normalization for use with an SVM classifier. Results are presented showing significant error rate reductions for an SVM-based system on a topic identification task using the Fisher corpus of audio recordings of humanhuman conversations. Index Terms: topic identification, topic spotting, MCE training, support vector machine...|$|R
40|$|Abstract. In {{this paper}} we explore some of the {{properties}} of the comb poset, whose notion was first introduced by J. M. Pallo. We show that three binary functions that are not well-behaved in the Tamari lattice are remarkably well-behaved within an interval of the comb poset: rotation distance, meets and joins, and the <b>common</b> parse <b>words</b> <b>function</b> for a pair of trees. We conclude by giving explicit expressions for the number of common parse words for a pair of trees within an interval of the comb poset, a problem whose generalization is known to be equivalent to the Four Color theorem. 1...|$|R
50|$|Words {{that are}} not <b>function</b> <b>words</b> are called content words (or open class words or lexical words or autosemantic words): these include nouns, verbs, adjectives, and most adverbs, {{although}} some adverbs are <b>function</b> <b>words</b> (e.g., then and why). Dictionaries define the specific meanings of content words, but can only describe the general usages of <b>function</b> <b>words.</b> By contrast, grammars describe the use of <b>function</b> <b>words</b> in detail, but treat lexical words in general terms only.|$|R
40|$|A {{large number}} of {{auditory}} studies explored the role of <b>function</b> <b>words</b> in syntactic processing, but few researched <b>function</b> <b>words</b> in written input. The present study probed the role of <b>function</b> <b>words</b> in word skipping, sentence compacting, chunking preference and the detection mechanism of grammatical incongruence by means of number estimation across 4 syntactic conditions (grammatical sentences, scrambled sentences, sentences with agreement errors and sentences with structural errors), 3 sentence lengths (6 or 7 words, 8 or 9 words, 10 or 11 words) and 3 ratios of <b>function</b> <b>words</b> and content words. We find that appropriate usage of <b>function</b> <b>words</b> highly facilitates syntactic analysis though <b>function</b> <b>words</b> are always skipped in proficient reading. Adjacent <b>function</b> <b>words</b> and content words in grammatical sentences {{are more likely to}} be processed as chunks, and this effect of chunking make sentences significantly more compact than scrambled sentences. In addition, the detection mechanism of grammatical incongruence is attributed to resolving the conflicts with prediction...|$|R
50|$|<b>Function</b> <b>words</b> {{might be}} prepositions, pronouns, {{auxiliary}} verbs, conjunctions, grammatical articles or particles, {{all of which}} belong {{to the group of}} closed-class words. Interjections are sometimes considered <b>function</b> <b>words</b> but they belong to the group of open-class <b>words.</b> <b>Function</b> <b>words</b> might or might not be inflected or might have affixes.|$|R
40|$|This study {{investigates the}} <b>function</b> <b>word</b> deficits in aphasic {{patients}} and, in particular, agrammatic Broca's aphasics. Several {{explanations for the}} <b>function</b> <b>word</b> problem are addressed including a <b>function</b> <b>word</b> vocabulary deficit theory, a general syntactic deficit theory, and an abstract word deficit theory. Subjects {{with varying degrees of}} agrammatism were tested on a variety of production, comprehension, and reading, and syntactic tests which isolate semantic and syntactic aspects of both <b>function</b> and content <b>words</b> in order to better define the nature of the <b>function</b> <b>word</b> deficits in agrammatism...|$|R
40|$|International audienceWe {{discuss some}} {{properties}} of a subposet of the Tamari lattice introduced by Pallo (1986), {{which we call}} the comb poset. We show that three binary functions that are not well-behaved in the Tamari lattice are remarkably well-behaved within an interval of the comb poset: rotation distance, meets and joins, and the <b>common</b> parse <b>words</b> <b>function</b> {{for a pair of}} trees. We relate this poset to a partial order on the symmetric group studied by Edelman (1989). Nous discutons d'un subposet du treillis de Tamari introduit par Pallo. Nous appellons ce poset le comb poset. Nous montrons que trois fonctions binaires qui ne se comptent pas bien dans le trellis de Tamari se comptent bien dans un intervalle du comb poset : distance dans le trellis de Tamari, le supremum et l'infimum et les parsewords communs. De plus, nous discutons un rapport entre ce poset et un ordre partiel dans le groupe symétrique étudié par Edelman...|$|R
50|$|All {{words can}} be {{classified}} as either content or <b>function</b> <b>words,</b> {{although it is not}} always easy to make the distinction. With only around 150 <b>function</b> <b>words,</b> 99.9% of words in the English language are content words. Although small in numbers, <b>function</b> <b>words</b> are used at a disproportionately higher rate and make up about 50% of any English text. This is due to the conventional patterns of words usage which bind <b>function</b> <b>words</b> to content words almost every time they are used, creating an interdependence between the two word groups.|$|R
50|$|Below {{are rules}} {{about the number}} of n’s in <b>function</b> <b>words,</b> which do not decline nor conjugate. Various <b>function</b> <b>words</b> which {{indicate}} movement end with -an and never -ann.|$|R
40|$|International audienceAuthorship {{attribution}} is {{the task}} of identifying {{the author of a}} given document. Various style markers have been proposed in the literature to deal with the authorship attribution task. Frequencies of <b>function</b> <b>words</b> {{have been shown to be}} very reliable and effective for this task. However, despite the fact that they are state-of-the-art, they basically rely on the invalid bag-of-words assumption, which stipulates that text is a set of independent words. In this contribution, we present a comparative study on using two different types of style marker based on <b>function</b> <b>words</b> for authorship attribution. We compare the effectiveness of using sequential rules of <b>function</b> <b>words</b> as style marker that do not relay on the bag-of-words assumption to that of the frequency of <b>function</b> <b>words</b> which does. Our results show that the frequencies of <b>function</b> <b>words</b> outperform the sequential rules...|$|R
40|$|One of {{the main}} issues in a word {{alignment}} task is the difficulty of handling <b>function</b> <b>words</b> {{that do not have}} direct translations which we call unique <b>function</b> <b>words.</b> They are often aligned to some words in the other language incorrectly. This is prominent in language pairs with very different sentence structures. In this paper, we propose a novel approach for handling unique <b>function</b> <b>words.</b> The proposed model monolingually derives unique <b>function</b> <b>words</b> from bilin-gually generated treelet pairs. The monolingual derivation prevents incorrect alignments for unique <b>function</b> <b>words.</b> The derivation probabilities are estimated from a large monolingual corpus, which is much easier to acquire than a parallel corpus. Also, the proposed alignment model uses semantic-head dependency trees where dependency relations between words be-come similar in each language. Experimental results on an English-Japanese corpus show that the proposed model achieves better alignment and translation quality compared with the baseline models...|$|R
40|$|The {{generation}} of precise and comprehensible translations {{is still a}} challenge in the patent and scientific domain. In particular, <b>function</b> <b>words</b> are often poorly translated in standard machine translation systems, particularly across language pairs with greatly differing syntax. In this paper we exploit the target-side structure in tree-to-tree machine translation to post-edit <b>function</b> <b>words</b> automatically using a tree-based <b>function</b> <b>word</b> language model. We show that a significant improvement in human evaluation can be achieved with our proposed method. ...|$|R
40|$|In {{statistical}} word alignment for machine translation, <b>function</b> <b>words</b> usually cause poor aligning performance {{because they}} do not have clear correspondence between different languages. This paper proposes a novel approach to improve word alignment by pruning alignments of <b>function</b> <b>words</b> from an existing alignment model with high precision and recall. Based on monolingual and bilingual frequency characteristics, a language-independent <b>function</b> <b>word</b> recognition algorithm is first proposed. Then a group of carefully defined syntactic structures combined with content word alignments are used for further <b>function</b> <b>word</b> alignment pruning. The experimental results show that the proposed approach improves both the quality of word alignment and the performance of statistical machine translation on Chinese-to-English, Germanto-English and French-to-English language pairs. ...|$|R
40|$|We {{describe}} a simple improvement to n-gram language models where we estimate the distribution over closed-class (<b>function)</b> <b>words</b> {{separately from the}} conditional distribution of open-class <b>words</b> given <b>function</b> <b>words.</b> In English, <b>function</b> <b>words</b> account for about 30 % of written language, and also form a natural skeleton for most sentences. By factoring a language model into a <b>function</b> <b>word</b> model and a conditional model over open-class <b>words</b> given <b>function</b> <b>words,</b> we largely avoid the problem of sparse training data in the first phase, and localize the need for sophisticated smoothing techniques primarily to the second conditional model. We test our factored approach on the Brown and Wall Street Journal corpora and observe a 3. 5 % to 25. 2 % improvement in perplexity over standard methods, depending on the particular smoothing method and test set used. Compared to other proposals for improving n-gram language models, our factorization {{has the advantage of}} inherent simplicity and efficiency, and improves generalization between data sets...|$|R
40|$|In a {{regression}} study of conversational speech, {{we show that}} frequency, contextual predictability and repetition have separate contributions to word duration, despite their substantial correlations. Moreover, content- and function-word durations are affected differently by their frequency and predictability. Content words are shorter when more frequent, and shorter when repeated, while <b>function</b> <b>words</b> are not so affected. <b>Function</b> <b>words</b> have shorter pronunciations, after controlling for frequency and predictability. While both content and <b>function</b> <b>words</b> are strongly affected by predictability from the word following them, sensitivity to predictability from the preceding word is largely limited to very frequent <b>function</b> <b>words.</b> The results {{support the view that}} content and <b>function</b> <b>words</b> are accessed differently in production. We suggest a lexical-access-based model of our results, in which frequency or repetition lead to shorter or longer word durations by causing faster or slower lexical access, mediated by a general mechanism that coordinates the pace of higher-level planning and the execution of the articulatory plan...|$|R
40|$|In {{the present}} paper, we propose the {{effective}} usage of <b>function</b> <b>words</b> to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules {{that account for}} multiple interpretations of both aligned and unaligned target <b>function</b> <b>words.</b> In order to constrain the exhaustive attachments of <b>function</b> <b>words,</b> we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target <b>function</b> <b>words</b> during decoding. Extensive experiments involving large-scale English-to-Japanese translation revealed a significant improvement of 1. 8 points in BLEU score, as compared with a strong forest-to-string baseline system. ...|$|R
5000|$|... lèih/làih (v. come, {{sometimes}} <b>function</b> <b>word)</b> Standard Chinese: ...|$|R
40|$|In this paper, we {{show some}} {{properties}} of <b>function</b> <b>words</b> in dependency trees. <b>Function</b> <b>words</b> are grammatical words, such as articles, prepositions, pronouns, conjunctions, or auxiliary verbs. These words are often short and very frequent in texts and therefore {{many of them}} can be easily recognized. We formulate a hypothesis that <b>function</b> <b>words</b> tend to have a fixed number of dependents and we prove this hypothesis on treebanks. Using this hypothesis, we are able to improve unsupervised dependency parsing and outperform previously published state-of-the-art results for many languages...|$|R
5000|$|Grammatical words, as a class, {{can have}} {{distinct}} phonological properties from content words. Grammatical words sometimes {{do not make}} full use of all the sounds in a language. For example, {{in some of the}} Khoisan languages, most content words begin with clicks, but very few <b>function</b> <b>words</b> do. In English, very few <b>words</b> other than <b>function</b> <b>words</b> begin with voiced th [...] (see Pronunciation of English th); English <b>function</b> <b>words</b> may have fewer than three letters 'I', 'an', 'in' while non-function words usually have three or more 'eye', 'Ann', 'inn' (see three letter rule).|$|R
40|$|Inspired by {{experimental}} psychological findings {{suggesting that}} <b>function</b> <b>words</b> play a special role in word learning, {{we make a}} simple modification to an Adaptor Grammar based Bayesian word segmentation model {{to allow it to}} learn sequences of monosyllabic "function words" at the beginnings and endings of collocations of (possibly multi-syllabic) words. This modification improves unsupervised word segmentation on the standard Bernstein- Ratner (1987) corpus of child-directed English by more than 4 % token f-score compared to a model identical except that it does not special-case "function words, setting a new state-of-the-art of 92. 4 % token f-score. Our <b>function</b> <b>word</b> model assumes that <b>function</b> <b>words</b> appear at the left periphery, and while this is true of languages such as English, it is not true universally. We show that a learner can use Bayesian model selection to determine the location of <b>function</b> <b>words</b> in their language, even though the input to the model only consists of unsegmented sequences of phones. Thus our computational models support the hypothesis that <b>function</b> <b>words</b> play a special role in word learning. 11 page(s...|$|R
5000|$|Other <b>function</b> <b>words</b> (...) are {{separated}} from other words, including: ...|$|R
5000|$|Unstressed: {{unstressed}} syllables of polysyllabic words; monosyllabic <b>function</b> <b>words.</b>|$|R
50|$|Below {{are some}} Mantauran Rukai <b>function</b> <b>words</b> from Zeitoun (2007).|$|R
5000|$|... ge (genitive, {{similar to}} 's; {{sometimes}} <b>function</b> <b>word)</b> Standard Chinese: ,, ...|$|R
40|$|<b>Function</b> <b>words</b> are lexical {{units with}} {{generally}} lit-tle semantic weight that often {{play a role}} of “gram-matical ” elements in a sentence, introducing or modifying content words. These include prepo-sitions (with), determiners (some), pronouns (she) and conjunctions (furthermore). Complex <b>function</b> <b>words</b> are <b>function</b> <b>words</b> made up of several to-kens, like complex prepositions (in front of), de-terminers (a lot of) and conjunctions (as long as). This abstract discusses the representation and detection of ADV+que constructions, a type of complex conjunction in French. These construc-tions are formed by adverbs like bien (well) or ainsi (likewise) followed by subordinative con-junction que (which) ...|$|R
50|$|The list of <b>function</b> <b>words</b> {{below is}} sourced from Adelaar (1997).|$|R
5000|$|<b>Function</b> <b>words</b> help in modifying meaning {{considered}} the following sentence - ...|$|R
