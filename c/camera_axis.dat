49|105|Public
25|$|No images {{provide a}} {{vertical}} view; in fact, the smallest angle between the planetary surface normal and the <b>camera</b> <b>axis</b> is about 50°. The high obliquity of the images, {{the wide range}} in sun-elevation angles, and the complete transection of the quadrangle by the gap in coverage greatly hamper geologic mapping. Only in about 15 percent of the quadrangle, near the southeast corner, do data permit separation of units with the confidence possible in other quadrangles on Mercury.|$|E
50|$|Moore-Brabazon also {{pioneered the}} {{incorporation}} of stereoscopic techniques into aerial photography, allowing the height of objects on the landscape to be discerned by comparing photographs taken at different angles. In 1916, the Austro-Hungarian Empire made vertical <b>camera</b> <b>axis</b> aerial photos above Italy for map-making.|$|E
50|$|No images {{provide a}} {{vertical}} view; in fact, the smallest angle between the planetary surface normal and the <b>camera</b> <b>axis</b> is about 50°. The high obliquity of the images, {{the wide range}} in sun-elevation angles, and the complete transection of the quadrangle by the gap in coverage greatly hamper geologic mapping. Only in about 15 percent of the quadrangle, near the southeast corner, do data permit separation of units with the confidence possible in other quadrangles on Mercury.|$|E
50|$|Axis Communications {{develops}} {{and sells}} network cameras for many applications. Products include PTZ, vandal resistant, thermal, outdoor, HDTV, wireless, motion detection and progressive scan cameras. It introduced the industry's first thermal network <b>camera,</b> the <b>AXIS</b> Q1910, in January 2010 and the industry's first HDTV network <b>camera,</b> the <b>AXIS</b> Q1755, in December 2008.|$|R
5000|$|... #Caption: A modern (P13 series) network <b>camera</b> from <b>Axis</b> Communications, circa 2013 ...|$|R
30|$|For {{our first}} round of experiments, sample ROS 001 was {{placed in front of}} the focused laser beam. The angle of {{incidence}} (AOI) of the laser beam with the plate’s surface, i.e., the angle between the incident beam on the surface and the line perpendicular to the surface at the point of incidence, was 0 ° while the <b>camera’s</b> optical <b>axis</b> AOI was about 15 °, i.e., the angle between the <b>camera</b> optical <b>axis</b> and the plate’s normal to the surface.|$|R
50|$|Frederick Charles Victor Laws started aerial {{photography}} {{experiments in}} 1912 with No.1 Squadron of the Royal Flying Corps (later No. 1 Squadron RAF), taking {{photographs from the}} British dirigible Beta. He discovered that vertical photos taken with 60% overlap {{could be used to}} create a stereoscopic effect when viewed in a stereoscope, thus creating a perception of depth that could aid in cartography and in intelligence derived from aerial images. The Royal Flying Corps recon pilots began to use cameras for recording their observations in 1914 and by the Battle of Neuve Chapelle in 1915, the entire system of German trenches was being photographed. In 1916 the Austro-Hungarian Monarchy made vertical <b>camera</b> <b>axis</b> aerial photos above Italy for map-making.|$|E
30|$|The {{first step}} of {{preprocessing}} is to detect the position and orientation of human face. Geometric transformations are used to “turn” the human face to directly against the <b>camera</b> <b>axis.</b> Then the preprocessing uses the help from clearly identifiable facial parts such as nose to isolate the human face area out from areas of the distracting features. This operation is called segmentation.|$|E
40|$|A photogrammetric flow {{visualisation}} {{technique used}} in a field experiment to study air motions around a ridge of high ground is described. Smoke generators fixed to helium filled balloons leave a smoke trail as the balloon rises. The smoke elements are photographed with two cine cameras. The analysis of the trajectories of smoke elements is described for general orientations of <b>camera</b> <b>axis,</b> thus allowing the three dimensional picture of the flow behaviour to be determined...|$|E
50|$|In 1996, Axis Communications {{introduced}} the industry's first network <b>camera,</b> the <b>AXIS</b> 200. This was followed in 1999 by the AXIS 2100 {{which was the}} first volume product using an embedded Linux. In 2004, the company {{introduced the}} AXIS 206, the then smallest network camera.|$|R
40|$|The {{experimental}} results were obtained with a strategy using motion {{about the three}} camera axes for visual tracking, motion along the three axes for singularity/joint limit avoidance, and motion along the <b>camera’s</b> optical <b>axis</b> (Z) for increasing spatial resolution and maintaining focus. Two features on the target object were tracked so that the position and orientation of the object on the image plane could be maintained as the target moved, subject to the other sensor placement criteria. The object being tracked moved 25 cm in a direction parallel to the <b>camera’s</b> X <b>axis.</b> Without sensor placement criteria, {{the depth of the}} object should increase as illustrated by the dashed line in Fig. 2. However, the spatial resolution constraint causes the depth to actually decrease, until th...|$|R
5000|$|The first {{centralized}} IP <b>camera,</b> the <b>Axis</b> Neteye 200, {{was released}} in 1996 by Axis Communications and {{was developed by the}} team of Martin Gren and Carl-Axel Alm. The camera was not capable of streaming real-time motion video, instead being limited to showing a snapshot image each time the camera was accessed. At the time of launch, it was considered to be incapable of operating as a motion camera due to what would conceived as [...] "enormous" [...] bandwidth requirements, thus was aimed primarily at the tourism industry. The Axis Neteye 200 was not intended to replace traditional analogue CCTV systems however, given it's capability was limited to just a single frame every 17 seconds, yet was promoted on its ability to be directly accessible from anywhere with a connection to the internet. Axis used a custom proprietary web server named OSYS, yet by the summer of 1998, had started working on a software port towards Linux to operate its <b>cameras.</b> <b>Axis</b> also released documentation for its low-level API called [...] "VAPIX", which builds on the open standards of HTTP and real time streaming protocol (RTSP). This open architecture was intended to encourage third-party software manufacturers to develop compatible management and recording software.|$|R
40|$|A full {{three-dimensional}} particle {{tracking system}} {{was developed and}} tested. By using three separate CCDs placed at the vertices of an equilateral triangle, the three-dimensional location of particles can be determined. Particle locations measured at two different times can then {{be used to create}} a three-component, three-dimensional velocity field. Key developments are: the ability to accurately process overlapping particle images, offset CCDs to significantly improve effective resolution, allowance for dim particle images, and a hybrid particle tracking technique ideal for three-dimensional flows when only two sets of images exist. An in-depth theoretical error analysis was performed which gives the important sources of error and their effect on the overall system. This error analysis was verified through a series of experiments, which utilized a test target with 100 small dots per square inch. For displacements of 2. 54 mm the mean errors were less than 2 % and the 90 % confidence limits were less than 5. 2 μm in the plane perpendicular to the <b>camera</b> <b>axis,</b> and 66 μm {{in the direction of the}} <b>camera</b> <b>axis.</b> The system was used for flow measurements around a delta wing at an angl...|$|E
40|$|In this paper, amonocularapproach for {{generation}} of the depthmaps of an object using computer vision is proposed. In this technique the object is immersed in a colour liquid and a single plan view is captured by a CCD camera. Due to the absorption property of the colour liquid, {{the intensity of the}} pixels of the captured image is modulated by the depths of the object along the <b>camera</b> <b>axis.</b> The depthmaps are generated from the grey level image by consecutive segmentation of the image into binary images at various thresholds. The depthmaps produced are parallel layers located in planes normal to the <b>camera</b> <b>axis.</b> The 2 -D features of the contours contained in each layer are derived. The features of the layers are then combined to produce the 3 -D model of the object. In order to employ the technique for object recognition a matching algorithm based on 3 -D features of the object is developed. The technique is then verified by an experiment to recognize a set of objects. The approach can be used as long as the object is not damaged when placed in the colour liquid...|$|E
40|$|The {{results of}} the lunar roving vehicle/traverse {{gravimeter}} experiment motion sensitivity test shows that the gravity measurements in both the normal and bypass modes should not be adversely affected by motion induced in the lunar roving vehicle by operation of the television camera position drive device or {{the operation of the}} surface electrical properties receiver/recorder. Motion of the traverse gravimeter experiment occurred when a 1. 4 -hertz resonant mode in pitch of the pallet was excited. Both of these modes were excited by camera elevation changes with the <b>camera</b> <b>axis</b> positioned fore and aft...|$|E
50|$|In 1984, Gren founded Axis Communications, {{together}} with Mikael Karlsson and Keith Bloodworth, {{a company that}} initially developed and sold print servers, but which later came to be a world leader in network video. In 1996 Gren invented the first network <b>camera,</b> the <b>AXIS</b> 200, NetEye, {{together with}} Carl-Axel Alm.|$|R
50|$|On 10 February 2015, Canon {{announced}} that it had intentions to buy Swedish Security <b>Camera</b> maker <b>Axis</b> Communications for $2.83 billion. On 23 February 2015, Axis Communications reacted to this news and confirmed that it had received a purchase proposal from Canon. The purchase was effectively completed in April 2015.|$|R
50|$|On July 18th, 2017, Security {{researchers}} published that <b>Axis</b> <b>cameras</b> have a bug {{that allows}} any attacker {{to take full}} control of the cameras.|$|R
40|$|The Surface-Approximation Polynomials (SAP) {{descriptor}} {{has been}} shown to be an appropriate global surface edscriptor for object categorization tasks in robotic applications. Nevertheless, in the original formulation the SAP descriptor is not invariant against rotations around the <b>camera</b> <b>axis.</b> This paper explains and evaluates two methods which pre-process the input data to yield repeatably well-aligned point clouds for the cimputation of the SAP descriptor. Whe show that the SAP descriptor can be rendered robust against rotations while retaining almost the full performance of the original approach which is superior to GFPFH, GRSD and VFH...|$|E
40|$|Most gait {{recognition}} approaches only study human walk-ing frontoparallel to {{the image}} plane which is not realistic in video surveillance applications. Human gait appearance depends on various factors including locations of the cam-era and the person, the <b>camera</b> <b>axis</b> and the walking direc-tion. By analyzing these factors, we propose a statistical approach for view-insensitive gait recognition. The pro-posed approach recognizes human using a single camera, and avoids the difficulties of recovering the human body structure and camera calibration. Experimental {{results show that the}} proposed approach achieves good performance in recognizing individuals walking along different directions. 1...|$|E
40|$|In the 3 -D {{shape of}} {{specular}} surface measurement, {{reconstruction of the}} shape is needed. This paper describes a method of shape reconstruction of specular surface using normal vectors. The normal vectors of the surface are extracted by five degrees of freedom camera system. A normal vector {{is said to be}} found when the alignment between the <b>camera</b> <b>axis</b> and the surface normal vector is achieved. These normal vectors then are used as the data for a cubic polynomial function to reconstruct the shape of the surface. The result shows that the methodology improves the 3 -D shape of object measurement with good accuracy...|$|E
50|$|An IP {{camera is}} {{typically}} either centralized (requiring a central {{network video recorder}} (NVR) to handle the recording, video and alarm management) or decentralized (no NVR needed, as camera can record to any local or remove storage media). The first centralized IP <b>camera</b> was <b>Axis</b> Neteye 200, released in 1996 by Axis Communications.|$|R
3000|$|Resolution (in pixels). This {{measures}} {{the size of}} each video frame in pixels (the higher, the better). This parameter consists of 4 levels on the <b>Axis</b> <b>cameras</b> (704 [...]...|$|R
50|$|Bill Weaver, Camera Operator (Arizona), He was the {{co-inventor}} (with Robert Steadman) of the Weaver-Steadman balanced fluid head, 2 and 3 <b>axis</b> <b>camera</b> supports {{widely used}} in Hollywood productions.|$|R
40|$|A novel {{algorithm}} for disparity/depth estimation from multi-view {{images is}} presented. A dynamic programming approach with window-based correlation and a novel cost function is proposed [...] The smoothness of disparity/depth map {{is embedded in}} dynamic programming approach, whilst the window-based correlation increases reliability. The enhancement methods are included, i. e. adaptive window size and shiftable window are used to increase reliability in homogenous areas and to increase sharpness at object boundaries. First, the algorithms estimates depth maps along a single <b>camera</b> <b>axis.</b> The algorithsm exploits then combines the depth estimates from different axis to derive a suitable depth map for multi-view images. The proposed scheme outperforms existing approaches in parallel and in the non-parallel camera configurations...|$|E
40|$|Abstract. A {{calibration}} method based on target features circle is proposed, which {{can determine the}} camera internal parameters based on the two different CCD images. One CCD image can be obtained in a position, and the other CCD image can be gotten by moving the target or camera to a new position. The principle of the {{calibration method}} based on one-step movement is expounded. The geometrical relationship and the criteria that the <b>camera</b> <b>axis</b> is orthogonal to the target plane are proven when the CCD image is circle. Finally, the experiment results verify that the method proposed is effective, and the data of the internal calibration parameters of the camera is given...|$|E
30|$|In this article, {{a method}} for the fusion of colour and 3 D {{information}} that is suitable for active security systems in industrial robotic environments is presented. To verify the proposed methods, a colour <b>camera,</b> <b>AXIS</b> 205, and a range camera, SR 4000, have been located over the workspace of the robot arm FANUC ARC MATE 100 iBe. The AXIS 205 Network Camera used has a resolution of 640 × 480 pixels and a pixel size of 5.08 × 3.81 mm. The SR 4000 range camera has a resolution of 176 × 140 pixels and a pixel size of 40 × 40 μ m. This camera has a modulation frequency of 29 / 20 / 31 Mhz and a detection range from 0.1 to 5 m.|$|E
40|$|It {{has been}} {{observed}} that in most videos recorded by surveillance cameras the image size of an object is a linear function of the y coordinate of its image location. This sim-ple linear relationship holds in the most common surveil-lance camera configurations, where objects move on a pla-nar surface and the <b>camera’s</b> X <b>axis</b> is parallel to that plane. This linear relationship enables us to easily per-form and enhance several geometric tasks based on track-ing an object over a few frames: (i) computing the hori-zon; (ii) computing the relative real world sizes of objects in the scene based on their image appearance; (iii) improv-ing tracking by constraining an object’s location and size. When the the <b>camera’s</b> X <b>axis</b> is not {{parallel to the ground}} plane, after tracking a couple of objects it is possible to find the rotation which rectifies the video so that its new X axis is parallel to the ground plane. 1...|$|R
25|$|Placing {{the flash}} {{away from the}} <b>camera's</b> optical <b>axis</b> ensures that {{the light from the}} flash hits the eye at an oblique angle. The light enters the eye in a {{direction}} away from the optical <b>axis</b> of the <b>camera</b> and is refocused by the eye lens back along the same axis. Because of this the retina will not be visible to the camera and the eyes will appear natural.|$|R
50|$|Axis Communications sells a video {{management}} software {{which it}} markets {{under the name}} <b>AXIS</b> <b>Camera</b> Station. The software provides remote video monitoring, recording and event management functionality. Its API allows the integration with other systems such as point of sale and access control.|$|R
40|$|A fully affine {{invariant}} image comparison method, Affine-SIFT (ASIFT) is introduced. While SIFT {{is fully}} invariant {{with respect to}} only four parameters namely zoom, rotation and translation, the new method treats the two left over parameters: the angles defining the <b>camera</b> <b>axis</b> orientation. Against any prognosis, simulating all views depending on these two parameters is feasible. The method permits to reliably identify features that have undergone very large affine distortions measured by a new parameter, the transition tilt. State-of-the-art methods hardly exceed transition tilts of 2 (SIFT), 2. 5 (Harris-Affine and Hessian-Affine) and 10 (MSER). ASIFT can handle transition tilts up 36 and higher (see Fig. 1). Index Terms — image matching, affine invariance, scale invariance, affine normalization, SIFT. 1...|$|E
40|$|If we {{histogram}} {{the normal}} ow vectors in {{images of a}} scene viewed by amoving observer, {{we can use the}} time-varying histogram to derive qualitative information about the observer's motion|for example, whether it is (primarily) translational or rotational, and whether the direction of translation or axis of rotation is (roughly) parallel or perpendicular to the <b>camera</b> <b>axis.</b> This is illustrated using ow histograms obtained from a variety of real image sequences. If the motion is translational, qualitative information about the scene depth can also be obtained from the ow histograms|for example, whether the scene depth is unimodal or bimodal. This is illustrated for real scenes containing a layer of vegetation seen against a textured background, or two layers of vegetation...|$|E
40|$|This paper {{presents}} {{a method of}} image restoration for projec-tive ground images which lie on a projection orthogonal to the <b>camera</b> <b>axis.</b> The ground images are initially transformed using homography, and then the proposed image restoration is applied. The process is performed in the dual-tree complex wavelet transform domain in conjunction with L 0 reweight-ing and L 2 minimisation (L 0 RL 2) employed to solve this ill-posed problem. We also propose instant estimation of a blur kernel arising from the projective transform and the subse-quent interpolation of sparse data. Subjective results show significant improvement of image quality. Furthermore, clas-sification of surface type at various distances (evaluated using a support vector machine classifier) is also improved for the images restored using our proposed algorithm. Index Terms — image restoration, projective transform, DT-CWT 1...|$|E
50|$|<b>Axis</b> <b>Camera</b> Application Platform, an open API, enables {{development}} of applications by third parties {{that can be}} downloaded and installed on Axis products. This allows software companies to offer video analytics applications for <b>Axis</b> network <b>cameras</b> providing functionalities such as recognition, counting, detection, and tracking.|$|R
5000|$|In 3D {{computer}} graphics a depth map {{is an image}} or image channel that contains information relating to {{the distance of the}} surfaces of scene objects from a viewpoint. The term is related to and may be analogous to depth buffer, Z-buffer, Z-buffering and Z-depth. The [...] "Z" [...] in these latter terms relates to a convention that the central axis of view of a camera is {{in the direction of the}} <b>camera's</b> Z <b>axis,</b> and not to the absolute Z axis of a scene.|$|R
50|$|The TV camera {{consisted}} of a vidicon tube, 25 millimeter and 100 millimeter focal-length lenses, a shutter, several optical filters, and iris-system mounted along an axis inclined approximately 16 degrees from the central axis of Surveyor 1. The camera was mounted under a mirror that could be moved in azimuth and elevation. This arrangement created a virtual stereo image pair so that adjacent overlapping images were stereo image pairs and {{could be viewed as}} three-dimensional images. This stereo capability permitted some photogrammetric measurements of various lunar features. The TV camera's operation was dependent on the receipt of the proper radio commands from the Earth. Frame-by-frame coverage of the lunar surface was obtained over 360 degrees in azimuth and from +40 degrees above the plane normal to the <b>camera's</b> <b>axis</b> to -65 degrees below this plane. Both 600-line and 200-line modes of operation were used. The 200-line mode transmitted over an omnidirectional antenna for the first 14 photos and scanned one frame every 61.8 seconds. The remaining transmissions were of 600-line pictures over a directional antenna, and each frame was scanned every 3.6 seconds. Each 200-line picture required 20 seconds for a complete video transmission and it used a radio bandwidth of about 1.2 kilohertz.|$|R
