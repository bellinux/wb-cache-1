3|27|Public
40|$|This work {{analyzes}} {{the ability to}} estimate and control the relative position and velocity of a Small Satellite {{with respect to a}} target vehicle using a single optical camera. Although the target range is generally unobservable when using anglesonly measurements, relative position/velocity observability can be achieved when the SmallSat is slowly rotating and the camera is offset from the center of gravity. The sensitivity of the navigation errors and trajectory dispersions to several simulation parameters is discussed, including SmallSat <b>camera</b> <b>offset,</b> spin rate, and range to target. Also included in the analysis is the effect of common sensor errors (e. g. camera and gyro bias/noise), external disturbances, and initial conditions. Future efforts are mentioned to extend the analysis to cooperative/uncooperative targets and to increase analysis efficiency through Linear Covariance analysis...|$|E
40|$|Kinematics, {{the study}} of motion {{exclusive}} of the influences of mass and force, {{is one of the}} primary methods used for the analysis of human biomechanical systems as well as other types of mechanical systems. The Anthropometry and Biomechanics Laboratory (ABL) in the Crew Interface Analysis section of the Man-Systems Division performs both human body kinematics as well as mechanical system kinematics using the Ariel Performance Analysis System (APAS). The APAS supports both analysis of analog signals (e. g. force plate data collection) as well as digitization and analysis of video data. The current evaluations address several methodology issues concerning the accuracy of the kinematic data collection and analysis used in the ABL. This document describes a series of evaluations performed to gain quantitative data pertaining to position and constant angular velocity movements under several operating conditions. Two-dimensional as well as three-dimensional data collection and analyses were completed in a controlled laboratory environment using typical hardware setups. In addition, an evaluation was performed to evaluate the accuracy impact due to a single axis <b>camera</b> <b>offset.</b> Segment length and positional data exhibited errors within 3 percent when using three-dimensional analysis and yielded errors within 8 percent through two-dimensional analysis (Direct Linear Software). Peak angular velocities displayed errors within 6 percent through three-dimensional analyses and exhibited errors of 12 percent when using two-dimensional analysis (Direct Linear Software). The specific results from this series of evaluations and their impacts on the methodology issues of kinematic data collection and analyses are presented in detail. The accuracy levels observed in these evaluations are also presented...|$|E
30|$|Structured light {{projection}} Another {{technique that}} can simultaneously capture color and topography is the structured light projection technique [10],[12],[14]. A common setup {{consists of a}} projector and a <b>camera</b> <b>offset</b> by a certain distance and aimed {{at the scene of}} interest. A structured light of known structure is then projected on the scene and captured by a camera. The camera can then compute the path of the light coming out of the projector, hitting the surface and then entering the camera. This triangulation allows for computing the topography while also capturing the color, if the projector’s illuminant is neutral. Their main drawbacks are that they are limited to the resolution of the projector and problems due to specular reflections. The constraints set by the projector’s resolution can be circumvented by using the fringe projection technique. In order to make the quantization or pixel pattern invisible, the projector’s projection can be blurred by defocusing the lens, resulting in a smooth fringe pattern (convolution theorem). The resolution of the capture is now constrained by that of the camera, and so we can employ cameras with a large pixel count and capture a large quantity of data per capture. Such large captures both increase speed and reduce the need for image registration (data stitching). Multiple cameras can be used to observe the projected pattern from different angles. Such arrangements often employ the same algorithm for each individual camera without much synergy, and so are only used to reduce occlusions and increase accuracy. One common problem with fringe projection {{is the fact that the}} its intensity pattern has to be exactly sinusoidal. This is difficult due to the non-linearity in the illumination of the projector and the sensor of the camera. Not only do these have to be accurately calibrated and accounted for; both the projector and the camera need to be geometrically and optically calibrated. The projector calibration can be achieved by projecting patterns, however, this projection is then again limited to the resolution of the projector, making precise calibration difficult. Instead of using structured light to do projector-to-camera light-ray calculations, we can also use the projection to encode the surface, uniquely labeling each point on the surface [15]. This unique label then has to be observed by multiple cameras, immediately solving the correspondence problem we encounter in stereo vision. Having solved this, camera-to-camera triangulation can be performed as is done in a stereo vision setup. Fringe projection can be used for this labeling as our system is then not limited by the projector’s resolution. Each point is then labeled with phase values. However, since these fringes were repetitive, the cameras do not yet know which fringe in the left image belongs to the fringe observed by the right image. In other words, there exists an offset between the phase value observed in the left and right phase images. This offset can be calculated if we compare phase values of at least one part of both images that we know that correspond. This can be done with the common stereo-vision approach, where we first search for keypoints in both color images, and then find corresponding matches. The offset of phase value observed from both cameras can now be nullified as we know the phase value should be the same for each point in the scene. This fringe projection aided stereo vision approach is therefore our selected method, since it can be both highly efficient and accurate.|$|E
40|$|In this paper, {{we propose}} an {{algorithm}} for estimating dense depth information of dynamic scenes from multiple video streams captured using unsynchronized stationary cameras. We {{solve this problem}} by first imposing two assumptions about the scene motion and the temporal <b>offset</b> between <b>cameras.</b> The motion of a scene is described using a local constant velocity model and the <b>camera</b> temporal <b>offset</b> {{is assumed to be}} constant within a short of period of time. Based on these models, geometric relations between the images of moving scene points, the scene depth, the scene motions, and the <b>camera</b> temporal <b>offset</b> are investigated and an estimation method is developed to compute the <b>camera</b> temporal <b>offset.</b> The three main steps of the proposed algorithm are 1) the estimation of the temporal <b>offset</b> between <b>cameras,</b> 2) the synthesis of synchronized image pairs based on the estimated <b>camera</b> temporal <b>offset</b> and optical flow fields computed in each view, and 3) the stereo computation based on the synthesized synchronous image pairs. The proposed algorithm has been tested on both synthetic data and real image sequences. Promising quantitative and qualitative experimental results are demonstrated in the paper. ...|$|R
50|$|Another {{method is}} to set up both left and right virtual <b>cameras,</b> both <b>offset</b> from the {{original}} camera but splitting the offset difference, then painting out occlusion edges of isolated objects and characters. Essentially clean-plating several background, mid ground and foreground elements.|$|R
5000|$|CompactFlash is {{physically}} larger than other card formats. This limits its use, especially in miniature consumer devices where internal space is limited, such as point-and-shoot digital <b>cameras.</b> (An <b>offsetting</b> benefit of larger size {{is that the}} card is easier to insert and remove, and harder to misplace.) ...|$|R
5000|$|Many <b>cameras</b> and {{displays}} <b>offset</b> the color components relative {{to each other}} or mix up temporal with spatial resolution:Image:Bayer matrix.svg|digital camera (Bayer color filter array)Image:Lcd_display_dead_pixel.jpg|LCD (Triangular pixel geometry)Image:Shadow_mask_closeup_cursor.jpg|CRT (shadow mask) ...|$|R
40|$|This note {{examines}} if offsetting {{driver behavior}} affects rear end crashes at intersections with red light <b>cameras.</b> It models <b>offsetting</b> driver behavior and estimates simultaneous probit equations to analyze this behavior. It finds {{that in the}} city considered the effect of red light cameras on {{the probability of a}} rear end crash occurring is very strong and positive, thus suggesting that offsetting behavior is present. ...|$|R
40|$|Computer vision {{attempts}} to provide camera-equipped machines with visual per-ception, i. e., {{the capability to}} comprehend their surroundings through the analysis and understanding of images. The ability to perceive depth is a vital component of visual perception that enables machines to interpret the three-dimensional structure of their surroundings and allows them to navigate through the environment. In computer vision, depth perception is achieved via stereo matching, a process that identifies correspondences between pixels in images acquired using a pair of horizontally <b>offset</b> <b>cameras.</b> It is possible to calculate depths from correspondences or, more specifically...|$|R
50|$|The {{music video}} was {{directed}} by duo Hammer & Tongs. It is filmed primarily (with cutaways to third party views) {{from the point of}} view of a collapsed cameraman in what appears to be an airport hangar. The cameraman collapses behind a car which then drives off to show the lead singer and the band forming to perform for the <b>offset</b> <b>camera.</b> The recording is interrupted by a little boy who, after being pulled out of the way of the camera abruptly, decides to run off with it and the band gives chase after him.|$|R
40|$|The {{discussion}} of the standard draft ISO/CD 11 146 yielded some agreement within the laser community, that the beam radius should be determined from the second moments of the power density distribution (PDD). However, specially if beam properties are determined in the most straightforward manner from {{the evaluation of the}} digitised data of cameras and camera-like devices, ambiguous results may appear. For CCD <b>camera</b> systems <b>offset</b> or baseline ambiguity is the most prominent error source. Even after a dark image has been subtracted this offset level error may occur. In theoretical considerations an incorrect offset level causes divergent integral expressions due to infinite integration domains. In the evaluation of measured PDDs an incorrect offset yields severe errors in the moments. The error may be minimised by using cut-offs or thresholds. In addition to offset misalignment other sources of errors may occur which may either be common to all camera-like systems or specific for the ac tual device...|$|R
40|$|Parametric {{uncertainty}} {{is one of}} many possible causes of divergence for the Kalman filter. Frequently, state estimation errors caused by imperfect model parameters are reduced by including the uncertain parameters as states (i. e., augmenting the state vector). For many situations, this not only improves the state estimates, but also improves the accuracy and precision of the parameters themselves. Unfortunately, not all filters benefit from this augmentation due to computational restrictions or because the parameters are poorly observable. A parameter with low observability (e. g., a set of high order gravity coefficients, a set of <b>camera</b> <b>offsets,</b> lens calibration controls, etc.) may not acquire enough measurements along a particular trajectory to improve the parameter's accuracy, which can cause detrimental effects in the performance of the augmented filter. The problem is then how to reduce the dimension of the augmented state vector while minimizing information loss. This dissertation explored possible implementations of reduced-order filters which decrease computational loads while also minimizing state estimation errors. A theoretically rigorous approach using the ?consider" methodology was taken at discrete time intervals were explored for linear systems. The continuous dynamics, discretely measured (continuous-discrete) model was also expanded for use with nonlinear systems. Additional techniques for reduced-order filtering are presented including the use of additive process noise, an alternative consider derivation, and the minimum variance reduced-order filter. Multiple simulation examples are provided to help explain critical concepts. Finally, two hardware applications are also included to show the validity of the theory for real world applications. It was shown that the minimum variance consider Kalman filter (MVCKF) is the best reduced-order filter to date both theoretically and through hardware and software applications. The consider method of estimation provides a compromise between ignoring parameter error and completely accounting for it in a probabilistic sense. Based on multiple measures of optimality, the consider filtering framework can be used to account for parameter error without directly estimating any or all of the parameters. Furthermore, by accounting for the parameter error, the consider approach provides a rigorous path to improve state estimation through the reduction of both state estimation error and with a consistent variance estimate. While using the augmented state vector to estimate both states and parameters may further improve those estimates, the consider estimation framework is an attractive alternative for complex and computationally intensive systems, and provides a well justified path for parameter order reduction...|$|R
500|$|Resident Evil 4 is {{regarded}} as one of the most influential games of the 2000s decade, due to its influence in redefining the third-person shooter genre by introducing a [...] "reliance on <b>offset</b> <b>camera</b> angles that fail to obscure the action". The new gameplay alterations and immersive style appealed to many not previously familiar with the series. The over-the-shoulder viewpoint introduced in Resident Evil 4 has later become standard in third-person shooters, including titles ranging from Gears of War to [...] It has also become a standard [...] "precision aim" [...] feature for action games in general, with examples ranging from Dead Space and Grand Theft Auto to the Ratchet & Clank Future series and Fallout.|$|R
40|$|Optical {{measurement}} techniques have been gaining ground for their vast applications in industry and scientific purposes. These techniques are beneficial comparing to the mechanical methods. Non-intrusive, robust, high accuracy and small measurement volume {{are some of}} the advantages of the optical metrology. However, these approaches are expensive. Interferometry {{is one of the most}} prominent principles of these optical measurements. It employs the study of fringe patterns in order to model surface roughness with a high precision up to nano-meter scale. In this thesis, we mainly focus on the multi-band frequency shifting interferometry based on polarization measurement for 3 D surface modeling. The system has advantages such as it is very accurate and there is no need of a phase unwrapping algorithm. Like the conventional phase shifting interferometry, four intensity images are recorded for the four polarization states correspondingly and then the images are processed by MATLAB and the final results are provided. In this technique the need for three <b>cameras</b> and <b>offset</b> correction between the cameras has been revised and optimized by using only one camera. In the first setup trial, a fiber optic switch has been used which does not lead to the desired results and then the switch has been removed and the corresponding images are satisfactory...|$|R
50|$|The {{viewfinder}} of any {{camera that}} does not allow viewing through the taking lens, including rangefinder <b>cameras,</b> is necessarily <b>offset</b> from the taking lens, so that the image shown is not exactly what will be recorded on the film; this parallax error is negligible at large subject distances, but increases as the distance decreases. More advanced rangefinder cameras project into the viewfinder a brightline frame that moves as the lens is focused, correcting parallax error down to the minimum distance at which the rangefinder functions. The angle {{of view of a}} given lens also changes with distance, and the brightline frames in the finders of a few cameras automatically adjust for this as well. For extreme close-up photography, the rangefinder camera is awkward to use, as the viewfinder no longer points at the subject.|$|R
40|$|EO- 3 GIFTS mission (2004). The {{camera is}} {{designed}} to be autonomously self-calibrating, and capable of a rapid/reliable solution of the lost-in-space problem as well as recursive attitude estimation. Two efficient Kalman filter algorithms for attitude, <b>camera</b> principal point <b>offset,</b> and focal length estimation are developed. These algorithms make use of three axis gyros for the rate data and star camera split field-of-view line-of-sight vector measurements. To model the optics of the camera the pinhole model is used, which is found to be sufficiently accurate for most of star cameras. The relative merits of the two algorithms are then studied for estimating the principal point offset, focal length and attitude of a simulated spacecraft motion. Simulation results indicate that both algorithms produce precise attitude estimates by determining the principal point offset, focal length and rate bias; however, reliability and robustness characteristics favor the second algorithm...|$|R
40|$|In {{creating}} a robotic system capable of autonomously making {{decisions based on}} an environment, it must be capable of visualizing and recording its surroundings. This thesis focuses {{on the use of}} stereoscopic imaging from two <b>offset</b> <b>camera</b> feeds to create a 3 D image. The robot is capable of combining multiple 3 D images to create a 3 D model of the entire area of operation. The robot must be able to generate a 3 D image from the cameras and convert the 3 D images into a digital map. Furthermore, the robot must be able to detect its location within the digital map for navigational and mapping purposes. This process of simultaneously localizing and mapping (SLAM) is a revolutionary procedure used to generate maps in real time. By combining these three aspects, the robotic system can generate an accurate 3 D map for the region of operation...|$|R
5000|$|Syphon Filter (1999) by Eidetic (now SCE Bend Studio) {{combined}} {{the perspective of}} Tomb Raider with action elements of games such as GoldenEye 007 (1997) and Metal Gear Solid (1998). Richard Rouse III wrote in GamaSutra that the game {{was the most popular}} third person shooter for the PlayStation. [...] While in Tomb Raider and Syphon Filter the protagonists automatically aimed at antagonists, later games such as Oni (2001), Max Payne (2001) and SOCOM (2002) forced players to control aiming themselves by means of two control sticks or a keyboard and mouse. Max Payne (2001) was acclaimed as a superlative third person shooter, inspired by Hong Kong action cinema. Resident Evil 4 (2005) was influential in helping to redefine the third-person shooter genre, with its use of [...] "over the shoulder" [...] <b>offset</b> <b>camera</b> angles, where the camera is placed directly over the right shoulder and therefore doesn't obscure the action.|$|R
5000|$|Resident Evil 4 is {{regarded}} as one of the most influential games of the 2000s decade, due to its influence in redefining at least two video game genres: the survival horror and the third-person shooter. Resident Evil 4 attempted to redefine the survival horror genre by emphasizing reflexes and precision aiming, thus broadening the gameplay of the series with elements from the wider action game genre. It helped redefine the third-person shooter genre by introducing a [...] "reliance on <b>offset</b> <b>camera</b> angles that fail to obscure the action." [...] The [...] "over the shoulder" [...] viewpoint introduced in Resident Evil 4 has now become standard in third-person shooters, including titles ranging from Gears of War to Batman: Arkham Asylum. It has also become a standard “precision aim” feature for action games in general, with examples ranging from Dead Space and Grand Theft Auto to the Ratchet & Clank Future series.|$|R
40|$|The paper investigates {{immersive}} videography and {{its application}} in close-range photogrammetry. Immersive video involves {{the capture of}} a live-action scene that presents a 360 ° field of view. It is recorded simultaneously by multiple cameras or microlenses, where the principal point of each <b>camera</b> is <b>offset</b> from the rotating axis of the device. This issue causes problems when stitching together individual frames of video separated from particular cameras, however {{there are ways to}} overcome it and applying immersive cameras in photogrammetry provides a new potential. The paper presents two applications of immersive video in photogrammetry. At first, the creation of a low-cost mobile mapping system based on Ladybug® 3 and GPS device is discussed. The amount of panoramas is much too high for photogrammetric purposes as the base line between spherical panoramas is around 1 metre. More than 92 000 panoramas were recorded in one Polish region of Czarny Dunajec and the measurements from panoramas enable the user to measure the area of outdoors (adverting structures) and billboards. A new law is being created in order {{to limit the number of}} illegal advertising structures in the Polish landscape and immersive video recorded in a short period of time is a candidate for economical and flexible measurements off-site. The second approach is a generation of 3 d video-based reconstructions of heritage sites based on immersive video (structure from immersive video). A mobile camera mounted on a tripod dolly was used to record the interior scene and immersive video, separated into thousands of still panoramas, was converted from video into 3 d objects using Agisoft Photoscan Professional. The findings from these experiments demonstrated that immersive photogrammetry seems t...|$|R
40|$|Creating {{a robotic}} system capable of {{autonomously}} making {{decisions based on}} the environment must first be capable of visualizing and recording its surroundings. This paper focuses on creating a robotic system that makes use of stereoscopic imaging from two <b>offset</b> <b>camera</b> feeds to create a 3 D image. The robot is able to generate a 3 D image from the cameras and convert the 3 D images into a digital map. Furthermore, the robot is able use this model to detect its location within the digital map for navigational and mapping purposes. This process of simultaneously localizing and mapping (SLAM) is a revolutionary procedure used to generate maps in real time. By combining these three aspects, the robotic system can generate an accurate 3 D map for the region of operation. Stereoscopic imaging provides many benefits over conventional mapping methods which allow for cheap, rapid, and detailed autonomous mapping. This paper demonstrates how a low cost robotic system can independently generate a 3 D map in real time...|$|R
40|$|A {{methodology}} for determining spacecraft attitude and autonomously calibrating star camera, both independent of each other, {{is presented in}} this paper. Unlike most of the attitude determination algorithms where attitude of the satellite depend on the camera calibrating parameters (like principal point offset, focal length etc.), the proposed method {{has the advantage of}} computing spacecraft attitude independently of camera calibrating parameters except lens distortion. In the proposed method both attitude estimation and star camera calibration is done together independent of each other by directly utilizing the star coordinate in image plane and corresponding star vector in inertial coordinate frame. Satellite attitude, <b>camera</b> principal point <b>offset,</b> focal length (in pixel), lens distortion coefficient are found by a simple two step method. In the first step, all parameters (except lens distortion) are estimated using a closed-form solution based on a distortion free camera model. In the second step lens distortion coefficient is estimated by linear least squares method using the solution of the first step {{to be used in the}} camera model that incorporates distortion. These steps are applied in an iterative manner to refine the estimated parameters. The whole procedure is faster enough for onboard implementation...|$|R
40|$|This thesis {{presents}} {{a complete and}} fully automatic method for estimating the volume of an airbag, through all stages of its inflation, with multiple synchronized high-speed cameras. Using recorded contours of the inflating airbag, its visual hull is reconstructed with a novel method: The intersections of all back-projected contours are first identified with an accelerated epipolar algorithm. These intersections, together with additional points sampled from concave surface regions of the visual hull, are then Delaunay triangulated to a connected set of tetrahedra. Finally, the visual hull is extracted by carving away the tetrahedra that are classified as inconsistent with the contours, according to a voting procedure. The volume of an airbag's visual hull is always larger than the airbag's real volume. By projecting a known synthetic model of the airbag into the <b>cameras,</b> this volume <b>offset</b> is computed, and an accurate estimate of the real airbag volume is extracted. Even though volume estimates can be computed for all camera setups, the cameras should be specially posed to achieve optimal results. Such poses are uniquely found for different airbag models with a separate, fully automatic, simulated annealing algorithm. Satisfying results are presented for both synthetic and real-world data...|$|R
40|$|In structure-from-motion with {{a single}} camera {{it is well known}} that the scene can be only {{recovered}} up to a scale. In order to compute the absolute scale, one needs to know the baseline of the camera motion or the dimension of at least one element in the scene. In this paper, we show that there exists a class of structure-from-motion problems where it is possible to compute the absolute scale completely automati-cally without using this knowledge, that is, when the camera is mounted on wheeled vehicles (e. g. cars, bikes, or mobile robots). The construction of these vehicles puts interest-ing constraints on the camera motion, which are known as “nonholonomic constraints ” The interesting case is when the <b>camera</b> has an <b>offset</b> to the vehicle’s center of motion. We show that by just knowing this offset, the absolute scale can be computed with a good accuracy when the vehicle moves. We give a mathematical derivation and provide ex-perimental results on both simulated and real data. To our knowledge this is the first time nonholonomic constraints of wheeled vehicles are used to estimate the absolute scale. We believe that the proposed method can be useful in those research areas involving visual odometry and mapping with vehicle mounted cameras. 1...|$|R
40|$|The paper {{describes}} {{an approach to}} quantify the amount of projective error produced by an offset of projection centres in a panoramic imaging workflow. We have limited this research to such panoramic workflows in which several sub-images using planar image sensor are taken and then stitched together as a large panoramic image mosaic. The aim is to simulate how large the offset can be before it introduces significant error to the dataset. The method uses geometrical analysis to calculate the error in various cases. Constraints for shooting distance, focal length {{and the depth of}} the area of interest are taken into account. Considering these constraints, it is possible to safely use even poorly calibrated panoramic camera rig with noticeable offset in projection centre locations. The aim is to create datasets suited for photogrammetric reconstruction. Similar constraints can be used also for finding recommended areas from the image planes for automatic feature matching and thus improve stitching of sub-images into full panoramic mosaics. The results are mainly designed to be used with long focal length <b>cameras</b> where the <b>offset</b> of projection centre of sub-images can seem to be significant {{but on the other hand}} the shooting distance is also long. We show that in such situations the error introduced by the offset of the projection centres results only in negligible error when stitching a metric panorama. Even if the main use of the results is with cameras of long focal length, they are feasible for all focal lengths...|$|R
40|$|A device {{combines}} video feeds {{from multiple}} cameras to provide wide-field-of-view, high-resolution, stereoscopic video to the user. The prototype under development {{consists of two}} camera assemblies, one for each eye. One of these assemblies incorporates a mounting structure with multiple <b>cameras</b> attached at <b>offset</b> angles. The video signals from the cameras are fed to a central processing platform where each frame is color processed and mapped into a single contiguous wide-field-of-view image. Because the resolution of most display devices is typically smaller than the processed map, a cropped portion of the video feed is output to the display device. The positioning of the cropped window will likely be controlled {{through the use of}} a head tracking device, allowing the user to turn his or her head side-to-side or up and down to view different portions of the captured image. There are multiple options for the display of the stereoscopic image. The use of head mounted displays is one likely implementation. However, the use of 3 D projection technologies is another potential technology under consideration, The technology can be adapted in a multitude of ways. The computing platform is scalable, such that the number, resolution, and sensitivity of the cameras can be leveraged to improve image resolution and field of view. Miniaturization efforts can be pursued to shrink the package down for better mobility. Power savings studies can be performed to enable unattended, remote sensing packages. Image compression and transmission technologies can be incorporated to enable an improved telepresence experience...|$|R
40|$|The second XMM-Newton serendipitous source catalogue, 2 XMM, {{provides}} the ideal data base for performing a statistical {{evaluation of the}} flux cross-calibration of the XMM-Newton European Photon Imaging Cameras (EPIC). We aim to evaluate {{the status of the}} relative flux calibration of the EPIC cameras on board XMM-Newton (MOS 1, MOS 2, and pn) and investigate the dependence of the calibration on energy, position in the field of view of the X-ray detectors, and lifetime of the mission. We compiled the distribution of flux percentage differences for large samples of 'good quality' objects detected with {{at least two of the}} EPIC <b>cameras.</b> The mean <b>offset</b> of the fluxes and dispersion of the distributions was then found by Gaussian fitting. Count rate to flux conversion was performed with a fixed spectral model. The impact on the results of varying this model was investigated. Excellent agreement was found between the two EPIC MOS cameras to better than 4 % from 0. 2 keV to 12. 0 keV. MOS cameras register 7 - 9 % higher flux than pn below 4. 5 keV and 10 - 13 % flux excess above 4. 5 keV. No evolution of the flux ratios is seen with time, except at energies below 0. 5 keV, where we found a strong decrease in the MOS to pn flux ratio with time. This effect is known to be due to a gradually degrading MOS redistribution function. The flux ratios show some dependence on distance from the optical axis in the sense that the MOS to pn flux excess increases with off-axis angle. Furthermore, in the 4. 5 - 12. 0 keV band there is a strong dependence of the MOS to pn excess flux on the azimuthal-angle. These results strongly suggest that the calibration of the Reflection Grating Array (RGA) blocking factors is incorrect at high energies. Finally, we recommend ways to improve the calculation of fluxes in future versions of XMM-Newton source catalogues. Comment: 11 pages, 10 figures, 3 tables. Abridged Abstract. Accepted for publication in Astronomy and Astrophysic...|$|R
40|$|Computer vision {{attempts}} to provide camera-equipped machines with visual perception, i. e., {{the capability to}} comprehend their surroundings through the analysis and understanding of images. The ability to perceive depth is a vital component of visual perception that enables machines to interpret the three-dimensional structure of their surroundings and allows them to navigate through the environment. In computer vision, depth perception is achieved via stereo matching, a process that identifies correspondences between pixels in images acquired using a pair of horizontally <b>offset</b> <b>cameras.</b> It is possible to calculate depths from correspondences or, more specifically, the positional offsets (disparities) between pixels in correspondence. ^ A stereo matching method implemented on massively parallel graphics hardware is presented that allows for recovery of highly accurate disparities in real-time. This method combines a pixel dissimilarity metric computed using both the gradients and the census transforms of the input images, a non-iterative local disparity selection scheme based on an efficient approximation of the well-known edge-preserving bilateral image filter, and a refinement technique that iteratively improves the accuracy of disparities. The refinement technique, which also benefits {{from the use of}} the bilateral filter, eliminates mismatches by penalizing disparities that disagree with the the disparity estimates generated using local disparity values and/or gradients. ^ When evaluated using the Middlebury stereo performance benchmark (version 3), the proposed method ranks first and second to date using the training and test image sets, respectively, in terms of the overall accuracy of stereo matching measured as the average percentage of pixels with the absolute disparity error greater than 2 pixels at the nominal image resolution. Simultaneously, the method achieves the lowest error rates for 5 out of 15 image pairs in the training set, and 3 out of 15 image pairs in the test set. This method is also shown to enable robust matching in the presence of radiometric distortions caused by changes in illumination or camera exposure. The high accuracy of matching, that is largely maintained in the presence of radiometric distortions and the ability to operate in real time, make the proposed method well-suited for applications such as robotic navigation and structure reconstruction. ...|$|R
40|$|Beside the {{creation}} of virtual animated 3 D City models, analysis for homeland security and city planning, the accurately determination of geometric features out of oblique imagery is an important task today. Due to the huge number of single images the reduction of control points force {{to make use of}} direct referencing devices. This causes a precise camera-calibration and additional adjustment procedures. This paper aims to show the workflow of the various calibration steps and will present examples of the calibration flight with the final 3 D City model. In difference to most other software, the oblique cameras are used not as co-registered sensors in relation to the nadir one, all camera images enter the AT process as single pre-oriented data. This enables a better post calibration in order to detect variations in the single camera calibration and other mechanical effects. The shown sensor (Oblique Imager) is based o 5 Phase One cameras were the nadir one has 80 MPIX equipped with a 50 mm lens while the oblique ones capture images with 50 MPix using 80 mm lenses. The cameras are mounted robust inside a housing to protect this against physical and thermal deformations. The sensor head hosts also an IMU which is connected to a POS AV GNSS Receiver. The sensor is stabilized by a gyro-mount which creates floating Antenna –IMU lever arms. They had to be registered together with the Raw GNSS-IMU Data. The camera calibration procedure was performed based on a special calibration flight with 351 shoots of all 5 cameras and registered the GPS/IMU data. This specific mission was designed in two different altitudes with additional cross lines on each flying heights. The five images from each exposure positions have no overlaps but in the block there are many overlaps resulting in up to 200 measurements per points. On each photo there were in average 110 well distributed measured points which is a satisfying number for the camera calibration. In a first step {{with the help of the}} nadir camera and the GPS/IMU data, an initial orientation correction and radial correction were calculated. With this approach, the whole project was calculated and calibrated in one step. During the iteration process the radial and tangential parameters were switched on individually for the camera heads and after that the camera constants and principal point positions were checked and finally calibrated. Besides that, the bore side calibration can be performed either on basis of the nadir <b>camera</b> and their <b>offsets,</b> or independently for each camera without correlation to the others. This must be performed in a complete mission anyway to get stability between the single camera heads. Determining the lever arms of the nodal-points to the IMU centre needs more caution than for a single camera especially due to the strong tilt angle. Prepared all these previous steps, you get a highly accurate sensor that enables a fully automated data extraction with a rapid update of you existing data. Frequently monitoring urban dynamics is then possible in fully 3 D environment...|$|R

