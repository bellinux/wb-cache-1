4000|180|Public
25|$|Some {{quantification}} {{methods can}} circumvent {{the need for}} an exact alignment of a read to a reference sequence all together. The kallisto method combines pseudoalignment and quantification into a single step that runs 2 orders of magnitude faster than comparable methods such as tophat/cufflinks, with less <b>computational</b> <b>burden.</b>|$|E
2500|$|... is sparse {{and/or the}} matrix {{elements}} are given by simple algebraic expressions because computing matrix elements {{can also be}} a <b>computational</b> <b>burden.</b>|$|E
5000|$|RRO {{spectrum}} {{contains many}} harmonics of the spindle frequency (e.g. ∼ 200 harmonics) {{that should be}} attenuated. This increases the <b>computational</b> <b>burden</b> in the controller.|$|E
40|$|This paper {{presents}} an efficient MPC algorithm for uncertain time-varying systems with input constraints. The main {{advantage of this}} algorithm with respect to other published algorithms is to significantly enlarge {{the size of the}} stabilization set without regard to <b>computational</b> <b>burdens.</b> Specially, we introduce an off-line region-dependent MPC scheme to avoid the size limitation of the control horizon caused by huge on-line <b>computational</b> <b>burdens.</b> A numerical example is included to illustrate the validity of the result...|$|R
30|$|As already commented in the introduction, {{both types}} of problem are proven {{to belong to the}} class of the NP-hard problems, that is, highly {{difficult}} problems in terms of <b>computational</b> <b>burdens</b> and efficacy.|$|R
3000|$|... can be also {{obtained}} by exploiting the SCM, NSCM, and AML estimators, respectively. However, taking the SCM estimator as an example, {{we at least}} need K ≥ JN training signals to ensure a full-rank estimate of the JN × JN matrix R. Obviously, the large JN spatio-temporal product will impose excessive training and <b>computational</b> <b>burdens</b> to the detector.|$|R
50|$|Alice {{wishes to}} {{convince}} Bob that z {{is not a}} valid signature of m under the key, gx; i.e., z ≠ mx. Alice and Bob have agreed an integer, k, which sets the <b>computational</b> <b>burden</b> on Alice and the likelihood that she should succeed by chance.|$|E
50|$|Some {{quantification}} {{methods can}} circumvent {{the need for}} an exact alignment of a read to a reference sequence all together. The kallisto method combines pseudoalignment and quantification into a single step that runs 2 orders of magnitude faster than comparable methods such as tophat/cufflinks, with less <b>computational</b> <b>burden.</b>|$|E
50|$|Another {{important}} area of this subject is the <b>computational</b> <b>burden</b> of these algorithms. Algorithms with faster execution times are sought {{to produce more}} of these results in less time in order to complete these projects quicker. As the resolution increases to produce animated feature films, the amount of processing can increase greatly.|$|E
30|$|To {{provide a}} depth {{application}} of some methods {{for the assessment}} of waveform distortion caused by PVS and WTS generators. In particular, the DFT method, parametric methods (Prony and ESPRIT), hybrid methods, and modified sliding-window parametric methods are presented and critically compared {{on the basis of the}} accuracy of the results they produce and their <b>computational</b> <b>burdens,</b> taking into account both test and measured waveforms.|$|R
30|$|In general, {{there are}} two fusion {{estimation}} methods commonly used to process the measured sensor data [3, 4]. If a central processor directly receives the measurement data of all local sensors and processes them in real time, the correlative result {{is known as the}} centralized estimation. However, this approach has some serious disadvantages, including bad reliability and survivability, as well as heavy communication and <b>computational</b> <b>burdens.</b>|$|R
3000|$|In this article, {{multichannel}} {{signal detection}} problem in space-time colored compound-Gaussian environment is discussed. By exploiting the structural {{information about the}} disturbance space-time covariance matrix, we model the disturbance signal as a multichannel AR process to ease the training and <b>computational</b> <b>burdens.</b> Modeling the texture as an unknown deterministic parameter, we first derive the CG-PGLRT detector {{under the assumption that}} the multichannel AR parameters Q and A [...]...|$|R
50|$|Fingerprint {{matching}} has {{an enormous}} <b>computational</b> <b>burden.</b> Some larger AFIS vendors deploy custom hardware while others use software to attain matching speed and throughput. In general, it is desirable to have, at the least, a two-stage search. The first stage will generally make use of global fingerprint characteristics while the second stage is the minutia matcher.|$|E
50|$|When {{problems}} {{have not yet}} been formalised, they can still be characterised by a model of computation that includes human computation. The <b>computational</b> <b>burden</b> of a problem is split between a computer and a human: one part is solved by computer and the other part solved by human. This formalisation is referred to as human-assisted Turing machine.|$|E
50|$|To {{address this}} problem, a {{complexity}} theory for AI has been proposed. It {{is based on}} a model of computation that splits the <b>computational</b> <b>burden</b> between a computer and a human: one part is solved by computer and the other part solved by human. This is formalised by a human-assisted Turing machine. The formalisation defines algorithm complexity, problem complexity and reducibility which in turn allows equivalence classes to be defined.|$|E
40|$|Bayesian {{estimation}} of divergence times from molecular sequences relies on sophisticated Markov chain Monte Carlo techniques, and Metropolis-Hastings (MH) samplers {{have been successfully}} used in that context. This approach involves heavy <b>computational</b> <b>burdens</b> that can hinder the analysis of large phylogenomic data sets. Reliable {{estimation of}} divergence times can also be extremely time consuming, if not impossible, for sequence alignments that convey weak or conflicting phylogenetic signals, [...] ...|$|R
40|$|Large scale agglomerative {{clustering}} is {{hindered by}} <b>computational</b> <b>burdens.</b> We propose a novel scheme where exact inter-instance distance calculation {{is replaced by}} the Hamming distance between Kernelized Locality-Sensitive Hashing (KLSH) hashed values. This results in a method that drastically decreases computation time. Additionally, we take advantage of certain labeled data points via distance metric learning to achieve a competitive precision and recall comparing to K-Means but in much less computation time...|$|R
3000|$|... using Equation 24 first, {{then proceed}} to {{evaluate}} λ and ν using Equations 25 and 26. The above process would be repeated until convergence. The iterative algorithm {{is based on}} alternating optimization and is computationally intensive. One of the <b>computational</b> <b>burdens</b> lies {{in the evaluation of}} Equations 20 and 21 required in the evaluation of Equation 24, where inverting matrices of size N[*]×[*]N is needed. This motivates the development of the following alternative algorithm.|$|R
5000|$|Such {{calculations}} {{are often}} extremely computationally intensive, and can {{require the use}} of the largest available supercomputers. To reduce the <b>computational</b> <b>burden,</b> the so-called quenched approximation can be used, in which the fermionic fields are treated as non-dynamic [...] "frozen" [...] variables. While this was common in early lattice QCD calculations, [...] "dynamical" [...] fermions are now standard. [...] These simulations typically utilize algorithms based upon molecular dynamics or microcanonical ensemble algorithms.|$|E
50|$|In {{an earlier}} work, Herb Simon {{had shown that}} with {{quadratic}} costs and under a certain set of assumptions about the probability distributions, optimal decision rules for production and inventories would be linear functions of the variables describing the state. In his model, firms only needed {{to take into account}} the expected value and ignore all higher moments of the probability distribution of future sales. This result, known as certainty equivalence, drastically reduces the <b>computational</b> <b>burden</b> on a representative decision maker.|$|E
5000|$|Approached {{from the}} regularization perspective, {{parameter}} tuning {{is similar to}} the scalar-valued case and can generally be accomplished with cross validation. Solving the required linear system is typically expensive in memory and time. If the kernel is separable, a coordinate transform can convert [...] to a block-diagonal matrix, greatly reducing the <b>computational</b> <b>burden</b> by solving D independent subproblems (plus the eigendecomposition of [...] ). In particular, for a least squares loss function (Tikhonov regularization), there exists a closed form solution for : ...|$|E
30|$|However, the <b>computational</b> <b>burdens</b> of SWEM and SWMEM (Table  3) were {{considerably}} {{greater than}} those observed for the same methods in Table  2. This proves that the optimal sampling rate for each parametric method guarantees the best performance with respect to accuracy and computational time. It also proves that {{the accuracy of the}} results was not affected by exceeding the optimal sampling rate, but the computational effort increased significantly, especially at the very high-sampling rates.|$|R
40|$|Estimation of trip {{tables and}} other {{matrices}} that {{are subject to}} constraints is a common practical problem. This note reviews four common estimation methods: (1) minimization of the sum of absolute deviations, (2) the biproportional technique, (3) information minimization and (4) constrained generalized least squares (CGLS) regression. A small example illustrates their application. <b>Computational</b> approaches and <b>burdens</b> as well as implicit error structure assumptions are reviewed. The paper concludes that estimates obtained from the biproportional, information minimization and CGLS regression (with a chi-square type error distribution assumption), {{are likely to be}} similar in practice. Choice of an appropriate technique depends upon a priori assumptions of error structures, relative <b>computational</b> <b>burdens</b> and the usefulness of estimate uncertainty measures. ...|$|R
30|$|Classified {{patches and}} {{low-dimensional}} dictionary are {{considered in the}} CKPF. Note that low-dimensional dictionary and classification parameters (CP) are learned by the label-consistent K-SVD (LC-KSVD) [17, 18] technique. To {{the best of our}} knowledge, this is the first work to extend the LC-KSVD approach to exploit the intrinsic structure among the patches of the visual target. The image patches in the dictionary trained using LC-KSVD will be more discriminative to classify foreground from the background, and the obtained low dictionary can reduce the <b>computational</b> <b>burdens.</b>|$|R
5000|$|Computational inefficiency: The {{original}} GCTA implementation scales poorly {{with increasing}} data size (...) , {{so even if}} enough data is available for precise GCTA estimates, the <b>computational</b> <b>burden</b> may be unfeasible. GCTA can be meta-analyzed as a standard precision-weighted fixed-effect meta-analysis, so research groups sometimes estimate cohorts or subsets and then pool them meta-analytically (at the cost of additional complexity and some loss of precision). This has motivated the creation of faster implementations and variant algorithms which make different assumptions, such as using moment matching ...|$|E
50|$|A traffic {{analysis}} zone (TAZ) is {{the unit of}} geography most commonly used in conventional transportation planning models. The size of a zone varies, but for a typical metropolitan planning software, a zone of under 3000 people is common. The spatial extent of zones typically varies in models, ranging from very large areas in the exurb to as small as city blocks or buildings in central business districts. There is no technical reason zones cannot be as small as single buildings, however additional zones add to the <b>computational</b> <b>burden.</b>|$|E
50|$|A {{large amount}} of data must be {{collected}} and processed for truly three-dimensional imaging, necessitating a large detector array, long scanning times, and heavy <b>computational</b> <b>burden.</b> To reduce these requirements, the three-dimensional problem is often simplified to a quasi-two-dimensional problem by using focused ultrasound detectors to limit ultrasound detection to a two-dimensional plane in the illuminated volume. The result {{is a series of}} two-dimensional, cross-sectional images, which can be collected in real time and can show quite high in-plane resolution if detector elements are packed at high density around the image plane. Translating the detector along the third dimension then allows volumetric scanning.|$|E
40|$|AbstractTwo-dimensional {{orthogonal}} triangular functions (2 D-TFs) {{are presented}} as {{a new set of}} basis functions for expanding 2 D functions. Their properties are determined and an operational matrix for integration obtained. Furthermore, 2 D-TFs are used to approximate solutions of nonlinear two-dimensional integral equations by a direct method. Since this approach does not need integration, all calculations can be easily implemented, and several advantages in reducing <b>computational</b> <b>burdens</b> arise. Finally, the efficiency of this method will be shown by comparison with some numerical results...|$|R
40|$|In {{the cloud}} {{computing}} era, {{in order to}} avoid <b>computational</b> <b>burdens,</b> many organizations tend to outsource their com- putations to third-party cloud servers. In order to protect service quality, the integrity of computation results need to be guaranteed. In this paper, we develop a game theoretic framework which helps the outsourcer to maximize its pay- o while ensuring the desired level of integrity for the out- sourced computation. We de ne two Stackelberg games and analyze the optimal setting's sensitivity for the parameters of the model...|$|R
40|$|Accurate {{modelling}} of substitution {{processes in}} protein-coding sequences is often {{hampered by the}} <b>computational</b> <b>burdens</b> associated with full codon models. Lately, codon partition models have been proposed as a viable alternative, mimicking the substitution behaviour of codon models at a low computational cost. Such codon partition models however impose independent evolution of the different codon positions, which is overly restrictive from a biological point of view. Given that empirical research has provided indications of context-dependent substitution patterns at four-fold degenerate sites, we take those indications into account in this paper. status: publishe...|$|R
50|$|Though the {{approach}} is simple and the <b>computational</b> <b>burden</b> of linearization scheduling approaches is often much less than for other nonlinear design approaches, its inherent drawbacks outweigh its advantages and necessitates a new paradigm for the control of dynamical systems. New methodologies such as Adaptive control based on Artificial Neural Networks (ANN), Fuzzy logic etc try to address such problems, the lack of proof of stability and performance of such approaches over entire operating parameter regime requires design of a parameter dependent controller with guaranteed properties for which, a Linear Parameter Varying controller could be an ideal candidate.|$|E
5000|$|Distributed {{source coding}} (DSC) is an {{important}} problem in information theory and communication. DSC problems regard the compression of multiple correlated information sources that do not communicate with each other. [...] By modeling the correlation between multiple sources at the decoder side together with channel codes, DSC is able to shift the computational complexity from encoder side to decoder side, therefore provide appropriate frameworks for applications with complexity-constrained sender, such as sensor networks and video/multimedia compression (see distributed video coding). One of the main properties of distributed source coding is that the <b>computational</b> <b>burden</b> in encoders is shifted to the joint decoder.|$|E
5000|$|... where [...] is a space-time {{snap-shot}} of {{the input}} data. The main difficulty of STAP is solving for and inverting the typically unknown interference covariance matrix, [...] Other difficulties arise when the interference covariance matrix is ill-conditioned, making the inversion numerically unstable. In general, this adaptive filtering must be performed {{for each of}} the unambiguous range bins in the system, for each target of interest (angle-Doppler coordinates), making for a massive <b>computational</b> <b>burden.</b> Steering losses can occur when true target returns do not fall exactly on one of the points in our 2-D angle-Doppler plane that we've sampled with our steering vector [...]|$|E
40|$|In {{this paper}} we suggest an {{extension}} to the Database-as-a-Service (DAS) model that intrdocues a secure coprocessor (SC) at an untrusted database service provider in ordet to overcome drawbacks in the plain DAS model. The processor {{serves as a}} neutral party between the clients and service providers {{with the goal of}} increasing security of outsourced data. Additionally, it supports a much broader range of queries performed and reduces both bandwidth and <b>computational</b> <b>burdens</b> on the client. We expect these improvements to make the DAS model more viable and attractive from a client’s perspective. ...|$|R
40|$|We {{present a}} simple and {{effective}} iterative procedure to estimate segmented mixed models in a likelihood based framework. Random effects and covariates are allowed for each model parameter, including the changepoint. The method is practical and avoids the <b>computational</b> <b>burdens</b> related to estimation of nonlinear mixed effects models. A conventional linear mixed model with proper covariates that account for the changepoints {{is the key to}} our estimating algorithm. We illustrate the method via simulations and using data from a randomized clinical trial focused on change in depressive symptoms over time which characteristically show two separate phases of change...|$|R
40|$|We {{propose a}} new graph model for {{folksonomy}} analysis. In order to compare our proposed model with the vector space model, which {{is widely used}} in the information retrieval field, we have performed multidimensional scaling schemes and clustering analysis by using the both models. While the vector space model is easy to implement in folksonomy analysis and even computationally lighter, the graph model is more suitable for analyzing folksonomies which can be represented as a tag graph. To overcome <b>computational</b> <b>burdens</b> occurred in the graph model, we implemented a parallel version of Floyd-Warshall algorithm for finding the shortest paths. ...|$|R
