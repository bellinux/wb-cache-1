230|10000|Public
25|$|Each HTM region learns by {{identifying}} and memorizing spatial patterns - <b>combinations</b> <b>of</b> <b>input</b> bits that often {{occur at the}} same time. It then identifies temporal sequences of spatial patterns {{that are likely to}} occur one after another.|$|E
2500|$|Many {{textbooks}} have reproduced the outdated Penfield-Rasmussen diagram, {{with the}} toes and genitals on the mesial {{surface of the}} cortex when they are actually represented on the convexity. The classic diagram implies a single primary sensory map of the body, when there are multiple primary maps. At least four separate, anatomically distinct sensory homunculi {{have been identified in}} the postcentral gyrus. They represent <b>combinations</b> <b>of</b> <b>input</b> from surface and deep receptors and rapidly [...] and slowly adapting peripheral receptors; smooth objects will activate certain cells, and rough objects will activate other cells.|$|E
5000|$|The {{most likely}} <b>combinations</b> <b>of</b> <b>input</b> {{variables}} which produce satisfactory response values.|$|E
40|$|Here {{we present}} novel {{insights}} regarding the tuning of MSTd neurons for <b>combinations</b> <b>of</b> different <b>input</b> variables using an information-theoretic approach. Neuronal tuning functions {{can be expressed}} by the conditional probability of observing a spike given any <b>combination</b> <b>of</b> <b>input</b> variables. However, accurately determining such probabilistic tuning functions from experimental data poses several challenges such as determining the neuronal latencies and finding the <b>combination</b> <b>of</b> <b>input</b> variables which is most related to neuronal activity. Our approach solves these issues by maximizing the mutual information between the probability distributions of spike occurrence and input variables. |$|R
50|$|The {{long run}} is a {{planning}} and implementation stage. Here a firm may decide {{that it needs}} to produce on a larger scale by building a new plant or adding a production line. The firm may decide that new technology should be incorporated into its production process. The firm thus considers all its long-run production options and selects the optimal <b>combination</b> <b>of</b> <b>inputs</b> and technology for its long-run purposes. The optimal <b>combination</b> <b>of</b> <b>inputs</b> is the least-cost <b>combination</b> <b>of</b> <b>inputs</b> for desired level of output when all inputs are variable. Once the decisions are made and implemented and production begins, the firm is operating in the short run with fixed and variable inputs.|$|R
50|$|Exhaustively testing all <b>combinations</b> <b>of</b> an {{integrated}} circuit with 309 inputs and 1 output requires testing {{of a total}} <b>of</b> 2309 <b>combinations</b> <b>of</b> <b>inputs.</b> Since the number 2309 is a transcomputational number (that is, a number greater than 1093), the problem of testing such a system of integrated circuits is a transcomputational problem. This {{means that there is}} no way one can verify the correctness of the circuit for all <b>combinations</b> <b>of</b> <b>inputs</b> through brute force alone.|$|R
50|$|The four <b>combinations</b> <b>of</b> <b>input</b> {{values for}} p, q, are read by row {{from the table}} above.The output {{function}} for each p, q combination, can be read, by row, from the table.|$|E
50|$|Each HTM region learns by {{identifying}} and memorizing spatial patterns - <b>combinations</b> <b>of</b> <b>input</b> bits that often {{occur at the}} same time. It then identifies temporal sequences of spatial patterns {{that are likely to}} occur one after another.|$|E
50|$|This approach, {{influenced}} by control theory and signal processing, treats neurons and synapses as time-invariant entities that produce outputs that are linear <b>combinations</b> <b>of</b> <b>input</b> signals, often depicted as sine waves with a well-defined temporal or spatial frequencies.|$|E
50|$|As with {{indifference}} curves, two isoquants {{can never}} cross. Also, every possible <b>combination</b> <b>of</b> <b>inputs</b> {{is on an}} isoquant. Finally, any <b>combination</b> <b>of</b> <b>inputs</b> above or {{to the right of}} an isoquant results in more output than any point on the isoquant. Although the marginal product <b>of</b> an <b>input</b> decreases as you increase the quantity <b>of</b> the <b>input</b> while holding all other inputs constant, the marginal product is never negative in the empirically observed range since a rational firm would never increase an input to decrease output.|$|R
50|$|The {{output drive}} of a gate {{is equal to}} the minimum - over all {{possible}} <b>combinations</b> <b>of</b> <b>inputs</b> - <b>of</b> the output drive of the gate for that input.|$|R
5000|$|A {{production}} set is the set <b>of</b> all <b>combinations</b> <b>of</b> <b>inputs</b> and outputs {{that comprise}} a technologically [...] way to produce. It {{is used as}} part of profit maximization calculations.|$|R
50|$|In general a test class' {{predicate}} is a {{conjunction of}} two or more predicates. It is likely, then, that some test classes are empty because their predicates are contradictions. These test classes must be pruned from the testing tree because they represent impossible <b>combinations</b> <b>of</b> <b>input</b> values, i.e. no abstract test case can be derived out of them.|$|E
5000|$|... #Caption: The Mark I Perceptron {{machine was}} the first {{implementation}} of the perceptron algorithm. The machine was connected to a camera that used 20×20 cadmium sulfide photocells to produce a 400-pixel image. The main visible feature is a patchboard that allowed experimentation with different <b>combinations</b> <b>of</b> <b>input</b> features. To the right of that are arrays of potentiometers that implemented the adaptive weights.|$|E
50|$|Contour {{lines are}} also used to display non-geographic {{information}} in economics. Indifference curves (as shown at left) are used to show bundles of goods to which a person would assign equal utility. An isoquant (in the image at right) is a curve of equal production quantity for alternative <b>combinations</b> <b>of</b> <b>input</b> usages, and an isocost curve (also in the image at right) shows alternative usages having equal production costs.|$|E
50|$|Infinite variety: Products are {{differentiated}} by {{quality and}} uniqueness; each product {{is a distinct}} <b>combination</b> <b>of</b> <b>inputs</b> leading to infinite variety options (e.g., works of creative writing, whether poetry, novel, screenplays or otherwise).|$|R
25|$|Given two operands, {{each with}} two {{possible}} values, there are 22 = 4 possible <b>combinations</b> <b>of</b> <b>inputs.</b> Because each output can have two possible values, {{there are a}} total of 24 = 16 possible binary Boolean operations.|$|R
30|$|In Table  4, {{we could}} obtain that {{different}} feature combinations {{are used as}} input parameters so that the accuracy of DBN is different; when the feature <b>combination</b> <b>of</b> <b>input</b> parameters is NDVI[*]+[*]RVI[*]+[*]DVI[*]+[*]EVI, the highest accuracy is 96.19.|$|R
50|$|Response Surface Modeling (RSM) is a {{collection}} of mathematical and statistical techniques that are useful to model and analyze problems in which a design response of interest is influenced by several design parameters. DOE methods in combination with RSM can predict design response values for <b>combinations</b> <b>of</b> <b>input</b> design parameters that were not previously calculated, with very little simulation effort. RSM thus allows further post-processing of DOE results.|$|E
50|$|Ordinary {{principal}} component analysis (PCA) uses a vector space transform to reduce multidimensional data sets to lower dimensions. It finds linear <b>combinations</b> <b>of</b> <b>input</b> variables, and transforms them into new variables (called {{principal component}}s) that correspond to directions of maximal variance in the data. The number of new variables created by these linear combinations is usually {{much lower than the}} number of input variables in the original dataset, while still explaining most of the variance present in the data.|$|E
50|$|In practice, the Xs are approximations to the S-boxes (substitution components) of block ciphers. Typically, X {{values are}} inputs to the S-box and Y values are the {{corresponding}} outputs. By simply {{looking at the}} S-boxes, the cryptanalyst can tell what the probability biases are. The trick is to find <b>combinations</b> <b>of</b> <b>input</b> and output values that have probabilities of zero or one. The closer the approximation is to zero or one, the more helpful the approximation is in linear cryptanalysis.|$|E
50|$|Given two operands, {{each with}} two {{possible}} values, there are 22 = 4 possible <b>combinations</b> <b>of</b> <b>inputs.</b> Because each output can have two possible values, {{there are a}} total of 24 = 16 possible binary Boolean operations.|$|R
30|$|The multivariate {{sensitivity}} analysis then investigates {{the sensitivity of}} the desire to participate variable to various <b>combinations</b> <b>of</b> <b>inputs</b> values (the level of culture and level of technology). This test can confirm that the model is consistent for any possible value.|$|R
40|$|This article {{developed}} an approached model of congestion, based on relaxed <b>combination</b> <b>of</b> <b>inputs,</b> in stochastic {{data envelopment analysis}} (SDEA) with chance constrained programming approaches. Classic data envelopment analysis models with deterministic data {{have been used by}} many authors to identify congestion and estimate its levels; however, data envelopment analysis with stochastic data were rarely used to identify congestion. This article used chance constrained programming approaches to replace stochastic models with ‘‘deterministic equivalents”. This substitution leads us to non-linear problems that should be solved. Finally, the proposed method based on relaxed <b>combination</b> <b>of</b> <b>inputs</b> was used to identify congestion input in six Iranian hospital with one input and two outputs in the period of 2009 to 2012...|$|R
50|$|Many {{textbooks}} have reproduced the outdated Penfield-Rasmussen diagram, {{with the}} toes and genitals on the mesial {{surface of the}} cortex when they are actually represented on the convexity. The classic diagram implies a single primary sensory map of the body, when there are multiple primary maps. At least four separate, anatomically distinct sensory homunculi {{have been identified in}} the postcentral gyrus. They represent <b>combinations</b> <b>of</b> <b>input</b> from surface and deep receptors and rapidly and slowly adapting peripheral receptors; smooth objects will activate certain cells, and rough objects will activate other cells.|$|E
50|$|A Warnier/Orr diagram (also {{known as}} a logical {{construction}} of a program/system) {{is a kind of}} hierarchical flowchart that allows the description of the organization of data and procedures. They were initially developed in France by Jean-Dominique Warnier and in the United States by Kenneth Orr. This method aids the design of program structures by identifying the output and processing results and then working backwards to determine the steps and <b>combinations</b> <b>of</b> <b>input</b> needed to produce them. The simple graphic method used in Warnier/Orr diagrams makes the levels in the system evident and the movement of the data between them vivid.|$|E
5000|$|A {{control table}} can be {{constructed}} along similar lines to a language dependent switch statement but with the added possibility of testing for <b>combinations</b> <b>of</b> <b>input</b> values (using boolean style AND/OR conditions) and potentially calling multiple subroutines (instead of just a single set of values and 'branch to' program labels). (The switch statement construct in any case may not be available, or has confusingly differing implementations in high level languages (HLL). The control table concept, by comparison, has no intrinsic language dependencies, but might nevertheless be implemented differently according to the available data definition features of the chosen programming language.) ...|$|E
50|$|A {{molecular}} {{logic gate}} is a molecule that performs a logical operation on one or more logic inputs and produces a single logic output. Unlike a molecular sensor, the {{molecular logic gate}} will only output when a particular <b>combination</b> <b>of</b> <b>inputs</b> are present.|$|R
50|$|Long run {{average cost}} is the unit cost of {{producing}} a certain output when all inputs are variable. The behavioral assumption is that the firm will choose that <b>combination</b> <b>of</b> <b>inputs</b> that will produce the desired quantity at the lowest possible cost.|$|R
40|$|DEA {{has been}} {{extensively}} {{used to measure the}} efficiency of financial institutions. Its advantages are clearly understood. But there are many unresolved problems. There are various views based on different modelling philosophies <b>of</b> what constitutes <b>inputs</b> and outputs in a financial institution. The paper explores up to what point the various <b>combinations</b> <b>of</b> <b>inputs</b> and outputs are equivalent, and up to what point the efficiency score obtained by a given institution changes under the various <b>combinations</b> <b>of</b> <b>inputs</b> and outputs. The extent to which two institutions that achieve the same efficiency score arrive at it following different strategies is explored with the aim of finding out what is behind such a score. It is suggested that, not one but many different DEA specifications, containing different <b>combinations</b> <b>of</b> <b>inputs</b> and outputs, be modelled and that the results be analysed with the tools of multivariate statistics. Particular emphasis is placed on using tools that visualise the main characteristics of the data. By-products of the approach proposed here are the creation of league tables of financial institutions in terms of efficiencies and the possibility to assess strengths and weaknesses of individual institutions. This methodology is applied to the particular case of Spanish savings banks (Cajas de Ahorros) and proves to be particularly rewarding...|$|R
5000|$|... #Caption: Isocost v. Isoquant Graph. Each {{line segment}} is an isocost line {{representing}} one particular level of total input costs, denoted TC, with PL being the unit price {{of labor and}} PK the unit price of physical capital. The convex curves are isoquants, each showing various <b>combinations</b> <b>of</b> <b>input</b> usages {{that would give the}} particular output level designated by the particular isoquant. Tangency points show the lowest cost input combination for producing any given level of output. A curve connecting the tangency points is called the expansion path because it shows how the input usages expand as the chosen level of output expands.|$|E
50|$|SELDM is a {{stochastic}} model because it uses Monte Carlo methods {{to produce the}} random <b>combinations</b> <b>of</b> <b>input</b> variable values needed to generate the stochastic population of values for each component variable. SELDM calculates the dilution of runoff in the receiving waters and the resulting downstream event mean concentrations and annual average lake concentrations. Results are ranked, and plotting positions are calculated, to indicate the level of risk of adverse effects caused by runoff concentrations, flows, and loads on receiving waters by storm and by year. Unlike deterministic hydrologic models, SELDM is not calibrated by changing values of input variables to match a historical record of values. Instead, input values for SELDM are based on site characteristics and representative statistics for each hydrologic variable. Thus, SELDM is an empirical model based on data and statistics rather than theoretical physicochemical equations.|$|E
30|$|The {{original}} RF method {{can handle}} cases {{with only a}} few inputs. The only viable solution to this problem is to increase the dimensionality of the feature space, which can be done by creating linear <b>combinations</b> <b>of</b> <b>input</b> features. Forest-RC (Breiman 2001) is one such method and was introduced to define more features by using random linear <b>combinations</b> <b>of</b> <b>input</b> variables.|$|E
50|$|There are {{significant}} differences, however, in how {{software and hardware}} behave. Most hardware unreliability {{is the result of}} a component or material failure that results in the system not performing its intended function. Repairing or replacing the hardware component restores the system to its original operating state. However, software does not fail in the same sense that hardware fails. Instead, software unreliability is the result of unanticipated results of software operations. Even relatively small software programs can have astronomically large <b>combinations</b> <b>of</b> <b>inputs</b> and states that are infeasible to exhaustively test. Restoring software to its original state only works until the same <b>combination</b> <b>of</b> <b>inputs</b> and states results in the same unintended result. Software reliability engineering must take this into account.|$|R
40|$|This report {{presents}} two further experiments {{over the}} Aphid data set. The {{first is a}} more detailed investigation of the sensitivity of Simple Evolving Connectionist Sys-tem (SECoS) networks to the exclusion <b>of</b> various <b>combinations</b> <b>of</b> <b>inputs.</b> This {{is in contrast to}} the previous work (Watts, 2004), where only the effect of excluding singl...|$|R
40|$|In {{this paper}} I present a {{methodology}} to produce nearly instantaneous model results given a <b>combination</b> <b>of</b> <b>input</b> parameter values. This {{is achieved by}} selecting from a set of pre-computed models the one that better matches the input parameters provided. In this paper I analyze, first, how to define the <b>combination</b> <b>of</b> <b>input</b> parameters to more evenly populate the parameters’ space, and second, how to determine which is the model that corresponds to the parameters specified. For the first part I will use a Latin Hypercube approach, detailing certain particularities related to the probability distribution to be used, and for the second I will explain how the selection can be done using a simple homogenized Euclidian distance calculation. I devised this methodology {{in the context of}} the CI-WATER project, with the intention to use it in the development of flood early warning systems...|$|R
