18|910|Public
5000|$|Collect {{a list of}} {{relevant}} pages for the results. These are ranked based on <b>content,</b> <b>link</b> citation data and usage data.|$|E
5000|$|There {{are many}} {{applications}} available which advertise themselves as Thin Clients or web interfaces. There are no open source development tools in widespread use that interface different entities in a persistent, secure manner using a thin client through web-browsers. [...] While {{there are many}} tools available, {{many of them are}} highly specialized and require proprietary software installed on both the client and server side. There are still other tools which produce virtual environments, avatars, and content, but none of the ones known tie them together, spontaneously create interfaces among them or provide seamless communications to all of the entities regardless of their timing and without a heavy burden in bandwidth or processing power for the client. VWF does this and will do more as it continues its evolution. VWF utilizes web sockets and WebGL, directly providing a <b>content</b> <b>link</b> between entities and build 3D objects in the quickest, most efficient manner possible. Other virtual interfacing programs utilize Flash which also provides direct content, but does so in a less efficient manner.|$|E
40|$|Search engines use {{content and}} link {{information}} to crawl, index, retrieve, and rank Web pages. The correlations between similarity measures {{based on these}} cues and on semantic associations between pages therefore crucially affects the performance of any search tool. Here I begin to quantitatively analyze the relationship between <b>content,</b> <b>link,</b> and semantic similarity measures across a massive number of Web page pairs. Maps of semantic similarity across textual and link similarity highlight the potential and limitations of lexical and link analysis for relevance approximation, and {{provide us with a}} way to study whether and how text and link based measures should be combined...|$|E
50|$|Hanshin <b>Contents</b> <b>Link,</b> Corp.|$|R
5000|$|PCT/SG98/00096.Method and {{apparatus}} for <b>content</b> <b>linking</b> supplemental {{information with}} time sequence data ...|$|R
5000|$|What Would the Founders Do?: Our Questions, Their Answers, 261 pages (Basic Books: 2006) [...] <b>Contents</b> <b>links.</b>|$|R
40|$|This brief report, {{prepared}} by Scott Ewing for the Australian Digital Inclusion Index, includes graphs {{of a few}} of the key data points that shed light on digital inclusion in Australia from the ABS’s recently released Household Use of Information Technology in Australia, 2014 - 15. The full data released can be accessed in the related <b>content</b> <b>link</b> below. The data shows that while the digital divide continues to narrow, persistent and significant differences remain between different groups of Australians in relation to both access and use of the internet. These differences are related to attributes including income, age, employment status, education levels and location...|$|E
40|$|First of {{all this}} study {{discusses}} {{the problem of the}} investigation of scientific information and of structured presentation of scientific information sources. As consequence of the conclusion that a universally usable information investigation system does not exist we designed a model of a system which frees the theoretical and intellectual process from technical ballast. It is based on two central functional elements: 1. The standardized compressing of the contents of information sources guarantees that the user finds quickly and exactly the information of his special interest. 2. The visualising principle generates a three-dimensional picture from a one dimensional information resource. This concept offers a functional and <b>content</b> <b>link</b> system for any information pool...|$|E
40|$|In Canberra today, {{leaders in}} suicide and mental health, led by {{consultancy}} ConNetica and the Brain and Mind Centre University of Sydney, released details on suicide across 28 Federal electorates. Supporting the release were four parents with lived experience of suicide and the National Congress for Australia’s First People. The {{information has been}} released with a call for all major parties and candidates in the 2 July election to spell out {{what they will do}} in the next Parliament to address the rising toll of suicide and self-harm across Australia. A useful web portal, which has been constructed for interested parties to access data breakdowns by electorate, has been included as a related <b>content</b> <b>link...</b>|$|E
5000|$|The Collected Scientific Papers of Paul A. Samuelson, MIT Press. Preview links for vol. 1-3 below. <b>Contents</b> <b>links</b> for vol. 4-7.|$|R
5000|$|Lazear, Edward et al., ed. (2004). Personnel Economics, Elgar, with 43 {{articles}} {{dating from}} 1962 to 2000 (<b>link</b> to <b>contents</b> <b>link</b> here).|$|R
25|$|Both links {{lead to a}} {{whole book}} on the program. For the HTML one, scroll down to see the table of <b>contents</b> <b>link.</b>|$|R
40|$|Abstract. The open {{nature of}} the World Wide Web makes {{evaluating}} webpage credibility challenging for users. In this paper, we aim to auto-matically assess web credibility by investigating various characteristics of webpages. Specifically, we first identify features from textual <b>content,</b> <b>link</b> structure, webpages design, {{as well as their}} social popularity learned from popular social media sites (e. g., Facebook, Twitter). A set of sta-tistical analyses methods are applied to select the most informative fea-tures, which are then used to infer webpages credibility by employing supervised learning algorithms. Real dataset-based experiments under two application settings show that we attain an accuracy of 75 % for classification, and an improvement of 53 % for the mean absolute error (MAE), with respect to the random baseline approach, for regression...|$|E
40|$|Search engine {{optimization}} is {{the process}} of improving a website`s position so that the webpage comes up higher in the search results of major search engines. All search engines have a unique way of ranking the importance of a website. Some search engines focus on the content while others review Meta tags to identify who and what a web site`s business is. Most engines use a combination of Meta tags, <b>content,</b> <b>link</b> popularity, click popularity and longevity to determine a sites ranking. To make it even more complicated, they change their ranking policies frequently however this study will attempt to ensure that you now have the insiders ‘know-how’ to ensure that your website (s) has the necessary ranking criteria to appeal to each individual search engines needs...|$|E
40|$|The Web is a {{major source}} of {{accurate}} information for users. It is a dynamic environment that is changing all the time. It consists of three dynamic components: <b>Content,</b> <b>Link</b> and Usage. These components are the goal of web search engine researchers. For search engines to do their job properly, they must pass through many steps: crawling, indexing, retrieving and ranking. Unfortunately, recent search engines suffer from a serious problem, that is: they can't keep up with this dynamic environment of the Web. Their indexes are not updated frequently. This paper presents a model that uses Ants to analysis the content of Web Space and Usage Space to achieve a well organized Web. The Content Space and the Usage Space is the field of our research. We will simulate this environment using Multi Agents System and Complex Adaptive System...|$|E
50|$|One of the {{companys}} subsidiaries is the Osaka-based company Hanshin <b>Contents</b> <b>Link,</b> {{that operates}} the Billboard Japan brand under licence from Billboards publisher.|$|R
50|$|The XSLT {{transformation}} step is exceptionally powerful. It {{allows for}} the automatic generation of a table of <b>contents,</b> <b>linked</b> references, an index, and various other possibilities.|$|R
5000|$|High-speed fibre Wi-Fi network - giving brands {{a portal}} to host long-form <b>content</b> <b>linked</b> to the screen, or as a data feed to trigger content {{on the screen}} ...|$|R
40|$|This {{year the}} UvA-MediaMill team {{participated in the}} Feature Extraction and Search Task. We {{developed}} a generic approach for semantic concept classification using the semantic value chain. The semantic value chain extracts concepts from video documents based on three consecutive analysis links, named the <b>content</b> <b>link,</b> the style link, and the context link. Various experiments within the analysis links were performed, showing amongst others the merit of processing beyond key frames, the value of style elements, {{and the importance of}} learning semantic context. For all experiments a lexicon of 32 concepts was exploited, 10 of which are part of the Feature Extraction Task. Top three system-based ranking in 8 out of the 10 benchmark concepts indicates that our approach is very promising. Apart from this, the lexicon of 32 concepts proved very useful in an interactive search scenario with our semantic video search engine, where we obtained the highest mean average precision of all participants. ...|$|E
40|$|This Bachelor's thesis {{deals with}} the matter of {{audiovisual}} piracy. It discusses the question of audiovisual piracy being caused not by the wrong interpretation of law but {{by the lack of}} competitiveness among websites with legal audiovisual content. This thesis questions the quality of legal interpretation in the matter of audiovisual piracy and focuses on its sufficiency. It analyses the responsibility of website providers, providers of the illegal content, the responsibility of illegal <b>content</b> <b>link</b> providers and legal responsibility of anyone who downloads the illegal content itself. In the end of this thesis the research results are analysed. The practical part of this thesis is devoted to the SWOT analysis of websites providing legal and illegal audiovisual content. The SWOT analysis results support the main idea of this thesis and also give us an outline of possible evolution of the audiovisual piracy. This prediction is mainly focused on the Netflix website. The SWOT analysis and website comparison are based on survey...|$|E
40|$|This report {{provides}} information on {{the operation of the}} National Electricity Market (NEM) and national power grid on Wednesday 8 February during a heatwave in eastern Australia. AEMO will release a second report on 22 February, focused on the events of 10 February. (see related <b>content</b> <b>link</b> below) During this heatwave period, involuntary load reduction was necessary on two occasions to preserve system security: · On 8 February 2017 in South Australia, the power system was not in a secure operating state for over 30 minutes. AEMO directed interruption of supply to 100 megawatts (MW) of customer load in South Australia and gave clearance to restore that load 27 minutes later. Following this direction, approximately 300 MW was interrupted. The reason for the additional interruption is being investigated. · On 10 February, in New South Wales, AEMO directed Transgrid to shed one of the Tomago Aluminium smelter potlines (290 MW), and cancelled the direction one hour later. This report focuses on South Australia’s electricity supply on Wednesday 8 February 2017...|$|E
50|$|When {{the colour}} series was {{subsequently}} released on DVD, {{some of the}} episodes whose <b>content</b> <b>linked,</b> were edited together, with the relevant closing and opening titles and credits removed.|$|R
50|$|A {{retrospective}} {{collection of}} the personnel economics-literature is in Lazear et al., ed. (2004), Personnel Economics, Elgar, with 43 articles dating from 1962 to 2000 (<b>link</b> to <b>contents</b> <b>link</b> here).|$|R
5000|$|Qi, G. J., Aggarwal, C., Tian, Q., Ji, H., Huang, T. S. (2012), [...] "Exploring Context and <b>Content</b> <b>Links</b> in Social Media: A Latent Space Method", IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 850-862.|$|R
40|$|Background and Aims: Mental Health Advance Directives (MHADs) are {{potentially}} useful for Bipolar patients {{due to the}} episodic characteristic of their disease. Interest {{for the development of}} a Cognitive Behavioural Therapy (CBT) approach in MHAD creation process arises due to a lack of efficiency of non collaborative process, potential impact of psychopathology and awareness of illness in MHADs <b>content,</b> <b>link</b> of patient's directives interest with the case manager interest, and the lower interest reported by the potentially high MHADs beneficiary. CBT intervention here proposed in MHAD creative process bases itself on: The self determination model for adherence, the cognitive representation of illness model and Concordance. Methods: The principles of the intervention were adapted from Motivational interviewing's (MI) emphasising personal choice and responsibility, and focusing on patient's concerns about treatment through Socratic dialogue. Results: During the course of 2004, 20 advance directives written by patients (17 with bipolar disorder, three with schizoaffective disorder) have been collected following the described intervention. Conclusions: Despite the presented preliminary feasibility data of this cognitive collaborative approach focusing on MHAD, its effect remain to be demonstrated. Controlled prospective studies are needed...|$|E
40|$|Abstract. Content {{and link}} {{information}} from the Web is used by search engines to crawl, index, retrieve, and rank pages. The correlations between similarity measures based on these cues and on semantic associations between pages is therefore crucial in determining the performance of any search tool. A great deal of research is under way {{to understand how to}} automatically extract semantic information from Web pages by mining their text and links. Here I quantitatively analyze the relationship between <b>content,</b> <b>link,</b> and semantic similarity measures across a massive number of Web page pairs. Maps of semantic similarity across textual and link similarity domains help visualize the potential and limitations of content and link analysis for relevance approximation, and provide us with a way to analyze whether and how text and link based measures should be combined. Highly heterogeneous topical maps suggest that links and content analysis should be specialized based on search context. Finally I show how semantic maps can be used to evaluate the performance of search engines in a semi-supervised fashion, by identifying a single relevant page for a given query. The methodology is illustrated by graphing precision-recall plots for three commercial search engines based on TREC queries...|$|E
40|$|Abstract. In a {{distributed}} system {{such as a}} Data Grid, it is desirable to maintain and query dynamic and timely information about active participants such as services, resources and user communities. This enables information discovery and collective collaborative functionality that operate on {{the system as a}} whole, rather than on a given part of it. However, it is not obvious how a database (registry) should maintain information populated from a large variety of unreliable, frequently changing, autonomous and heterogeneous remote data sources. In particular, how can one avoid sacrificing reliability, predictability and simplicity while allowing to express powerful queries over time-sensitive dynamic information? We propose the so-called hyper registry, which has a number of key properties. An XML data model allows for structured and semi-structured data, which is important for integration of heterogeneous content. The XQuery language allows for powerful searching, which is critical for non-trivial applications. Database state maintenance is based on soft state, which enables reliable, predictable and simple content integration from a large number of autonomous distributed content providers. <b>Content</b> <b>link,</b> content cache and a hybrid pull/push communication model allow {{for a wide range of}} dynamic content freshness policies, which may be driven by all three system components: content provider, hyper registry and client. Key words. Dynamic Database, XQuery, Service Discover...|$|E
50|$|The Billboard Japan Hot 100 is a music singles chart in Japan. It {{has been}} {{compiled}} by Billboard Japan and Hanshin <b>Contents</b> <b>Link</b> ever since February 2008. The chart is updated every Wednesday at billboard-Japan.com (JST) and every Thursday at billboard.com (UTC).|$|R
40|$|Abstract—Social media {{networks}} contain both {{content and}} context-specific information. Most existing methods work {{with either of}} the two for the purpose of multimedia mining and retrieval. In reality, both content and context information are rich sources of information for mining, and the full power of mining and processing algorithms can be realized only {{with the use of a}} combination of the two. This paper proposes a new algorithm, which mines both context and <b>content</b> <b>links</b> in social media networks to discover the underlying latent semantic space. This mapping of the multimedia objects into latent feature vectors enables the use of any off-theshelf multimedia retrieval algorithms. Compared to the state-of-the-art latent methods in multimedia analysis, this algorithm effectively solves the problem of sparse context links by mining the geometric structure underlying the <b>content</b> <b>links</b> between multimedia objects. Specifically for multimedia annotation, we show that an effective algorithm can be developed to directly construct annotation models by simultaneously leveraging both context and content information based on latent structure between correlated semantic concepts. We conduct experiments on the Flickr data set which contains user tags linked with images. We illustrate the advantages of our approach over the state-of-the-art multimedia retrieval techniques. Index Terms—Context and <b>content</b> <b>links,</b> latent semantic space, low-rank method, social Media, multimedia information networks,. ...|$|R
50|$|Heeii, {{formerly}} known as Elkoog B.V., is a company based in Groningen, Netherlands that provides a recommendation service for web browsers {{by means of a}} plug-in. Website visitors that use Heeii recommendations are able to get the most appropriate <b>content</b> <b>links</b> that result in reaching their online objectives.|$|R
40|$|This paper {{presents}} {{a study of}} the various aspects of link prediction and page ranking in blogs. Social networks have taken on a new eminence from the prospect of the analysis of social networks, which is a recent area of research which grew out of the social sciences as well as the exact sciences, especially with the computing capacity for mathematical calculations and even modelling which was previously impossible. An essential element of social media, particularly blogs, is the hyperlink graph that connects various pieces of <b>content.</b> <b>Link</b> prediction has many applications, including recommending new items in online networks (e. g., products in eBay and Amazon, and friends in Face book), monitoring and preventing criminal activities in a criminal network, predicting the next web page users will visit, and complementing missing links in automatic web data crawlers. Page Rank is the technique used by Google to determine importance of page on the web. It considers all incoming links to a page as votes for Page Rank. Our findings provide an overview of social relations and we address the problem of page ranking and link prediction in networked data, which appears in many applications such as network analysis or recommended systems. Keywords- web log, social networks analysis, readership, link prediction, Page ranking. I...|$|E
40|$|This report {{considers}} {{the circumstances in}} which a person can lawfully use force in self-defence, in particular how the law should treat mistaken beliefs in the need for self-defence arising from delusions caused by mental illness, heightened perceptions of danger produced by psychological factors, intoxication and drug-induced psychosis. The report also contributes to the current conversation about family violence by considering the operation of the defence where victims of family violence take self-protective action. Background to this Final Report In September 2011, the Director of Public Prosecutions wrote to the Attorney-General to raise concerns that the current Tasmanian law on self-defence, as contained in s 46 of the Criminal Code (Tas), was too lenient and was out of step with modern standards. In November 2012, the Attorney-General requested that the Tasmania Law Reform Institute conduct a far-reaching examination of the law in Tasmania relating to self-defence and provide advice as to whether the law should be amended. The Institute released an Issues Paper in November 2014, Self-defence, Issues Paper No 20 and a Submission Template, which contained a series of case scenarios, with a call for submissions by 20 February 2015. The Institute received 10 responses to the Issues Paper (see related <b>content</b> <b>link)</b> ...|$|E
40|$|From the {{analytical}} viewpoint a norm can formally {{be regarded as}} a right-duty (or claim-obligation) relation (1) that regulates behaviour (action/inaction) (2) among subjects (3) in definite space (4) and time (5). Consequently, a legal order can be defined as a system of right-duty (or claim-obligation) relations that regulate behaviour (action/inaction) among subjects in definite space and are procedurally organised in the vertical and horizontal sense according to time. An iconic representation of these minimum necessary concepts allows switching from natural language to a purely formal representation of the legal system(s) (deontic network). Within this ‘geometrical’ frame, general principles (the ‘basis’) of (international) law can be conceived of as general obligations, i. e. obligations erga omnes (towards everyone). Obligations erga omnes (ties), indivisible or divisible because of their <b>content,</b> <b>link</b> a subject (node) to every other subject of international law, endowed with a correlative claim (s – s), so that the whole obligations erga omnes are matched by the whole claims erga omnes of all the subjects of international law. Indivisible obligations erga omnes are unavailable from the viewpoint of the power, so cogentes, breaches violate necessarily all the correlative claims, possibly enabling every subject to invoke the responsibility and impose sanctions. Correspondingly, sanctions should be regarded as indivisible obligations erga omnes, the violation of which allows universal enforcement. Nevertheless, specifically by reason of the gravity of the breach, it is possible to split primary and secondary norms, conceiving of the sanction as a bilateral relation allowing solely reciprocal enforcement {{in the case of an}} infringement. Divisible obligations erga omnes are available from the viewpoint of the power, so dispositivae, breaches must be seen as relative, enabling only the subject(s) injured to invoke the responsibility and impose sanctions. Correspondingly, sanctions should be regarded as bilateral obligations, the infringement of which gives rise to reciprocal enforcement. Nevertheless, it is possible to figure out that specifically the gravity of the breach ‘unifies’ the primary divisible obligation, allowing universal invocation of the responsibility, so that the secondary obligation could be either bilateral or a general indivisible one, respectively permitting relative or absolute enforcement in the case of a breach...|$|E
40|$|The Automatic <b>Content</b> <b>Linking</b> Device {{monitors}} {{a conversation}} and uses automatically recognized words to retrieve documents {{that are of}} potential use to the participants. The document set includes project related reports or emails, transcribed snippets of past meetings, and websites. Retrieval results are displayed at regular intervals. © 2010 ACM...|$|R
40|$|There {{are huge}} amounts of User Generated Contents (UGCs) {{consisting}} of authors’ articles of different themes and readers’ on-line comments on social networks every day. Generally, an article often gives rise to thousands of readers’ comments, which are related to specific points of the originally published article or previous comments. Hence it has suggested {{the urgent need for}} automated methods to implement the <b>content</b> <b>linking</b> task, which can also help other related applications, such as information retrieval, summarization and content management. So far <b>content</b> <b>linking</b> is still a relatively new issue. Because of the unsatisfactory of traditional ways based on feature extraction, we look forward to using deeper textual semantic analysis. The Word Embedding model based on deep learning has performed well in Natural Language Processing (NLP), especially in mining deep semantic information recently. Therefore, we study further on the Word Embedding model trained by different neural network models from which we can learn the structure, principles and training ways of the neural network language model in more depth to complete deep semantic feature extraction. With the aid of the semantic features, we expect to do further research on <b>content</b> <b>linking</b> between comments and their original articles from social networks, and finally verify the validity of the proposed method by comparison with traditional ways based on feature extraction...|$|R
50|$|Audio search {{has evolved}} slowly through several basic search formats which exist today and all use {{keywords}}. The keywords for each search {{can be found}} in the title of the media, any text attached to the media and <b>content</b> <b>linked</b> web pages, also defined by authors and users of video hosted resources.|$|R
