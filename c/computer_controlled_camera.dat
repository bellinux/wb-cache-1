3|10000|Public
40|$|This paper {{define a}} spatiotemporal mutual {{information}} on the pixels of a given video image {{on the basis of}} information theory (Shannon's communication theory), which can be interpreted as the theoretical estimation of interested spots for human being. As an application of this spatiotemporal mutual information, we propose a method of producing a vivid video image of the distance learning by using the <b>computer</b> <b>controlled</b> <b>camera</b> operation and switching of plural camera images {{on the basis of the}} video image processing. The results of questionnaire survey for the produced video image confirm the effectiveness of our approach. 1...|$|E
40|$|This paper {{define a}} spatiotemporal mutual {{information}} on the pixels of a given video image {{on the basis of}} information theory (Shannon's communication theory), which can be interpreted as the theoretical estimation of interested spots for human being. As an application of this spatiotemporal mutual information, we propose a method of producing a vivid video image of the distance learning by using the <b>computer</b> <b>controlled</b> <b>camera</b> operation and switching of plural camera images {{on the basis of the}} video image processing. The results of questionnaire survey for the produced video image confirm the effectiveness of our approach. 1. Introduction Recent years, high-speed digital data transmission becomes possible accompanied with development of network technique, and a distant learning system, which uses communications satellite or ISDN network, is developed not only at universities but other kind of school and companies [1] [2]. One of the representative methods takes video image of distant [...] ...|$|E
40|$|A Charge Injection Device (CID) {{detector}} {{has been}} evaluated as a detector for simultaneous multielement atomic emission spectroscopy. The CID {{was incorporated into}} a special liquid nitrogen cooled, <b>computer</b> <b>controlled</b> <b>camera</b> system. Electro-optical characterization of the CID and camera system included determination of readout noise, quantum efficiency, spatial crosstalk, temporal hysteresis, spatial response uniformity, and linear dynamic range. The CID {{was used as a}} spectroscopic detector for an echelle grating spectrometer equipped with a direct current plasma emission source. The spectrometer was a standard commercial instrument modified to provide a reduced image format more suitable for use with the CID detector. The optical characteristics of this spectrometer, including wavelength coverage, and optical aberrations are described. The spectroscopic system was evaluated with respect to detection limits, linear dynamic range, and accuracy in both single element and simultaneous multielement modes. Detection limits compared well to literature values reported for photomultiplier tube detector based systems under similar conditions. CID detection limits were superior in the near infrared and visible wavelength region, comparable in the middle UV, and higher in the far UV. The detection limits were determined to be limited by background radiation shot noise. Several elements of a certified standard reference material were simultaneously determined in order to assess the accuracy of the spectroscopic system. The results were highly accurate, even when operating near or below the 3 σ limits of detection. Spectral interferences for elements were avoided by using several analytical lines for each element. The results of these investigations indicate that the CID is a superior multichannel detector for analytical atomic emission spectrometry. The capability to simultaneously monitor a wide, continuous spectral range with high spatial resolution, high dynamic range, low readout noise, and insignificant signal crosstalk is now possible. Many analytical benefits of this approach, such as the potential capability to perform rapid qualitative and semiquantitative analysis and the ability to select the optimum spectral lines for highly accurate quantitative analysis are now readily achievable...|$|E
50|$|Earlier this morning, Mission Specialist Dan Bursch took a {{break from}} his work to provide a {{television}} tour of the crew's orbital home office, explaining the shuttle's displays, <b>controls,</b> <b>computers</b> and <b>cameras,</b> as well as living accommodations.|$|R
40|$|Atmospheric {{visibility}} {{is measured}} with a low-cost consumer digital camera. By using the dark pixel approach the visibility can be derived {{beyond the limits of}} the landmark detection method. The image gathering and processing is totally automated by <b>computer</b> <b>controlling</b> the digital <b>camera.</b> This approach is tested in different atmospheric conditions and the results are validated by measurements made by a human observer...|$|R
40|$|<b>Computer</b> <b>controls</b> remote {{television}} <b>camera.</b> General-purpose controller {{developed to}} serve as interface between host computer and pan/tilt/zoom/focus functions on series of automated video cameras. Interface port based on 8251 programmable communications-interface circuit configured for tristated outputs, and connects controller system to any host computer with RS- 232 input/output (I/O) port. Accepts byte-coded data from host, compares them with prestored codes in read-only memory (ROM), and closes or opens appropriate switches. Six output ports control opening and closing {{of as many as}} 48 switches. Operator <b>controls</b> remote television <b>camera</b> by speaking commands, in system including general-purpose controller...|$|R
40|$|This thesis {{describes}} the design, construction {{and operation of}} a prototype wide field polarimeter {{to be used in}} large scale polarisation studies of the sky. A personal <b>computer</b> <b>controls</b> the stepper motors which drive an equatorial mount arrangement, whilst a lap top <b>computer</b> <b>controls</b> the CCD <b>camera</b> software. The third, fourth and fifth chapters describe the initial design process. The sixth chapter describes in detail the final design of the telescope mount. The seventh chapter gives an insight into how the structure was fabricated. Finally, the thesis {{describes the}} commissioning and operation of the telescope mount, with a section incorporating some results obtained from observing Jupiter...|$|R
40|$|This paper {{describes}} a recently completed electrooptical camera flying onboard the NASA ER- 2 high altitude aircraft. The device includes a six-position filter wheel {{which can be}} fitted {{with a combination of}} polarizing and/or spectral filters. An alternate configuration will include a polarizing filter which can be rotated to any angle under <b>computer</b> <b>control.</b> The <b>camera</b> mount in the nose of the ER- 2 can tilt forward or aft up to 40 degrees, both for bidirectional reflectance studies and for image motion compensation (the aircraft moves 34 meters between frame acquisitions). The ground resolution is nominally 5 meters from and altitude of 20 km. Spectral responsivity is that of the silicon imaging array (Kodak KAF- 1400). Initial data sets were acquired in support of the International Satellite Cloud Climatology Program Regional Experiment of November, 1991, and will be used to study cirrus cloud properties...|$|R
40|$|A {{new method}} is {{described}} for recovering {{the distance of}} objects in a scene from images formed by lenses. The recovery is based on measuring {{the change in the}} scene's image due to a known change in the three intrinsic camera parameters: (i) distance between the lens and the image detector, (ii) focal length of the lens, and (iii) diameter of the lens aperture. The method is parallel involving simple local computations. In comparison with stereo vision and structure-frommotion methods, the correspondence problem does not arise. This method for depth-map recovery may also be used for (i) obtaining focused images (i. e. images having large depth of field) from two images having finite depth of field, and (ii) rapid autofocusing of <b>computer</b> <b>controlled</b> video <b>cameras.</b> 1. Introduction Here we describe a new passive ranging method which in principle is fast and involves relatively weak assumptions that are generally valid. The method is basically a generalized version of the `depth-from-focu [...] ...|$|R
40|$|Over {{the last}} ten years several {{observations}} have been made of compressive failure in glass by a so called fracture wave. A high-speed photographic study has been conducted in order to observe the propagation of fracture waves in glass. Streak and framing photography have been used to determine details of the wave speed and surface structure of fracture waves induced in glasses by planar impact. A 50 mm single stage gas gun was used to launch copper flyer plates at velocities of up to 1 km s- 1. A <b>computer</b> <b>controlled</b> high-speed <b>camera</b> was used capable of exposure and interframe times from 50 ns upwards. Simultaneous measurements of the longitudinal stresses were made using manganin pressure gauges embedded in the samples. Results will be presented showing separation between the shock and fracture fronts suggesting that the failure mechanism is by compression rather than resulting from relief waves propagating from the free surfaces...|$|R
40|$|The great {{flexibility}} of a view camera allows to take high quality photographs {{that would not}} be possible any other way. But making a given object into focus is a long and tedious task, although the underlying laws are well known. This paper presents the result of a project which has lead to the design of a <b>computer</b> <b>controlled</b> view <b>camera</b> and to its companion software. Thanks to the high precision machining of its components, and to the known optical parameters of lenses and sensor, {{we have been able to}} consider a reliable mathematical model of the view camera, allowing the acquisition of 3 D coordinates to build a geometrical model of the object. Then many problems can be solved, e. g. minimizing the f-number while maintaining the object within the depth of field, which takes the form of a constrained optimization problem. All optimization algorithms have been validated on a virtual view camera before implementation on the prototypeComment: 17 pages, 19 figure...|$|R
40|$|This paper explores issues {{surrounding}} interaction with virtual (computer generated) objects which are interfaced to real world devices. In addition a virtual room corresponding {{to a real}} physical room has been created to allow collaborative meetings with those physically and virtually present. In particular we have created a set of VRML (Virtual Reality Modeling Language) cameras, which interface to real <b>computer</b> <b>controlled</b> <b>cameras</b> in a “smart ” room. Interacting with the virtual <b>camera</b> <b>controls</b> the real <b>camera.</b> A second interaction method is through a Java applet, which appears on a Web page as {{an image of the}} remote <b>control</b> for the <b>camera.</b> The VRML camera object can also function as a status display. When someone moves the camera in the real world via the real remote control, the position of the VRML camera updates to reflect the actual status. We have two methods of viewing the video from the camera, one a C program executing on the CPU with the camera which quickly updates the images that can be viewed on the web via a server-push HTML page. The second method is to stream the video through video streaming software (commercial off-theshelf). The entire VRML view of the “smart ” room with cameras is placed {{within the context of a}} multi-user VRML world. This world can be visited by whomever desires to participate in meetings, held in the physical “smart ” room. Our intent is to create intuitive user interfaces to the cameras, and other facilities of the room. We allow people not physically able or willing to attend the physical meeting to participate “virtually”. People attending meetings remotely are represented as avatars. The room and objects in it are used for control and status. Depending on which, the dialog between the user and the displays change. When controlling real devices feedback must be rapid and sufficient to allow meaningful control. When using the virtual devices of the entire room only for status the user is more passive and can focus on other aspects of the meeting such as the actual content of the talk while still getting a feeling of the physical nature of the meeting space...|$|R
40|$|Transport of {{contaminants}} and bacteria in aqueous heterogeneous saturated porous {{systems have been}} studies experimentally using a novel fluorescent microscopic imaging (FMI) technique. The approach involves color visualization and quantification of bacterium and contaminant distributions within a transparent porous column. By introducing stained bacteria and an organic dye as a contaminant into the column and illuminating the porous regions with a planar sheet of laser beam, contaminant and bacterial transport processes through the porous medium can be observed and measured microscopically. A <b>computer</b> <b>controlled</b> CCD <b>camera</b> is used to record the fluorescent images {{as a function of}} time. These images are recorded by a frame accurate high resolution VCR and are then analyzed using a color image analysis code written in our laboratories. The color images are digitalized this way and simultaneously concentration and velocity distributions of both contaminant and bacterium are evaluated as a function of time and pore characteristics. The approach provides a unique dynamic probe to observe these transport processes microscopically. These results are extremely valuable in in-situ bioremediation problems since microscopic particle-contaminant-bacterium interactions are the key to understanding and optimization of these processes...|$|R
40|$|Tampere School of Architecture had {{to leave}} its old down-town {{building}} {{and move to the}} TU Tampere university campus in Hervanta, 10 km away. In this process, the 20 years old endoscopic system “The Urban Simulator” was one of the victims. Old mechanical parts and especially the original home-built microcomputer system were too old to compete with modern computer-aided methods. A new endoscopical system is now under construction, using all of the 20 -year experience, new technical components and <b>computers</b> for <b>camera</b> <b>control</b> and picture processing. Real-material modelling is used together with computer-aided planning and visualization methods taking the best from both sides...|$|R
40|$|Gear {{teeth and}} cam-follower {{couplings}} are typical non-conformal contacts working under transient elastohydrodynamic conditions undergoing variations of load, speed and geometry. Therefore, both numerical and experimental simulation of these contacts are difficult. Experimental results can validate numerical results and furnish useful indications for design optimization of gears and cam systems. In this work, a new test rig is presented {{that has been}} purposely designed for investigations on lubricated contacts working under transient conditions, as gear teeth and cam-follower contacts. Some theoretical/numerical simulations has been performed to investigate the dynamic behaviour of the apparatus to obtain important indications for its design. The contact is formed between a specimen of suitable shape moved by an electric motor and a flat counterface. The load is applied thanks to a lever system and calibrated springs. Film thickness and contact forces are the main quantities measured by the experimental apparatus. Film thickness and shape are estimated using optical interferometry; interference images are recorded using a microscope connected with a <b>computer</b> <b>controlled</b> high-speed <b>camera</b> when using glass counterfaces. A complex dynamometric system with load cells has been realised for measuring all components of the contact force. An air cooled insulated base is used for thermal insulation. A real-time controller and data acquisition system are used...|$|R
40|$|A motion <b>control</b> <b>camera</b> is {{one with}} a system for repeating camera {{operations}} identically by <b>computer</b> <b>control.</b> It is often used in cinema and other productions that require complex multiple compositions. NHK have applied state-of-the-art Japanese industrial robotic technologies to develop a motion <b>control</b> <b>camera</b> based on a new concept for use in HDTV production. It is compact thanks {{to the use of}} a small HD camera and has a convenient user interface. Compatibility of the camera data with the popular MAYA CG software makes shooting more efficient by enabling advance simulation of the robotic motion. The ease of reproducing identical camera operations reduces the time required for the multiple and CG compositions of VFX productions. 1...|$|R
40|$|A Marine Icing Monitoring System (MIMS) {{has been}} {{developed}} and deployed on a Marine Atlantic ferry and two offshore supply vessels. The MIMS is a visual based technology for monitoring marine icing accumulation on offshore rigs and vessels where icing poses operational and safety hazards. The system consists of a CPU connected to two high-resolution digital cameras that are positioned to view expansive areas, or smaller areas depending on requirements, where icing could occur when environmental conditions are within certain parameters. The system is stand-alone, requiring no maintenance and needing only a standard 110 VAC power source. All components of the system are weatherproofed so that installation can be anywhere on the deck or superstructure of a facility. The <b>computer</b> <b>controls</b> each <b>camera</b> so that pictures are taken at regular intervals (every 12 minutes) and stored on the computer 2 ̆ 019 s large-capacity hard drive. Data are retrieved from the hard drive {{at the end of}} the icing season each year. The system has a satellite phone so that it can be checked and controlled from IOT. Thumbnail images can be quickly downloaded to monitor current conditions at the site, and the <b>cameras</b> can be <b>controlled</b> to zoom in or out. Visual data from the system 2 ̆ 019 s deployments are presented along with some discussion of image analysis strategies, including real-time assessment of icing accumulation. Past and on-going development hurdles are also discussed. Peer reviewed: NoNRC publication: Ye...|$|R
50|$|Home {{automation}} {{for healthcare}} {{can range from}} very simple alerts to lavish <b>computer</b> <b>controlled</b> network interfaces. Some of the monitoring or safety devices that can be installed in a home include lighting and motion sensors, environmental <b>controls,</b> video <b>cameras,</b> automated timers, emergency assistance systems, and alerts.|$|R
5000|$|<b>Camera</b> <b>Control</b> Software: {{software}} that can remotely <b>control</b> a <b>camera</b> {{from a computer}} connected via USB (tethered shooting). Normally included as utilities with camera, these allow photographers to <b>control</b> the <b>camera</b> from a nearby computer. Cameras such as the Canon 40D include such software and a live view mode so that a user may use a <b>computer</b> to <b>control</b> numerous functions of the camera while seeing a virtual viewfinder onscreen.|$|R
40|$|Motion Control was {{produced}} with an Arts Council England / BBC Dance for Camera Award and examines the synergy between camera and performer. The film is notable for hyper-sound Foley score overlaid with text and electro-opera. First broadcast on BBC 2 3 rd March 2002, Motion Control concerns {{itself with a}} dynamic partnership between <b>computer</b> operated Motion <b>Control</b> <b>camera,</b> and performer. This screen dance film tests the boundaries of dialogue between camera and performer, camera and space, camera and sound and has raised the profile internationally of screen dance from the UK and in particular from Brighton. It has received numerous international awards, worldwide screenings and has helped to locate South East Dance Agency as screen dance specialist organization. Awards for Motion Control include; Czech Crystal Golden Prague Television Award 2002, Honourable Mention Paula Citron Award Toront o 2002, Special Jury Golden Award World FilmFest Houston 2003, Best Female Film Mediawaves Hungary 2003. Since its publication in 2002 Motion Control continues to tour to international film festival and is included with Forward Motion, Take 7 compilation packages. The iconic 'red dress' image featuring Professor Aggiss has been used worldwide as publicity for screen dance the art form...|$|R
40|$|This paper {{describes}} a distributed software control structure developed describe a Distributed Control System {{designed for the}} Rover, and for the CMU Rover, an advanced mobile robot equipped with a variety present of one control configuration, which is being developed for obstacle-sensors. Expert modules are used to control {{the operation of the}} sensors avoidance tasks. and actuators, interpret sensory and feedback data, build an internal model of the robot's environment, devise strategies to accomplish proposed 2 Hardware tasks Structure and execute these strategies. Each expert module is composed of a master The CMU Rover Project is a continuation of research begun with the process and a slave process, where the master process controls the Stanford Cart [2. 4], a minimal <b>computer</b> <b>controlled</b> mobile <b>camera</b> scheduling and working of the slave process. Communication among platform. I&quot;hc Rover [1] is intended to support a variety of Al research in expert modules occurs asynchronously over a blackboard structure. the areas of perception (sensory data processing and understanding), Information specific to the execution of a given task is provided through control, a real-world modelling, problem-solving, planning and related control plan. The system is distributed over a network of processors. issues. Real- For this reason, the system is being designed along the following time operating system kernels local to each processor and an interprocess guidelines: message communication mechanism ensure transparency of the underlying network structure. The various parts of the system are presented in this • mechanical, sensor and controller flexibility; paper and future work to be performed is mentioned • enough onboard processing capabilities to enable it to function autonomously, but with connections to a remote...|$|R
40|$|This {{paper is}} {{proposed}} about three axis motion <b>control</b> <b>camera</b> design method based on wires. Original motion <b>control</b> <b>camera</b> consists of track, boom, L-Head, Camera {{and so on}} and is enormous and expensive. But proposed motion <b>control</b> <b>camera</b> adjusts wire length using encoders and motors. And position control use position based straight line of straight-line move method for moving precise position. Proposed simple design is able to use various place and inexpensive than original motion <b>control</b> <b>camera.</b> But, camera was vibrated and rotated due to basic property of wire. So we proposed solutions that connected method of wire and using a tensional object for reducing rotation. For proposed algorithm verification, we realized three axis motion <b>control</b> <b>camera</b> based on wire and measured oscillation while moving same trace. We confirmed the results that standard deviation of oscillation was reduced 4. 93 degree than before design method...|$|R
50|$|A {{novel about}} a young street fighter who {{continuously}} films himself using remote <b>controlled</b> <b>cameras.</b>|$|R
40|$|Holographic {{data are}} {{acquired}} during hydrodynamic experiments at the Pegasus Pulsed Power Facility at the Los Alamos National Laboratory. These experiments produce a fine spray of fast-moving particles. Snapshots of the spray are captured using in-line Fraunhofer holographic techniques. Roughly one cubic centimeter is {{recorded by the}} hologram. Minimum detectable particle size in the data extends down to 2 microns. In a holography reconstruction system, a laser illuminates the hologram as it rests in a three axis actuator, recreating the snapshot of the experiment. A computer guides the actuators through an orderly sequence programmed by the user. At selected intervals, slices of this volume are captured and digitized with a CCD camera. Intermittent on-line processing of the image data and <b>computer</b> <b>control</b> of the <b>camera</b> functions optimizes statistics of the acquired image data for off-line processing. Tens of thousands of individual data frames (30 to 40 gigabytes of data) are required to recreate a digital representation of the snapshot. Throughput of the reduction system is 550 megabytes per hour (MB/hr). Objects and associated features from the data are subsequently extracted during off-line processing. Discrimination and correlation tests reject noise, eliminate multiple particles, and build an error model to estimate performance. Objects surviving these tests are classified as particles. The particle distributions are derived from the data base formed by these particles, their locations and features. Throughput of the off-line processing exceeds 500 MB/hr. This paper describes the reduction system, outlines the off-line processing procedure, summarizes the discrimination and correlation tests, and reports numerical results for a sample data set...|$|R
5000|$|Supported the {{placement}} of speed <b>control</b> <b>cameras</b> placed in work zones when there are workers present ...|$|R
40|$|This course {{deals with}} {{practical}} techniques for the specification, {{design and implementation}} of real-time <b>computer</b> <b>control</b> systems. Topics include: overview of <b>computer</b> <b>control</b> strategies; introduction to real-time systems; hardware and software requirements; implementation of digital control algorithms; design of real-time <b>computer</b> <b>control</b> systems; design analysis; considerations for fault detection and fault tolerance...|$|R
50|$|This was {{the first}} Leica {{microprocessor}} <b>controlled</b> <b>camera,</b> viewfinder display of shutter speed was digital with backlighting.|$|R
40|$|The Virtual Environment Workstation Project (VIEW) at NASA's Ames Research Center has {{developed}} a remotely <b>controlled</b> stereoscopic <b>camera</b> system {{that can be used}} for telepresence research and as a tool to develop and evaluate configurations for head-coupled visual systems associated with space station telerobots and remore manipulation robotic arms. The prototype camera system consists of two lightweight CCD video cameras mounted on a <b>computer</b> <b>controlled</b> platform that provides real-time pan, tilt, and roll <b>control</b> of the <b>camera</b> system in coordination with head position transmitted from the user. This paper provides an overall system description focused on the design and implementation of the camera and platform hardware configuration and the development of control software. Results of preliminary performance evaluations are reported with emphasis on engineering and mechanical design issues and discussion of related psychophysiological effects and objectives...|$|R
5000|$|Local Strategy {{aimed at}} {{conforming}} {{the installation of}} traffic <b>control</b> <b>cameras</b> to the provisions on the protection of personal data ...|$|R
5000|$|... 1975: Resurrected {{the use of}} VistaVision; {{first use}} of a motion <b>control</b> <b>camera</b> (Star Wars Episode IV: A New Hope) ...|$|R
50|$|Dykstra's {{development}} of this first digital motion <b>control</b> <b>camera</b> system earned him, Al Miller and Jerry Jeffress Academy Awards in 1978.|$|R
25|$|Long-range {{maritime}} reconnaissance variant, {{equipped with}} a FuG 200 Hohentwiel radar and a trio of remotely <b>controlled</b> <b>cameras</b> in the aft fuselage.|$|R
50|$|The SNAPit <b>Camera</b> app <b>controls</b> <b>cameras</b> on {{phones and}} tablets. It allows {{shooting}} panoramas, low-light scenes, photo editing, and creating animated GIFs.|$|R
50|$|Long-range {{maritime}} reconnaissance variant, {{equipped with}} a FuG 200 Hohentwiel radar and a trio of remotely <b>controlled</b> <b>cameras</b> in the aft fuselage.|$|R
5000|$|Nokia OZO {{comes with}} OZO Remote {{that gives you}} {{complete}} wireless <b>control</b> over your <b>camera</b> on-set or in the field. You can <b>control</b> <b>camera</b> settings, and monitor in real-time all eight of OZO’s lenses, or a single lens at a time.|$|R
40|$|A {{proposed}} {{method of}} automated, precise alignment of a ground-based astronomical telescope {{would eliminate the}} need for initial manual alignment. The method, based on automated identification of known stars and other celestial objects in the telescope field of view, would also {{eliminate the need for}} an initial estimate of the aiming direction. The method does not require any equipment other than a digital imaging device such as a charge-coupled-device digital imaging <b>camera</b> and <b>control</b> <b>computers</b> of the telescope and camera, all of which are standard components in professional astronomical telescope systems and in high-end amateur astronomical telescope systems. The method could be implemented in software running in the telescope or <b>camera</b> <b>control</b> <b>computer</b> or in an external computer communicating with the telescope pointing mount and <b>camera</b> <b>control</b> <b>computers...</b>|$|R
