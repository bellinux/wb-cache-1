0|737|Public
40|$|Abstract: This paper {{presents}} a hardware implementation of {{real time data}} <b>compression</b> and <b>decompression</b> <b>circuits</b> based on the LZW algorithm. LZW is a dictionary based data compression, which {{has the advantage of}} fast speed, high compression, and small resource occupation. In compression circuit, the design creatively utilizes two dictionaries alternately to improve efficiency and compressing rate. In <b>decompression</b> <b>circuit,</b> an integrated State machine control module is adopted to save hardware resource. Through hardware description and language programming, the circuits finally reach function simulation and timing simulation. The width of data sample is 12 bits, and the dictionary storage capacity is 1 K. The simulation results show the <b>compression</b> and <b>decompression</b> <b>circuits</b> have complete function. Compared to software method, hardware implementation can save more storage and compressing time. It has a high practical value in the future...|$|R
5000|$|Intel Communications Chipset 89xx Series (Cave Creek) for the Intel Xeon E5-2600 and E5-2400 Processor Series (Sandy Bridge-EP/EN) {{supports}} hardware <b>compression</b> and <b>decompression</b> using QuickAssist Technology. Depending on the chipset, <b>compression</b> and <b>decompression</b> {{rates of}} 5Gbit/s, 10Gbit/s, or 20Gbit/s are available.|$|R
40|$|Using only {{a single}} {{bidirectional}} optical delay line lattice, a technique for performing ultrahigh-data-rate <b>compression</b> and <b>decompression</b> of optical time-division multiplexed (OTDM) packets for photonic packet switching networks is presented. The simultaneous <b>compression</b> and <b>decompression</b> of 100 -Gb/s OTDM using the technique is demonstrated...|$|R
40|$|Abstract [...] The {{aim of the}} Paper is {{to provide}} the {{security}} during transmission of the data. Commonly used technologies are cryptography, <b>Compression</b> and <b>decompression.</b> This {{can be used for}} secure and fast sending for security purpose we are using the Cryptography technology and to increase the file transfer speed we are using the <b>compression</b> and <b>decompression</b> technologies. In this paper we described lot of processes for secure data transmission. First perform the <b>compression</b> and <b>decompression</b> techniques for decrease the file size. It will increase the transmission speed here maintains <b>compression</b> and <b>decompression</b> technique based on the GZIP Algorithm. Than perform the cryptography technique based on the TINY Encryption Algorithm. In sending process the TCP (Transaction control protocol) was involved...|$|R
5000|$|Raster Image: Allows the <b>compression</b> and <b>decompression</b> of data ...|$|R
40|$|A novel Smart Pixel Opto-VLSI {{architecture}} {{to implement}} a complete 2 -D wavelet transform of real-time captured images is presented. The Smart Pixel architecture enables the realisation of a highly parallel, compact, low power device capable of real-time capture, <b>compression,</b> <b>decompression</b> and display of images suitable for Mobile Multimedia Communication applications...|$|R
5000|$|Two-dimensional {{image data}} <b>compression</b> and <b>decompression</b> system. US Pat. T985005.|$|R
50|$|Does the {{filesystem}} support real-time transparent <b>compression</b> and <b>decompression</b> {{of individual}} files.|$|R
40|$|This paper {{describes}} a new reconfigurable processor architecture called REMARC (Reconfigurable Multimedia Array Coprocessor). REMARC is a reconfigurable coprocessor that is tightly coupled to a main RISC processor {{and consists of}} a global control unit and 64 programmable logic blocks called nano processors. REMARC is designed to accelerate multimedia applications, such as video <b>compression,</b> <b>decompression,</b> and image processing. These applications typically use 8 bit or 16 -bit data therefore, each nano processor has a 16 -bit datapath that is much wider than those of other reconfigurable coprocessors. We have developed a programming environment for REMARC and several realistic application programs, DES encryption, MPEG- 2 decoding, and MPEG- 2 encoding. REMARC achieves speedups ranging from a factor of 2. 3 to 21. 2 on these applications. 1 Introduction As the demand for multimedia applications, such as video <b>compression,</b> <b>decompression,</b> and image processing, increasing the performance of thes [...] ...|$|R
50|$|Does the {{filesystem}} support real-time transparent <b>compression</b> and <b>decompression</b> of {{an entire}} volume.|$|R
40|$|Abstract—An {{efficient}} and minimum hardware implementation for the Voice data <b>compression</b> and <b>decompression</b> {{will be presented}} in this paper. Voice data <b>compression</b> and <b>decompression</b> is about a process which reduces the data rate or file size of digital audio signals. This process reduces the dynamic range (withoutchanging the amount of digital data) of audio signals [1]. The Huffman coding {{is used to have}} lossless audio compression. Very High Speed Integrated Circuit Hardware Description Language (VHDL) is used for to code the Huffman encoder and decoder and Actel’s ProASIC kit is used for hard ware implementation of it. This system is minimal model of real time audio <b>compression</b> and <b>decompression</b> system...|$|R
5000|$|Hewlett-Packard holds [...] on <b>compression</b> and <b>decompression</b> of {{data points}} on {{elliptic}} curves. It expires in 2018.|$|R
50|$|In turn, if the <b>compression</b> and <b>decompression</b> {{times of}} an {{algorithm}} are vastly different, {{it is considered}} asymmetrical.|$|R
50|$|The primary {{developer}} was Ray Bunnage from <b>compression</b> / <b>decompression</b> libraries {{developed by}} Howard Gordon and Chris Eddy.|$|R
40|$|We present two {{computational}} {{approaches for}} the purpose of point <b>compression</b> and <b>decompression</b> on Edwards curves over the finite field Fp where p is an odd prime. The proposed algorithms allow <b>compression</b> and <b>decompression</b> for the x or y affine coordinates. We also present a x-coordinate recovery algorithm that can be used at any stage of a differential addition chain during the scalar multiplication of a point on the Edwards curve...|$|R
50|$|Zstandard was {{designed}} to give compression {{comparable to that of}} DEFLATE (.ZIP, gzip) with higher <b>compression</b> / <b>decompression</b> speeds.|$|R
40|$|This {{study was}} {{undertaken}} {{to understand and}} predict results of experimental cardiopulmonary resuscitation (CPR) techniques involving <b>compression</b> and <b>decompression</b> of either the chest or the abdomen. Simple mathematical models of the adult human circulation were used. Assumptions of the models are limited to normal human anatomy and physiology, the definition of compliance (volume change/pressure change), and Ohm’s law (flow = pressure / resistance). Interposed abdominal compression-CPR, active <b>compression</b> and <b>decompression</b> of the chest, and Lifestick CPR, which combines interposed abdominal compression and active <b>compression</b> and <b>decompression,</b> produce, respectively, 1. 9 -, 1. 2 -, and 2. 4 - fold greater blood flow than standard CPR and systemic perfusion pressures of 45, 30, and 58 mm Hg, respectively. These positive effects are explained by improved pump priming and are consequences of fundamental principles of cardiovascular physiology...|$|R
50|$|LZS <b>compression</b> and <b>decompression</b> uses an LZ77 type algorithm. It {{uses the}} last 2 KB of {{uncompressed}} data as a sliding-window dictionary.|$|R
5000|$|... use a {{software}} {{implementation of a}} video <b>compression</b> or <b>decompression</b> algorithm (commonly called a CODEC) and execute this software on the CPU ...|$|R
40|$|H. 264 /AVC is {{the most}} popular {{standard}} of video <b>compression</b> and <b>decompression</b> today, encapsulating all the advantages of MPEG as well as VCEG, both of them having their own independent codecs. The software used for <b>compression</b> and <b>decompression</b> is JM [Joint Video Team (JVT) of ISO/IEC MPEG & ITUT-T VCEG] version 18. 2. This paper is a study of H. 264 codec. The yuv file is first encoded to h. 264 format. Some other output files are also generated, which are useful for overall analysis of the input yuv file or sequence. These files help us to analyze parameters like PSNR, sequence parameter set, picture parameter set, information about different frames as regards slices and macroblocks. In this paper, all the three profiles (baseline, main and extended) are operated for the sequence (foreman_part_qcif. yuv) with one reference frame. A quantization parameter for the I, P and B slices is taken as 30. The graph of bitrate vs. PSNR (rate distortion curves) in all the three profiles show a striking similarity {{as can be seen in}} figures shown below. Keywords-H. 264, PSNR, <b>compression,</b> <b>decompression...</b>|$|R
50|$|The Blackfin {{instruction}} set contains media-processing extensions to help accelerate pixel-processing operations {{commonly used in}} video compression and image <b>compression</b> and <b>decompression</b> algorithms.|$|R
50|$|Symmetry and asymmetry, in {{the context}} of data compression, refer to the time {{relation}} between <b>compression</b> and <b>decompression</b> for a given compression algorithm.|$|R
50|$|A {{software}} package called etcpack for <b>compression</b> and <b>decompression</b> of ETC1/ETC2 textures {{used to be}} available for free download for usage with Khronos APIs.|$|R
40|$|Abstract—Data <b>compression</b> and <b>decompression</b> {{utilities}} can {{be critical}} in increasing communication throughput, reducing communication latencies, achieving energy-efficient communi-cation, and making {{effective use of}} available storage. This paper experimentally evaluates several such utilities for mul-tiple compression levels on systems that represent current mobile platforms. We characterize each utility {{in terms of its}} compression ratio, <b>compression</b> and <b>decompression</b> through-put, and energy efficiency. We consider different use cases that are typical for modern mobile environments. We find a wide variety of energy costs associated with data <b>compression</b> and <b>decompression</b> and provide practical guidelines for selecting the most energy efficient configurations for each use case. The best performing configurations provide 6 -fold and 4 -fold im-provements in energy efficiency for compressed uploads and downloads over WLAN, respectively, when compared to un-compressed data transfers. Keywords—mobile computing; measurement techniques; da-ta compression; energy-aware systems I...|$|R
40|$|Data {{compression}} {{plays an}} important role to deal with high volumes of DNA sequences in the field of Bioinformatics. Again data compression techniques directly affect the alignment of DNA sequences. So the time needed to decompress a compressed sequence has to be given equal priorities as with compression ratio. This article contains first introduction then a brief review of different biological sequence compression after that my proposed work then our two improved Biological sequence compression algorithms after that result followed by conclusion and discussion, future scope and finally references. These algorithms gain a very good compression factor with higher saving percentage and less time for <b>compression</b> and <b>decompression</b> than the previous Biological Sequence compression algorithms. Keywords: Hash map table, Tandem repeats, compression factor, compression time, saving percentage, <b>compression,</b> <b>decompression</b> process. Comment: 9 pages, 3 figures, 5 table...|$|R
50|$|LZ4 is a {{lossless}} {{data compression}} algorithm that {{is focused on}} <b>compression</b> and <b>decompression</b> speed. It belongs to the LZ77 family of byte-oriented compression schemes.|$|R
40|$|In this paper, {{complementary}} Huffman coding {{techniques are}} proposed for test data <b>compression</b> / <b>decompression</b> of complex SOC designs during manufacturing testing. In-stead of the compatible relationship between test data blocks, complementary features between test blocks are also exploited. Based on this property, more test data blocks can {{share the same}} codeword {{and the size of}} the modified Huffman tree can be reduced. This will not only reduce the area overhead of the decoding circuitry but also substantially in-crease the compression ratio. A graph model is also proposed for don’t-care assignment of test cube. This problem can be transformed to the clique partitioning problem whose complexity is basically NP-hard. Therefore, in order to exploit the compatible and com-plementary relationships between test blocks, a heuristic algorithm is proposed for filling test cubes. Thereafter, given the set of test vectors, two algorithms are proposed for complementary Huffman encoding. The compression ratio and hardware overhead of the decoding circuitry are analyzed. According to experimental results, the area overhead of the <b>decompression</b> <b>circuit</b> is lower than that of the full Huffman coding technique. However, it is a little bit higher than that of the selective Huffman coding technique. Moreover, the compression ratio is higher than that of the selective and optimal selective Huffman coding techniques...|$|R
5000|$|... use a {{software}} {{implementation of a}} video <b>compression</b> or <b>decompression</b> algorithm (commonly called a CODEC) and execute this software on the GPU (the 3D rendering engine) ...|$|R
40|$|Software-only {{digital video}} {{involves}} the <b>compression,</b> <b>decompression,</b> rendering, and display of digital video on general-purpose computers without specialized hardware. Today’s faster processors are making software-only video an attractive, low-cost alternative to hardware solutions {{that rely on}} specialized compression boards and graphics accelerators. This paper describes the building blocks behind popular ISO, ITU-T, and industry-standard compression schemes, along with some novel algorithms for fast video rendering and presentation. A platform-independent software architecture that organizes the functionality of compressors and renderers into a unifying software interfac...|$|R
50|$|<b>Compression</b> and <b>decompression</b> may be {{interrupted}} if the occupants experience {{problems caused by}} the pressure change, such as ear or sinus squeezes, or symptoms of decompression illness.|$|R
5000|$|Quality {{measurement}} for monitoring of a storage or transmission system that utilizes Video <b>compression</b> and <b>decompression</b> techniques, either a single pass or a concatenation of such techniques.|$|R
40|$|Encryption is {{the widely}} used {{technique}} to offer security for video communication and considerable numbers of video encryption algorithms have been proposed. The paper explores the literature for already proposed video encryption algorithms with {{the focus on}} the working principle of already proposed video encryption schemes. This study is aimed to give readers a quick overview about various video encryption algorithms proposed so far. General Terms Encryption, video security, video encryption metric, <b>compression,</b> <b>decompression.</b> Keywords Video encryption, video security requirements, video encryption evaluation metric, self adjustable encryption, codec independent. 1...|$|R
40|$|We present work-optimal PRAM {{algorithms}} for Burrows-Wheeler <b>compression</b> and <b>decompression</b> of strings over {{a constant}} alphabet. For {{a string of}} length n, {{the depth of the}} compression algorithm is O(log 2 n), and the depth of the the corresponding decompression algorithm is O(log n). These appear to be the first polylogarithmic-time work-optimal parallel algorithms for any standard lossless compression scheme. The algorithms for the individual stages of <b>compression</b> and <b>decompression</b> may also be of independent interest: 1. a novel O(log n) -time, O(n) -work PRAM algorithm for Huffman decoding; 2. original insights into the stages of the BW <b>compression</b> and <b>decompression</b> problems, bringing out parallelism that was not readily apparent, allowing them to be mapped to elementary parallel routines that have O(log n) -time, O(n) -work solutions, such as: (i) prefix-sums problems with an appropriately-defined associative binary operator for several stages, and (ii) list ranking for the final stage of decompression. NSF grant CCF- 081150...|$|R
40|$|Abstract:- Digitized {{voice is}} {{transmitted}} during different sorts of communications. For transmitting voice first the analog voice message is sampled and converted into digital signal. Then the signal is encoded and finally transmitted. In {{order to minimize}} the traffic over network, voice message is compressed during transmission. <b>Compression</b> and <b>decompression</b> process should not take much time. Again in cellular technology the <b>compression</b> and <b>decompression</b> need to be implemented in hardware level. If they require a complex hardware, {{that may not be}} effective. In this paper a very simple, linear, effective and easy to implement <b>compression</b> and <b>decompression</b> technique has been proposed. Our proposed technique keeps track of change in the digitized voice. Considering the digitized signal as a graph of amplitude vs. time, it keeps track of the change in direction of the wave. The proposed technique is not a loss-less compression scheme and it introduces a very little noise within acceptance range...|$|R
40|$|Data {{compression}} {{techniques have}} long been assisting in making effective use of disk, network and other similar resources. Most compression utilities require explicit user action for compressing and decompressing of file data. However, there are some systems in which <b>compression</b> and <b>decompression</b> of file data is done transparently by the operating system. A compressed file requires fewer sectors for storage on the disk. Hence, incorporating data compression techniques into a file system gives the advantage of larger effective disk space. At the same time, the additional time needed for <b>compression</b> and <b>decompression</b> of file data gets compensated {{to a large extent}} by the time gained because of fewer disk accesses. In this paper we describe the design and implementation of a file system for the Linux kernel, with the feature of on-the-fly data <b>compression</b> and <b>decompression</b> in a fashion that is transparent to the user. We also present some experimental results that show that th [...] ...|$|R
50|$|Α {{video codec}} is {{software}} or {{a device that}} provides encoding and decoding for digital video, and {{which may or may}} not include the use of video <b>compression</b> and/or <b>decompression.</b>|$|R
