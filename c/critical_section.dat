647|851|Public
25|$|Concurrency {{errors in}} {{critical}} sections, mutual exclusions {{and other features}} of concurrent processing. Time-of-check-to-time-of-use (TOCTOU) {{is a form of}} unprotected <b>critical</b> <b>section.</b>|$|E
500|$|If {{instruction}} 1B is executed between 1A and 3A, or if instruction 1A is executed between 1B and 3B, {{the program}} will produce incorrect data. This {{is known as a}} race condition. The programmer must use a lock to provide mutual exclusion. A lock is a programming language construct that allows one thread to take control of a variable and prevent other threads from reading or writing it, until that variable is unlocked. The thread holding the lock is free to execute its <b>critical</b> <b>section</b> (the section of a program that requires exclusive access to some variable), and to unlock the data when it is finished. Therefore, to guarantee correct program execution, the above program can be rewritten to use locks: ...|$|E
500|$|The second charge by the Native Americans {{was made}} against both {{the north and}} south ends of the camp, with the far {{southern}} end again being the hardest hit. Over half of Harrison's casualties were suffered among the companies on the southern end, including Captain Spencer and five other men in his company, and seven other men in the adjoining company. [...] With the regulars reinforcing that <b>critical</b> <b>section</b> of the line, and the surprise over, the men held their position as the attacks continued. On the northern end of the camp, Major Daveiss led the dragoons on a counter charge that punched through the Native Americans' line before being repulsed. Most of Daveiss' company retreated to Harrison's main line, but Daveiss was killed. Throughout the next hour, Harrison's troops fought off several more charges. When the warriors began to run low on ammunition and the sun rose, revealing the small size of Tenskwatawa's forces, the warriors began to slowly withdraw. A second charge by the dragoons forced the remaining Native Americans to flee.|$|E
50|$|<b>Critical</b> <b>sections</b> often allow nesting. Nesting allows {{multiple}} <b>critical</b> <b>sections</b> to {{be entered}} and exited at little cost.|$|R
50|$|Since <b>critical</b> <b>sections</b> may execute {{only on the}} {{processor}} on which they are entered, synchronization is only required within the executing processor. This allows <b>critical</b> <b>sections</b> to be entered and exited at almost zero cost. No inter-processor synchronization is required. Only instruction stream synchronization is needed. Most processors provide the required amount of synchronization by {{the simple act of}} interrupting the current execution state. This allows <b>critical</b> <b>sections</b> in most cases to be nothing more than a per processor count of <b>critical</b> <b>sections</b> entered.|$|R
40|$|Transactional memory (TM) is a {{scalable}} and concurrent way {{to build}} atomic sections. One aspect of TM that remains unclear is how side-effecting operations – that is, those which cannot be transparently undone by a TM system – should be handled. This uncertainty poses a significant barrier to the general applicability and acceptance of TM. Further, the absence of transactional workloads {{makes it difficult to}} study this aspect In this paper, we characterize the usage of I/O, and in particular system calls, within <b>critical</b> <b>sections</b> in two large applications, exploring both the actions performed and the characteristics of the <b>critical</b> <b>sections</b> in which they are performed. Shared memory programs employing <b>critical</b> <b>sections</b> are the closest approximation available to transactional workloads, so using this characterization, we attempt to reason about how the behavior we observed relates to the previous proposals for handling side-effecting operations within transactions. We find that the large majority of syscalls performed within <b>critical</b> <b>sections</b> can be handled with a range of existing techniques in a way transparent to the application developer. We also find that while side-effecting <b>critical</b> <b>sections</b> are rare, they tend to be quite long-lasting, and that many of these <b>critical</b> <b>sections</b> perform their first syscall (and thus become side-effecting) relatively early in their execution. Finally, we show that while these long-lived, side-effecting <b>critical</b> <b>sections</b> tend to execute concurrently with many <b>critical</b> <b>sections</b> on other threads, we observe little concurrency between side-effecting <b>critical</b> <b>sections.</b> ...|$|R
2500|$|C# {{provides}} the , which {{is yet another}} example of beneficial syntactic sugar. It works by marking a block of code as a <b>critical</b> <b>section</b> by mutual exclusion of access to a provided object. Like the [...] statement, it works by the compiler generating a [...] block in its place.|$|E
2500|$|The <b>critical</b> <b>section</b> of Cvetkovich's book {{provides}} analysis {{into some}} of the topics brought up in “The Depression Journals.” Playing a part in Cvetkovich's memoir, spirituality and religion are also discussed in her critical essay. For Cvetkovich, acedia – “a form of spiritual crisis” that is understood to be an historical precursor to depression – can help provide an alternative to the medical model. Cvetkovich's critical essay also explores the relationship between depression and histories of racism and colonialism, tying “history and depression into the emotional crises that undergird race studies.” Furthermore, looking at the artistic work of Sheila Pepe and Allyson Mitchell, she views “crafting as a model for creative ways of living in a depressive culture.” ...|$|E
5000|$|Once {{the first}} {{producer}} exits its <b>critical</b> <b>section,</b> [...] is incremented, allowing one consumer to enter its <b>critical</b> <b>section.</b>|$|E
40|$|If two {{parallel}} threads access {{the same location}} {{and at least one}} of them performs a write, a race exists. The detection of races [...] -a major problem in parallel debugging [...] -is complicated by the presence of atomic <b>critical</b> <b>sections.</b> In programs without <b>critical</b> <b>sections,</b> the existence of a race is usually a bug leading to nondeterministic behavior. In programs with <b>critical</b> <b>sections,</b> however, accesses in parallel <b>critical</b> <b>sections</b> are not considered bugs, as the programmer, in specifying the <b>critical</b> <b>sections,</b> presumably intends them to run in parallel. Thus, a race detector should find "data races" [...] -races between accesses not contained in atomic <b>critical</b> <b>sections.</b> We present algorithms for detecting data races in programs written in the Cilk multithreaded language. These algorithms do not verify programs, but rather find data races in all schedulings of the computation generated when a program executes serially on a given input. We present two algorithms for programs in which atomicit [...] ...|$|R
40|$|Abstract. The current denition of the Java Bytecode Verier, {{as well as}} the {{proposals}} to formalize it, do not include any check about consis-tency of <b>critical</b> <b>sections</b> (those between monitorenter and monitorexit instructions). So code is run, even if <b>critical</b> <b>sections</b> are corrupted. In this paper we isolate a sublanguage of the Java Virtual Machine with thread creation and mutual exclusion. For this subset we dene a se-mantics and a formal verier that enforces basic properties of threads and <b>critical</b> <b>sections.</b> The verier integrates well with previous formal-izations of the Java Bytecode Verier. Our analysis of <b>critical</b> <b>sections</b> reveals the presence of bugs in the current compilers from Sun and IBM. ...|$|R
50|$|<b>Critical</b> <b>sections</b> {{should not}} be used as a {{long-lasting}} locking primitive. <b>Critical</b> <b>sections</b> should be kept short enough {{so that it can be}} entered, executed, and exited without any interrupts occurring from the hardware and the scheduler.|$|R
5000|$|Several {{producers}} {{enter the}} producer <b>critical</b> <b>section.</b> No more than N producers may enter their <b>critical</b> <b>section</b> due to [...] constraining their entry.|$|E
50|$|Initially, {{the lock}} {{is free and}} all three {{processors}} attempt to acquire the lock simultaneously (Time 1). Due to P1 having the fastest access time to the lock, it acquires it first and enters the <b>critical</b> <b>section.</b> P2 and P3 now spin while P1 is in the <b>critical</b> <b>section</b> (Time 2). Upon exiting the <b>critical</b> <b>section</b> (Time 3), P1 executes an unlock, releasing the lock. Since P2 has faster access to the lock than P3, it acquires the lock next and enters the <b>critical</b> <b>section</b> (Time 4). While P2 is in the <b>critical</b> <b>section,</b> P1 once again attempts to acquire the lock but can’t (Time 5), forcing it to spin wait along with P3. Once P2 finishes the <b>critical</b> <b>section</b> and issues an unlock, both P1 and P3 simultaneously attempt to acquire it once again (Time 6). But P1, with its faster access time wins again, thus entering the <b>critical</b> <b>section</b> (Time 7). This pattern of P3 being unable to obtain the lock will continue indefinitely until either P1 or P2 stops attempting to acquire it.|$|E
50|$|On uniprocessor systems, the {{simplest}} solution to achieve mutual exclusion is to disable interrupts during a process's <b>critical</b> <b>section.</b> This will prevent any interrupt service routines from running (effectively preventing a process from being preempted). Although this solution is effective, {{it leads to}} many problems. If a <b>critical</b> <b>section</b> is long, then the system clock will drift every time a <b>critical</b> <b>section</b> is executed because the timer interrupt is no longer serviced, so tracking time is impossible during the <b>critical</b> <b>section.</b> Also, if a process halts during its <b>critical</b> <b>section,</b> control will never be returned to another process, effectively halting the entire system. A more elegant method for achieving mutual exclusion is the busy-wait.|$|E
50|$|Performance {{enhancements}} include executing pending interrupts at {{the exit}} of all <b>critical</b> <b>sections</b> and allowing the scheduler to run {{at the exit}} of all <b>critical</b> <b>sections.</b> Furthermore, pending interrupts may be transferred to other processors for execution.|$|R
40|$|We {{introduce}} a new parallel programming paradigm, namely synchronous parallel <b>critical</b> <b>sections.</b> Such parallel <b>critical</b> <b>sections</b> must {{be seen in the}} context of switching between synchronous and asynchronous modes of computation. Thread farming allows to generate bunches of threads to solve independent subproblems asynchronously and in parallel. Opposed to that, synchronous parallel <b>critical</b> <b>sections</b> allow to organize bunches of asynchronous parallel threads to execute certain tasks jointly and synchronously. We show how the PRAM language Fork 95 can be extended by a construct join supporting parallel <b>critical</b> <b>sections.</b> We explain its semantics and implementation, and discuss possible applications. 1. Introduction In a parallel environment, <b>critical</b> <b>sections</b> (for a survey, see e. g. [4]) are segments of code accessing data which are visible to more than one parallel thread. Their implementation {{is one of the key}} problems, e. g., of global resource management or consistency in parallel da [...] ...|$|R
5000|$|... synchronize_rcu (...) : It blocks {{until all}} {{pre-existing}} RCU read-side <b>critical</b> <b>sections</b> on all CPUs have completed. Note that [...] {{will not necessarily}} wait for any subsequent RCU read-side <b>critical</b> <b>sections</b> to complete. For example, consider the following sequence of events: ...|$|R
5000|$|Progress {{is defined}} as the following: if no process is {{executing}} in its <b>critical</b> <b>section</b> and some processes wish to enter their critical sections, then only those processes that are not executing in their remainder sections can participate in making the decision as to which process will enter its <b>critical</b> <b>section</b> next. This selection cannot be postponed indefinitely. [...] A process cannot immediately re-enter the <b>critical</b> <b>section</b> if the other process has set its flag to say that it would like to enter its <b>critical</b> <b>section.</b>|$|E
5000|$|The {{algorithm}} uses two variables, [...] and [...] A [...] {{value of}} [...] {{indicates that the}} process [...] wants to enter the <b>critical</b> <b>section.</b> Entrance to the <b>critical</b> <b>section</b> is granted for process P0 if P1 {{does not want to}} enter its <b>critical</b> <b>section</b> or if P1 has given priority to P0 by setting [...] to [...]|$|E
5000|$|... starvation, {{which occurs}} when a process is waiting to enter the <b>critical</b> <b>section,</b> but other {{processes}} monopolize the <b>critical</b> <b>section,</b> and the first process is forced to wait indefinitely; ...|$|E
50|$|All {{possible}} {{levels of}} these three factors yielded 8 sampling categories. The study intended to analyze 9 <b>critical</b> <b>sections</b> from each sampling category, but only 69 <b>critical</b> <b>sections</b> could be selected because only 6 articles (histories) were simultaneously featured, controversial, and policy laden.|$|R
50|$|Taking as {{parameters}} {{the average}} time interval spent by a processor in kernel level <b>critical</b> <b>sections</b> (L, for time in locked state), and {{the average time}} interval spent by a processor in tasks outside <b>critical</b> <b>sections</b> (E), the ratio L/E is crucial in evaluating software lockout.|$|R
40|$|Optimistic {{synchronization}} allows concurrent {{execution of}} <b>critical</b> <b>sections</b> while performing dynamic conflict detection and recovery. Optimistic synchronization will increase performance only if critical regions are data independent—concurrent <b>critical</b> <b>sections</b> access disjoint data {{most of the}} time. Optimistic synchronization primitives, such as transactional memory, will improve the performance of complex systems like an operating system kernel only if the kernel’s critical regions have reasonably high rates of data independence. This paper introduces a novel method and a tool called syncchar for exploring the potential benefit of optimistic synchronization by measuring data independence of potentially concurrent <b>critical</b> <b>sections.</b> Experimental data indicate that the Linux kernel has enough data independent <b>critical</b> <b>sections</b> to benefit from optimistic concurrency on smaller multiprocessors. Achieving further scalability will require data structure reorganization to increase data independence. ...|$|R
50|$|If {{a process}} wishes {{to enter the}} <b>critical</b> <b>section,</b> it must first execute the trying section and wait until it acquires access to the <b>critical</b> <b>section.</b> After the process has {{executed}} its <b>critical</b> <b>section</b> and is finished with the shared resources, it needs to execute the exit section to release them for other processes' use. The process then returns to its non-critical section.|$|E
5000|$|Similarly, if an {{interrupt}} {{occurs in}} a <b>critical</b> <b>section,</b> the interrupt information is recorded for future processing, and execution is returned to the process or thread in the <b>critical</b> <b>section.</b> Once the <b>critical</b> <b>section</b> is exited, {{and in some cases}} the scheduled quantum completed, the pending interrupt will be executed. The concept of scheduling quantum applies to [...] "round-robin" [...] and similar scheduling policies.|$|E
50|$|The {{algorithm}} is modeled {{on a waiting}} room with an entry and exit doorway. Initially the entry door is open and the exit door is closed. All processes which request entry into the <b>critical</b> <b>section</b> {{at roughly the same}} time enter the waiting room; the last of them closes the entry door and opens the exit door. The processes then enter the <b>critical</b> <b>section</b> one by one (or in larger groups if the <b>critical</b> <b>section</b> permits this). The last process to leave the <b>critical</b> <b>section</b> closes the exit door and reopens the entry door, so the next batch of processes may enter.|$|E
5000|$|... #Caption: Fig 2: Locks and <b>critical</b> <b>sections</b> in {{multiple}} threads ...|$|R
40|$|One of {{the major}} {{productivity}} issues in parallel programming arises {{from the use of}} lock/unlock operations or atomic/critical sections to enforce mutual exclusion. Not only are these constructs complicated to understand and debug, but they are also often an impediment to achieving scalable parallelism. In this paper, we propose to give the programmer the convenience of <b>critical</b> <b>sections</b> combined with the scalability of fine-grained locks, by solving two technical problems related to optimized assignment of locks to <b>critical</b> <b>sections.</b> The first problem, Minimum Lock Assignment (MLA), addresses the problem of finding the minimum number of locks needed to enforce mutual exclusion among interfering <b>critical</b> <b>sections</b> without any loss of concurrency. The second problem, K-Lock Allocation (K-LA) addresses the problem of allocating a fixed number (K) of locks to <b>critical</b> <b>sections</b> so as to minimize the serialization overhead. ...|$|R
50|$|Kernel-level <b>critical</b> <b>sections</b> are {{the base}} of the {{software}} lockout issue.|$|R
50|$|As {{shown in}} Fig 2, {{in the case}} of mutual {{exclusion}} (Mutex), one thread blocks a <b>critical</b> <b>section</b> by using locking techniques when it needs to access the shared resource and other threads have to wait to get their turn to enter into the section. This prevents conflicts when two or more threads share the same memory space and want to access a common resource.The simplest method to prevent any change of processor control inside the <b>critical</b> <b>section</b> is implementing a semaphore. In uni processor systems, this can be done by disabling interrupts on entry into the <b>critical</b> <b>section,</b> avoiding system calls that can cause a context switch while inside the section, and restoring interrupts to their previous state on exit. Any thread of execution entering any <b>critical</b> <b>section</b> anywhere in the system will, with this implementation, prevent any other thread, including an interrupt, from being granted processing time on the CPU—and therefore from entering any other <b>critical</b> <b>section</b> or, indeed, any code whatsoever—until the original thread leaves its <b>critical</b> <b>section.</b>|$|E
5000|$|It must {{be free of}} deadlocks: if {{processes}} are trying to enter the <b>critical</b> <b>section,</b> one of them must eventually {{be able to do}} so successfully, provided no process stays in the <b>critical</b> <b>section</b> permanently.|$|E
50|$|This is a {{modification}} to Ricart-Agrawala algorithm {{in which a}} REQUEST and REPLY message are used for attaining the <b>critical</b> <b>section.</b> but in this algorithm they introduced a method in which a seniority vise and also by handing over the <b>critical</b> <b>section</b> to other node by sending a single PRIVILEGE message to other node. So, the node which has the privilege it can use the <b>critical</b> <b>section</b> {{and if it does}} not have one it cannot. If a process wants to enter its <b>critical</b> <b>section</b> and it does not have the token, it broadcasts a request message to all other processes in the system. The process that has the token, if it is not currently in a <b>critical</b> <b>section,</b> will then send the token to the requesting process. The algorithm makes use of increasing Request Numbers to allow messages to arrive out-of-order.|$|E
40|$|Serialization of threads due to <b>critical</b> <b>sections</b> is a {{fundamental}} bottleneck to achieving high performance in multithreaded programs. Dynamically, such serialization may be unnecessary because these <b>critical</b> <b>sections</b> could have safely executed concurrently without locks. Current processors cannot fully exploit such parallelism {{because they do not}} have mechanisms to dynamically detect such false inter-thread dependences...|$|R
5000|$|... {{initialisation}} {{of structures}} in the DLL itself (i.e. <b>critical</b> <b>sections,</b> module lists); ...|$|R
40|$|The {{contributions}} {{of this paper}} are twofold. First, a protocol for distributed mutual exclusion is introduced using a token-based decentralized approach, which allows either multiple concurrent readers or a single writer to enter their <b>critical</b> <b>sections.</b> This protocol utilizes a dynamic structure incorporating path compression to keep the messages overhead low resulting in an average complexity of O(log n) messages per request. Second, this protocol is evaluated in comparison with another protocol that uses a static structure instead of dynamic path compression. The measurements show that although concurrent readers may require at most one additional message per entry, the concurrent execution of <b>critical</b> <b>sections</b> results in faster responses of up to 30 % for short <b>critical</b> <b>sections.</b> For longer <b>critical</b> <b>sections,</b> savings in the overall execution time increase with the fraction of readers to up to 50 %. In particular applications with large fractions of readers, e. g., datab [...] ...|$|R
