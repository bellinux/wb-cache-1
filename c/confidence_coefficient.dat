127|91|Public
5000|$|... and {{identifies}} {{any difference}} between two means that {{is greater than the}} expected standard error. The <b>confidence</b> <b>coefficient</b> for the set, when all sample sizes are equal, is exactly [...] for any [...] For unequal sample sizes, the <b>confidence</b> <b>coefficient</b> is greater than 1 − α. In other words, the Tukey method is conservative when there are unequal sample sizes.|$|E
5000|$|The Tukey {{confidence}} limits for all pairwise comparisons with <b>confidence</b> <b>coefficient</b> {{of at least}} 1 − α are ...|$|E
5000|$|However, as {{multiple}} mean {{responses are}} estimated, the confidence level declines rapidly. To fix the <b>confidence</b> <b>coefficient</b> at , the Working-Hotelling approach employs an F-statistic: ...|$|E
40|$|A {{confidence}} interval for the between group variance is proposed which is deduced from Wald 2 ̆ 019 s exact {{confidence interval}} for {{the ratio of}} the two variance components in the one-way random effects model and the exact confidence interval for the error variance resp. an unbiased estimator of the error variance. In a simulation study the <b>confidence</b> <b>coefficients</b> for these two intervals are compared with the <b>confidence</b> <b>coefficients</b> of two other commonly used confidence intervals. There, the confidence interval derived here yields <b>confidence</b> <b>coefficients</b> which are always greater than the prescribed level...|$|R
30|$|In CT, all the {{compressive}} {{features are}} supposed independent and equal {{contribution to the}} classifier (Zhang et al. 2012; Ng and Jordan 2002). Actually, different compressive features have different <b>confidence</b> <b>coefficients.</b> In our proposed algorithm, the <b>confidence</b> <b>coefficients</b> of features are computed and used to achieve different contribution to the classifier.|$|R
40|$|AbstractSimultaneous {{confidence}} intervals for multinomial proportions {{are useful in}} many areas of science. Since 1964, approximate simultaneous 1 -α {{confidence intervals}} have been proposed for multinomial proportions. Although at each point in the parameter space, these confidence sets have asymptotic 1 -α coverage probability, the exact <b>confidence</b> <b>coefficients</b> of these simultaneous confidence intervals for a fixed sample size are unknown before. In this paper, we propose a procedure for calculating exact <b>confidence</b> <b>coefficients</b> for simultaneous <b>confidence</b> intervals of multinomial proportions for any fixed sample size. With this methodology, exact <b>confidence</b> <b>coefficients</b> can be clearly derived, and {{the point at which the}} infimum of the coverage probability occurs can be clearly identified...|$|R
5000|$|... we {{will also}} write: , which follows the Student's {{t-statistic}} distribution with n degrees of freedom. The lower confidence limits with joint <b>confidence</b> <b>coefficient</b> [...] for the [...] treatment effects [...] will be given by: ...|$|E
50|$|However, when , {{intervals}} {{from the}} first procedure are guaranteed to contain the true value : Therefore, the nominal 50% <b>confidence</b> <b>coefficient</b> is unrelated to the uncertainty we should have that a specific interval contains the true value. The second procedure does not have this property.|$|E
5000|$|The joint {{statement}} {{consisting of the}} above three conclusions has a <b>confidence</b> <b>coefficient</b> of 95%, i.e., in the long run, 95% of such {{joint statement}}s will actually be correct. Upper limits for the three differences could be obtained in an analogous manner.For two-sided limits, the allowance is A=(2.94)(3.56)=11 and the experimenter can conclude that: ...|$|E
40|$|There {{are only}} {{relatively}} few <b>confidence</b> <b>coefficients</b> and levels available for distribution-free confidence intervals and {{test for the}} median based on the sign statistic. This problem can be overcome by interpolating confidence intervals or by studentizing the median by an estimate of its standard error. Such methods are discussed and then compared via simulation. median standard error estimate studentization interpolation...|$|R
40|$|Positive estimators of the between-group (between-study) {{variance}} are proposed. Explicit variance formulae for the estimators {{are given}} and approximate confidence intervals for the between-group variance are constructed, as our proposal {{to a long}} outstanding problem. By Monte Carlo simulation, the bias and standard deviation of the proposed estimators are compared with the truncated versions of the maxi- mum likelihood (ML) estimator, restricted maximum likelihood (REML) estimator and a (lately) standard estimator in meta-analysis. Attained <b>confidence</b> <b>coefficients</b> of the constructed confidence intervals are also presented...|$|R
30|$|In this paper, {{we propose}} a scale {{adaptive}} CT method which can adaptively adjusts {{the scale of}} tracking box with the size variation of the objects. Furthermore, the <b>confidence</b> <b>coefficients</b> of features are computed and used to achieve different contribution to the classifier. Finally, a variable learning parameter λ is adopted, which can be adjusted according to the object appearance variation rate. Extensive experiments on the CVPR 2013 tracking benchmark demonstrate the superior performance of the proposed method compared to state-of-the-art tracking algorithms in terms of efficiency, accuracy and robustness.|$|R
5000|$|Technically {{there are}} {{infinitely}} many contrasts. The simultaneous <b>confidence</b> <b>coefficient</b> is exactly 1 − α, whether the factor level sample sizes are equal or unequal. (Usually only {{a finite number}} of comparisons are of interest. In this case, Scheffé's method is typically quite conservative, and the family-wise error rate (experimental error rate) will generally be much smaller than α.) ...|$|E
5000|$|... and [...]The joint <b>confidence</b> <b>coefficient</b> {{for these}} three {{statement}} {{is greater than}} 95%.(Due to an approximation made in computing Tables 2a and 2b, the tabulated values of t are somewhat larger than necessary so that the actual p's attained are slightly greater than 95 and 99%.No such approximation was made in computing Tables 1a and 1b).|$|E
5000|$|... {{considering}} {{a sample of}} 80 cities amongst the 6345 listed by Ptolemy, those that are both identifiable and for which we can expect a better distance measurement since they were well known, there is a systematic overestimation of the longitude by a factor 1.428 with a high <b>confidence</b> (<b>coefficient</b> of determination r² = 0.9935). This error produces an evident deformations in Ptolemy's world map most apparent for exampled in the profile of Italy, which is markedly stretched horizontally; ...|$|E
40|$|A two-sample test is studied which {{rejects the}} null {{hypothesis}} of equal population medians when two Wilcoxon distribution free confidence intervals are disjoint. A confidence interval for the difference in population medians is constructed by subtracting the endpoints of two one-sample confidence intervals. Two different ways to select the one-sample intervals are presented. A solution that specifies equal <b>confidence</b> <b>coefficients</b> for the one-sample intervals is recommended. All solutions are shown {{to have the same}} asymptotic (Pitman) efficiency as the Mann-Whitney two-sample test. Mann-Whitney-Wilcoxon test nonparametric test sign test nonparametric confidence intervals notched box plots...|$|R
40|$|Building on {{the work}} of Holm (2012), we {{continue}} to theorize the construction of the confidence intervals for ranks (CIR) and to analyze their empirical performance. First, we replace the original asymptotic test with two exact tests. Then we apply all the tests to real medical data to identify the high/low achieving groups of hospitals. Finally, we conduct a simulation study to compare the relative performance of test statistics, and to compute the empirical <b>confidence</b> <b>coefficients</b> of CIRs. This paper also identifies areas of improvement for future study...|$|R
30|$|Recently, the {{compressive}} tracking (CT) method (Zhang et al. in Proceedings of European {{conference on}} computer vision, pp 864 – 877, 2012) has {{attracted much attention}} due to its high efficiency, but it cannot well deal with the scale changing objects due to its constant tracking box. To address this issue, {{in this paper we}} propose a scale adaptive CT approach, which adaptively adjusts the scale of tracking box with the size variation of the objects. Our method significantly improves CT in three aspects: Firstly, the scale of tracking box is adaptively adjusted according {{to the size of the}} objects. Secondly, in the CT method, all the compressive features are supposed independent and equal contribution to the classifier. Actually, different compressive features have different <b>confidence</b> <b>coefficients.</b> In our proposed method, the <b>confidence</b> <b>coefficients</b> of features are computed and used to achieve different contribution to the classifier. Finally, in the CT method, the learning parameter λ is constant, which will result in large tracking drift on the occasion of object occlusion or large scale appearance variation. In our proposed method, a variable learning parameter λ is adopted, which can be adjusted according to the object appearance variation rate. Extensive experiments on the CVPR 2013 tracking benchmark demonstrate the superior performance of the proposed method compared to state-of-the-art tracking algorithms.|$|R
50|$|Like the {{closely related}} Scheffé's method in the {{analysis}} of variance, which considers all possible contrasts, the Working-Hotelling procedure considers all possible values of the independent variables; that is, in a particular regression model, the probability that all the Working-Hotelling confidence intervals cover the true value of the mean response is the <b>confidence</b> <b>coefficient.</b> As such, when only a small subset of the possible values of the independent variable is considered, it is more conservative and yields wider intervals than competitors like the Bonferroni correction at the same level of confidence. It outperforms the Bonferroni correction as more values are considered.|$|E
40|$|Gini {{index is}} a widely used measure of {{economic}} inequality. This article develops a general theory for constructing a confidence interval for Gini index with a specified <b>confidence</b> <b>coefficient</b> and a specified width. Fixed sample size methods cannot simultaneously achieve both the specified <b>confidence</b> <b>coefficient</b> and specified width. We develop a purely sequential procedure for interval estimation of Gini index with a specified <b>confidence</b> <b>coefficient</b> and a fixed margin of error. Optimality properties of the proposed method, namely first order asymptotic efficiency and asymptotic consistency are proved. All theoretical results are derived without assuming any specific distribution of the data...|$|E
40|$|This {{paper is}} {{concerned}} with the problem whether the coverage probability of a well-known asymptotic confidence interval for the binomial parameter p, derived by the central limit theorem, is uniformly convergent. On the basis of several good properties this interval possesses, it seems reasonable for statisticians to conjecture that it has the uniform asymptotic <b>confidence</b> <b>coefficient</b> equal to the nominal coefficient. Surprisingly, as will be shown in this article, this conjecture is not correct for the commonly adopted nominal coefficients. Especially, this interval has a zero uniform asymptotic <b>confidence</b> <b>coefficient</b> when the nominal coefficient is less than 0. 5205. Coverage probability uniform asymptotic <b>confidence</b> <b>coefficient</b> nominal coefficient...|$|E
30|$|In this paper, we {{proposed}} a novel scale adaptive compressive tracking method, which improves the CT algorithm by a significantly large margin on the CVPR 2013 tracking benchmark. Our method significantly improves CT in three aspects: Firstly, the scale of tracking box is adaptively adjusted according {{to the size of}} the objects. Secondly, the <b>confidence</b> <b>coefficients</b> of features are computed and used to achieve different contribution to the classifier. Finally, a variable learning parameter λ is adopted in our method, which can be adjusted according to the object appearance variation rate. Numerous experiments have shown that the superior performance of the proposed method over other 7 state-of-the-art tracking algorithms in dealing with scale and pose change, illumination change, background clutters, occlusion and multiple challenges.|$|R
40|$|This paper {{proposes a}} {{technique}} for learning kernel functions {{that can be}} used in non-linear SVM classification. The technique uses genetic programming to evolve kernel functions as additive or multiplicative combinations of linear, polynomial and RBF kernels, while a procedure inspired from InfoBoost helps the evolved kernels concentrate on the most difficult objects to classify. The kernels obtained at each boosting round participate in the training of non-linear SVMs which are combined, along with their <b>confidence</b> <b>coefficients,</b> into a final classifier. We compared on several data sets the performance of the kernels obtained in this manner with the performance of classic RBF kernels and of kernels evolved using a pure GP method, and we concluded that the boosted GP kernels are generally better. 1...|$|R
40|$|Abstract: This paper {{presents}} a comparison study of data fusion techniques for precise position estimation useful for autonomous vehicle navigation. The aim is {{the integration of}} redundant encoder's data and absolute positioning data provided by GPS information. First we propose a technique, based on a constrained optimization method, which analyses data from four wheel encoders and two steering encoders. The proposed technique includes a set of <b>confidence</b> <b>coefficients</b> that reveal the undesirable effects of wheels slippage. This technique is compared with an Extended Kalman filter approach. Next, in both techniques a second fusion stage is included in order to integrate odometric and absolute positioning data, considering the vehicle kinematics model. This procedure generates two data fusion methods that are compared. Simulations are performed considering the Robucar model, a four-wheel actuated electrical vehicle...|$|R
3000|$|... ζ: A <b>confidence</b> <b>coefficient</b> {{that allows}} {{distribution}} centers to store {{a percentage of}} their previous period delivered to retailers.|$|E
40|$|Evidently, if p e, but if p> 0, a < e. The {{effect of}} {{positive}} values of p upon the <b>confidence</b> <b>coefficient</b> xc {{is given in}} Table II for c = 0. 99 and 0. 95. V. CONCLUSION This correspondence has demonstrated {{that the presence of}} intraclass correlation affects the confidence coefficients of the confidence sets obtained under the assumption of independence for the mean of a normal population with known dispersion matrix and the dispersion scalar a 2 in a 2 E with E known. Also, the following was demonstrated. a) If the sample is simply equicorrelated with positive coefficient of simple equicorrelation, then the <b>confidence</b> <b>coefficient</b> decreases with sample size. b) If the sample is simply equicorrelated with negative coefficient of simple equicorrelation (- 1 /(n- 1) < p < 0), then the <b>confidence</b> <b>coefficient</b> increases with sample size. Therefore, to be sure about the accuracy of the inference regarding the <b>confidence</b> <b>coefficient,</b> it is necessary to test the sample for independence or for the type of correlation-positive or negative. In case the sample is found to be positively correlated, it is advocated that test statistics appropriate for simply equicorrelated data be used. When the coefficient of positive simple equicorrelation is known or has been estimated, the test statistics appropriate for simply equicorrelated samples can be easily derived from the corresponding statistics for the independent samples...|$|E
40|$|In this paper, the distribution-free {{confidence}} region (rectangular) for the vector of the co-ordinate-wise quantiles {{of a general}} continuous bivariate distribution function is explicitly derived. This {{confidence region}}, in general, depends on the dependence function. A procedure is suggested which enables one to attach a <b>confidence</b> <b>coefficient</b> to the estimate of the confidence region even if the dependence function is unknown. Moreover, some approximated distribution-free lower bounds, which are independent of the dependence function, of the <b>confidence</b> <b>coefficient</b> of this region are derived. Finally, some {{results of this study}} are extended to the three-dimensional vector of the co-ordinate-wise quantiles. Distribution-free confidence region Bivariate order statistics Bivariate quantiles...|$|E
40|$|Abstract. Locating {{fire source}} {{position}} {{is a significant}} aspect of automatic fire detection since it {{plays an important role}} in extinguishing fire in large space building in a short time. On the basis of previous literatures of visible fire location detection through analyzing the features of fire images, binocular vision based fire source spatial location is proposed in accordance with the computer vision principle in this paper. The binocular vision location algorithm is employed to identify the fire source position. To reach this, three typical spatial positions of fire source are measured in terms of camera scanning firstly; Secondly, <b>confidence</b> <b>coefficients</b> are added into the measured data to obtain the accurate position of fire source. Finally, the amount of water in cannon is adjusted to the right direction to put out the fire...|$|R
30|$|For the Fish {{sequence}} {{shown in}} Fig.  5 c, the object undergoes several times of illumination change. The tracking result indicates that illumination getting stronger {{will have little}} effect on the tracking results of each algorithm (e.g., # 156). But all the algorithms except SACT drift once the illumination get weaker (e.g., # 160 and # 437). The proposed SACT can deal with illumination Change in terms of its adaptive local appearance model, that is to say, different compressive features have different <b>confidence</b> <b>coefficients</b> in our tracker. For the Car dark sequence shown in Fig.  5 d, the object undergoes large changes in environmental illumination with the car running along the street. CT, VTS, VTD and TLD drift gradually (320 – 388) as illumination changing while SCM, STRUCK, VTS and the proposed SACT achieve much better performance.|$|R
40|$|When {{the same}} {{parameters}} are estimated by data from several independent samples, it may happen that, for any pair of samples, {{even though the}} test for parameter discrepancy is statistically significant, the two individual confidence intervals overlap. To overcome this potential contradiction, {{a new type of}} one-sample confidence intervals is developed. Their evaluation will lead to the same statistical decisions reached by the two-sample test for parameter discrepancy. Moreover, the simultaneous decisions on parameter estimation, statistical inference, and directional prediction can be made with specified <b>confidence</b> <b>coefficients</b> and error rates by simply comparing a pair of comparable confidence intervals. In contrast with conventional confidence intervals, the comparable new confidence intervals have narrower widths, disjoint or overlap depending on whether the parameter discrepancy is statistically significant or not. The proposed procedure can be applied to both simple and multiple a-priori comparisons o...|$|R
40|$|Sampling distributions, {{constructed}} by Monte Carlo simulation {{are used in}} hardware development to establish a design reliability goal, to place a <b>confidence</b> <b>coefficient</b> on reliability estimates, and to determine whether sample stress/strength data demonstrate a specified reliability at a specified confidence level...|$|E
40|$|A novel {{adaptive}} weight online sequential extreme {{learning machine}} (AWOS-ELM) is proposed for predicting time series problems {{based on an}} online sequential extreme learning machine (OS-ELM) in this paper. In real-world online applications, the sequentially coming data chunk usually possesses varying confidence coefficients, and the data chunk with a low <b>confidence</b> <b>coefficient</b> tends to mislead the subsequent training process. The proposed AWOS-ELM can improve the training process by accessing the <b>confidence</b> <b>coefficient</b> adaptively and determining the training weight accordingly. Experiments on six time series prediction data sets have verified that the AWOS-ELM algorithm performs better in generalization performance, stability, and prediction ability than the OS-ELM algorithm. In addition, a real-world mechanical system identification problem is considered to test the feasibility and efficacy of the AWOS-ELM algorithm...|$|E
40|$|This paper {{addresses}} {{the issue of}} constructing a confidence interval of a small area mean in a random effect or mixed effects linear model. A crude confidence interval based on the empirical Bayes method has the drawback that its coverage probability is much less than a nominal <b>confidence</b> <b>coefficient.</b> For improving on this confidence interval, the paper provides the procedure of adjusting the critical value, and the resulting confidence interval has a coverage probability which {{is identical to the}} nominal <b>confidence</b> <b>coefficient</b> in second order asymptotics when the number of small areas is large. The proposed confidence interval is numerically investigated based on simulation experiments and applied to posted land price data. These numerical studies illustrate the practical usefulness of the proposal. ...|$|E
40|$|International audiencePut {{in place}} by the International Maritime Organization, the Automatic Identification System is a {{worldwide}} maritime electronic system that sends radio broadcasted messages at a high rate between the stations, either on board the vessels or on shores. However, some misuses of the system such as identity theft, localization spoofing or disappearances have been demonstrated. The high rate of transmission implies a considerable amount of data to process in order to point out those irregularities. This paper proposes a method based on data mining and clustering methods combined to an integrity assessment of AIS messages for anomaly detection, with a proposition of software architecture for a data processing done both on-The-fly and with archived data. The computation of <b>confidence</b> <b>coefficients</b> and the use of data mining techniques will lead to behaviour characterization with the purpose of enhance the maritime situational awarenes...|$|R
40|$|International audienceAdvances in new {{information}} and communication technology have considerably increased the quantity of available data and permitted the implementation of more and more complex processing techniques. As {{a result of these}} changes it appeared the need of evaluating the information fusion system quality and to propose to the user the information accompanied with <b>confidence</b> <b>coefficients.</b> The information fusion systems used nowadays are complex systems that are difficult to be evaluated. To overcome this problem we propose a new quality evaluation methodology based on the decomposition of the information fusion system in its elementary modules that allows the evaluation of the quality at two levels. The first one (global) that describes the entire information fusion system and the second one (local) for each elementary module. As data and information change over time, this decomposition allows to directly evaluate the global quality of the information fusion system using the local quality evaluation...|$|R
40|$|Methods for {{combining}} 2 2 contingency tables {{from multiple}} studies are reviewed and new methods are proposed. Methods for combining 2 2 contingency tables from multiple studies {{are presented in}} the context of three different statistical models: the constant coefficient model, the varying coefficient model, and the random coefficient model. The constant <b>coefficient</b> <b>confidence</b> intervals have unacceptable performance characteristics in typical applications where effect sizes and samples sizes are unequal across studies. The random <b>coefficient</b> <b>confidence</b> intervals, which assume that the studies are a random sample from a definable superpopulation of studies and that the superpopulation distribution is normally distributed, exhibit unacceptable performance characteristics. The varying <b>coefficient</b> <b>confidence</b> intervals have excellent performance characteristics under realistic conditions. New methods for comparing parameter values across study populations are presented. Keyterms: G-index of agreement, odds ratio, positive and negative predictive values, risk ratio, risk difference, sensitivity and specificity, varying coefficient model...|$|R
