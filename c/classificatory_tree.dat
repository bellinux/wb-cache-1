1|2|Public
5000|$|... #Caption: Lignum Febrium, a <b>classificatory</b> <b>tree</b> of fevers {{according}} to Torti (1756) ...|$|E
40|$|Recent claims, {{mainly from}} {{computer}} scientists, concerning a largely automated and model-free data-intensive science have been countered by critical reactions {{from a number}} of philosophers of science. The debate suffers from a lack of detail in two respects, regarding (i) the actual methods used in data-intensive science and (ii) the specific ways in which these methods presuppose theoretical assumptions. I examine two widely-used algorithms, <b>classificatory</b> <b>trees</b> and non-parametric regression, and argue that these are theory-laden in an external sense, regarding the framing of research questions, but not in an internal sense concerning the causal structure of the examined phenomenon. With respect to the novelty of data-intensive science, I draw an analogy to exploratory as opposed to theory-directed experimentation. 1...|$|R
40|$|In any learnability setting, {{hypotheses}} are conjectured {{from some}} hypothesis space. Studied herein are {{the effects on}} learnability of {{the presence or absence}} of certain control structures in the hypothesis space. First presented are control structure characterizations of some rather specific but illustrative learnability results. Then presented are the main theorems. Each of these characterizes the invariance of a learning class over hypothesis space V (and a little more about V) as: V has suitable instances of all denotational control structures. 1 Introduction In any learnability setting, hypotheses are conjectured from some hypothesis space, for example, in [OSW 86] from general purpose programming systems, in [ZL 95, Wie 78] from subrecursive systems, and in [Qui 92] from very simple classes of <b>classificatory</b> decision <b>trees.</b> 3 Much is known theoretically about the restrictions on learning power resulting from restricted hypothesis spaces [ZL 95]. In the present paper we begin to [...] ...|$|R

