19|17|Public
50|$|As {{bishop and}} {{chairperson}} of the EKD’s Council Huber initiated and supported numerous reform programs. In {{the context of the}} challenges mainline Protestantism face, especially in the eastern parts of Germany, Huber advocated for a missionary reorientation of the church. For him church reform is closely connected to the rediscovery of the church’s evangelical essence and requires openness to those who have distanced themselves from the Christian faith. These impulses characterize the large-scale reform process, subsumed under the theme “Church of freedom”, which Huber headed. The document Kirche der Freiheit describes how the church can set its profile in society, whilst respecting societal plurality. This document formulates four goals for the reform of the Protestant church in Germany, namely (a) spiritual profiling instead of indistinct activity, (b) prioritising instead of aiming for <b>completeness,</b> (<b>c)</b> structural mobility and (d) shifting the focus of the activities of the church to the outside instead of self-contentment. In his own regional church Huber also oversaw a reform process, “Salt of the earth”. Huber’s tenure as {{chairperson of the}} EKD’s Council also saw the incorporation of the Vereinigte Evangelisch-Lutherische Kirche Deutschlands and the Union Evangelischer Kirchen with the EKD, the streamlining of regional churches from 23 to 21, and the initiation of further reform processes.|$|E
40|$|A {{well-known}} {{generating function}} of the classical Laguerre polynomials was recently rederived probabilistically by Lee. In this paper, some other (presumably new) generating functions for the Laguerre polynomials are derived by means of probabilistic considerations. A direct (analytical) proof {{of each of these}} generating functions is also presented for the sake of <b>completeness.</b> (<b>C)</b> 1999 Elsevier Science Ltd. All rights reserved...|$|E
30|$|The {{evaluation}} {{has been}} based on the quality indicators developed within the ILCD handbook (EC-JRC 2010 a, b, 2011): Technological representativeness (TeR), Geographical representativeness (GR), Time-related representativeness (TiR), <b>Completeness</b> (<b>C),</b> Precision/Uncertainty (P) and Methodological appropriateness and consistency (M). Each of those has been evaluated according to the degree of accomplishment of the criterion, from 1 (very good, so meets the criterion to a very high degree) to 5 (very poor, so does not at all meet the criterion).|$|E
40|$|Abstract: We {{established}} a minimum conditional system C 2 Lm with primary-conditional and secondary-conditional in LIU zhuanghu and LI Xiaowu[2004] and [2006], LI Xiaowu and LIU zhuanghu[2004], and thus prove the soundness and the <b>completeness</b> of <b>C</b> 2 Lm with respective to an ordered neighborhood semantics or a relation semantics. In this paper, we will introduce the update semantics presented by Veltman[1996], and thus prove the soundness and the completeness of an extension system LPSC of C 2 Lm with respective to the update semantics...|$|R
40|$|We {{show that}} the satisfiability problem for LTL (with past operators) over {{arithmetic}} constraints (Constraint LTL) can be answered by solving a finite amount of instances of bounded satisfiability problems when atomic formulae belong to certain suitable fragments of Presburger arithmetic. A formula is boundedly satisfiable when it admits an ultimately periodic model of the form δπ^ω, where δ and π are finite sequences of symbolic valuations. Therefore, for every formula there exists a <b>completeness</b> bound <b>c,</b> such that, {{if there is no}} ultimately periodic model with |δπ|[*]≤[*]c, then the formula is unsatisfiable...|$|R
40|$|The Propositional CalculusPropositional Connectives. Truth TablesTautologies Adequate Sets of Connectives An Axiom System for the Propositional Calculus Independence. Many-Valued LogicsOther AxiomatizationsFirst-Order Logic and Model TheoryQuantifiersFirst-Order Languages and Their Interpretations. Satisfiability and Truth. ModelsFirst-Order TheoriesProperties of First-Order Theories Additional Metatheorems and Derived Rules Rule <b>C</b> <b>Completeness</b> Theorems First-Order Theories with EqualityDefinitions of New Function Letters and Individual Constants Prenex Normal Forms Isomorphism of Interpretat...|$|R
40|$|A wide {{spectrum}} of certificate revocation mechanisms is currently in use. A number {{of them have been}} proposed by standardisation bodies, while some others have originated from academic or private institutions. What is still missing is a systematic and robust framework for the sound evaluation of these mechanisms. We present a mechanism-neutral framework for the evaluation of certificate status information (CSI) mechanisms. These mechanisms collect, process and distribute CSI. A detailed demonstration of its exploitation is also provided. The demonstration is mainly based on the evaluation of Certificate Revocation Lists, {{as well as of the}} Online Certificate Status Protocol. Other well-known CSI mechanisms are also mentioned for <b>completeness.</b> (<b>C)</b> 2003 Elsevier B. V. All rights reserved. status: publishe...|$|E
30|$|The {{evaluation}} {{has been}} based on the quality indicators developed within the ILCD handbook (EC-JRC, 2010 a, 2010 b, 2011): Technological representativeness (TeR), Geographical representativeness (GR), Time-related representativeness (TiR), <b>Completeness</b> (<b>C),</b> Precision/Uncertainty (P) and Methodological appropriateness and consistency (M). Each of those has been evaluated according to the degree of accomplishment of the criterion (from 1 to 5), and an overall DQR of the datasets has been calculated by summing up the achieved quality rating for each of the quality criteria indicator, divided {{by the total number of}} considered indicators, as described in Garraín et al. Background qualitative analysis of the European Reference Life Cycle Database (ELCD) energy datasets – Part I: Fuel datasets. Springer Plus - Submitted in 2014.|$|E
40|$|Body {{shortening}} {{was observed}} in the pearlfish Carapus homei during metamorphosis. The tenuis larva at first possessed a suite of osseous vertebral bodies of similar length. The reduction in both the number and size of vertebrae followed increasing decalcification, degeneration of organic tissue and shortening. This involved a complete degradation and disappearance of the caudal tip vertebrae, {{and there was a}} reduction in the size of most of the remaining vertebrae. The further development of the vertebrae began with ossification of the neural and haemal arches before that of the vertebral body. This second part of the development followed a gradient: a gradual decreases towards the caudal tip {{in the size of the}} vertebrae and their <b>completeness.</b> (<b>C)</b> 2004 The Fisheries Society of the British Isles. Peer reviewe...|$|E
40|$|Combining goal-oriented and {{use case}} {{modeling}} {{has been proven}} {{to be an effective}} method in requirements elicitation and elaboration. To ensure the quality of such modeled artifacts, a detailed model analysis needs to be performed. However, current requirements engineering approaches generally lack reliable support for automated analysis of consistency, correctness and <b>completeness</b> (3 <b>Cs</b> problems) between and within goal models and use case models. In this paper, we present a goal–use case integration framework with tool support to automatically identify such 3 Cs problems. Our new framework relies on the use of ontologies of domain knowledge and semantics and our goal–use case integration meta-model. Moreover, functional grammar is employed to enable the semiautomated transformation of natural language specifications into Manchester OWL Syntax for automated reasoning. The evaluation of our tool support shows that for representative example requirements, our approach achieves over 85 % soundness and completeness rates and detects more problems than the benchmark applications...|$|R
40|$|A unified {{catalog of}} {{earthquakes}} in Iran and adjacent regions (the area bounded in 22 º - 42 º N and 42 º - 66 º E) covering {{the period of}} 4 th century B. C. through 2012 with M w ≥ 4 is provided. The catalog includes all events for which magnitude have been determined by international agencies and most reliable individual sources. Since the recurrence time of maximum credible earthquake cannot be directly estimated from the m b, empirical formulae are established to convert m b to M s, m b to M w and M s to M w for each major seismotectonic province separately. The unified catalog is declustered using conjugated distance-time windows. In order to estimate completeness thresholds, magnitude-time (M-T) diagram and Stepp’s method are applied on the declustered catalog for each seismotectonic province. The magnitude of <b>completeness</b> (M <b>c)</b> decreases with development of local and regional seismic stations. The results of present study are particularly important in seismic hazard analysis in Iran...|$|R
40|$|We {{prove that}} the family of {{embezzlement}} states defined by van Dam and Hayden [vanDamHayden 2002] is universal for both quantum and classical entangled two-prover non-local games with an arbitrary number of rounds. More precisely, we show that for each ϵ> 0 and each strategy for a k-round two-prover non-local game which uses a bipartite shared state on 2 m qubits and makes the provers win with probability ω, there exists a strategy for the same game which uses an embezzlement state on 2 m + 2 m/ϵ qubits and makes the provers win with probability ω-√(2 ϵ). Since {{the value of a}} game can be defined as the limit of the value of a maximal 2 m-qubit strategy as m goes to infinity, our result implies that the classes QMIP*_c,s[2,k] and MIP*_c,s[2,k] remain invariant if we allow the provers to share only embezzlement states, for any <b>completeness</b> value <b>c</b> in [0, 1] and any soundness value s < c. Finally we notice that the circuits applied by each prover may be put into a very simple universal form...|$|R
40|$|Abstract This study {{aimed to}} {{describe}} the ideology of: (a) journalists, (b) politicians, and (c) observers/community in framing President Joko Widodo in Suara Merdeka, Kompas, and Republika daily newspapers. The data sources were the three newspapers published in 2016. The data were collected through: (a) literature study, (b) discourse analysis, and (c) in-depth interviews with linguists and mass media experts. The data analysis was carried out using Pan and Kosicki’s framing analysis model. The results show that there are ideological differences among the three media in framing President Joko Widodo. The differences are manifested in the aspects of: (a) syntax, (b) script, (c) theme, and (d) rhetoric. The three media also differ in presenting: (a) news schemes, (b) news <b>completeness,</b> (<b>c)</b> details, (d) lexicon, and (e) completeness of 5 W + 1 H elements in reporting President Joko Widodo. Keywords: frame, ideology, media, framing analysi...|$|E
40|$|For {{collating}} point-source flux measurements {{derived from}} multiple infrared passbands of Spitzer-Space-Telescope data – e. g., channels 1 - 4 of the Infrared Array Camera (IRAC) and channels 1 - 3 of the Multiband Imaging Pho- tometer for Spitzer (MIPS) – {{it is best}} to use the ‘bandmerge’ software developed at the Spitzer Science Center rather than the relatively simple method of general source association (GSA). The former method uses both source positions and positional uncertainties to form a chi-squared statistic that can be thresholded for optimal matching, while the latter method finds nearest neighbors across bands that fall within a user-specified radius of the primary source. Our assertion is supported by our study of <b>completeness</b> (<b>C)</b> vs. reliability (R) for the two methods, which involved MIPS- 24 /IRAC- 1 matches in the SWIRE Chandra Deep Field South. Both methods can achieve C = 98...|$|E
40|$|This report W ~ S prcpared as {{an account}} of work sponsolcd by an a g c x, of the l lnited StatesGoverrlmciii Nsithei ihellni! & StatesGoverninent nor any ayeiicy tht?:oof, nor any of their ernployses, makes anv {{warranty}} express cr rrrlplied or assumes any legal liability or responsibility for tile accuracy, <b>completeness,</b> <b>c)</b> : usefillness of any information, apparatus, product, or PiOCZSS disclosed, or represents that 1;s use wculd not infilnge PI ivaiely owned rights Rzfxoncs herein to any specific coiniil?rcial product, grocess or service by trhde name tradci-raik, mamfacturer or otherwise, does not necess;lrily constitute or impiy Its endorsement. recornitwildation. or fzvoring b y the United Sta!ssGo any aqency thereof I h e vie and opinions of authors expressed 5 erslR do not riac. ?ssarily state or reflect thcse of the United States(Govc-,m;?eiii C I any agency liie: enf ORNL/TM- 10691 Engineering Physics and Mathematics Divisio...|$|E
40|$|Abstract We {{introduce}} a new method to determine the magnitude of <b>completeness</b> M <b>c</b> and its uncertainty. Our method models the entire magnitude range (EMR method) consisting of the self-similar complete part of the frequency-magnitude distribution and the incomplete portion, thus providing a comprehensive seismicity model. We compare the EMR method with three existing techniques, finding that EMR shows a superior performance when applied to synthetic test cases or real data from regional and global earthquake catalogues. This method, however, {{is also the most}} computationally intensive. Accurate knowledge of M c is essential for many seismicity-based studies, and particularly for mapping out seismicity parameters such as the b-value of the Gutenberg-Richter relationship. By explicitly computing the uncertainties in M c using a bootstrap approach, we show that uncertainties in b-values are larger than traditionally assumed, especially when considering small sample sizes. As examples, we investigated temporal variations of M c for the 1992 Landers aftershock sequence and found that it was underestimated on average by 0. 2 with former techniques. Mapping M c on a global scale, M c reveals considerable spatial variations for the Harvard Centroid Moment Tensor (CMT) (5. 3 � M c � 6. 0) and the International Seismological Centre (ISC) catalogue (4. 3 � M c � 5. 0) ...|$|R
40|$|Khot (STOC 2002) conjectures that, {{for every}} {{constant}} γ> 1, {{there is a}} PCP characterization of NP in terms of unique games (a restricted type of 2 -provers 1 -round proof systems) with completeness 1 − γ and soundness γ. Khot also conjectures that a characterization in terms of 2 -to- 1 games exists with perfect completeness and soundness γ. A 2 -to- 1 game is also a restricted type of 2 -prover 1 -round, {{but it is more}} general than a unique game. The generalization of the unique games conjecture to the case of sub-constant γ has been used to prove non-approximability results for the sparsest cut problem, and a generalization of the 2 -to- 1 conjecture to the case of sub-constant γ might be used in future work on coloring problems. In this paper we present polynomial time algorithms that show that, for constants c 1, c 2, c 3 and for every ε> 0 : 1. The unique game conjecture with <b>completeness</b> 1 − <b>c</b> 1 ε 3 /(log n) 3 and soundness 1 − ε is false. 2. The unique game conjecture with <b>completeness</b> 1 − <b>c</b> 2 ε 2 /(log n), soundness 1 − ε, and the requirement that all constraints are linear is false. 3. The 2 -to- 1 conjecture with perfect completeness and soundness 1 / 2 c 3 √ log n is false. (n is the size of the input.) In contrast, a 2 -prover 1 -round characterization of NP with perfect completeness and sound-(log n). 99 ness 1 / 2 is known, and it is also known that it’s hard to approximate the value of a (log n). 99 linear unique game within a factor 2. ...|$|R
40|$|This paper {{describes}} {{methods for}} automatically analyzing formal, state-based requirements specifications for completeness and consistency. The approach uses a low-level functional formalism, simplifying the analysis process. State space explosion problems are eliminated {{by applying the}} analysis {{at a high level}} of abstraction; i. e, instead of generating a reachability graph for analysis, the analysis is performed directly on the model. The method scales up to large systems by decomposing the specification into smaller, analyzable parts and then using functional composition rules to ensure that verified properties hold for the entire specification. The analysis algorithms and tools have been validated on TCAS II, a complex, airborne, collisionavoidance system required on all commercial aircraft with more than 30 passengers that fly in U. S. airspace. 1 Introduction This paper describes methods and tools for automatically analyzing software requirements for <b>completeness</b> and consistency. <b>C</b> [...] ...|$|R
40|$|A {{property}} tester is a fast, randomized algorithm {{that reads}} {{only a few}} entries of the input, {{and based on the}} values of these entries, it distinguishes whether the input has a certain property or is “different ” from any input having this property. Furthermore, we say that a property tester has <b>completeness</b> <b>c</b> and soundness s if it accepts all inputs having the property with probability at least c and accepts “different ” inputs with probability at most s+ o(1). In this thesis we present two property testers for boolean functions on the boolean cube { 0, 1 }n. We summarize our contribution as follows. • We present a new dictatorship test that determines whether the function is a dictator (of the form f(x) = xi for some coordinate i), or a function that is an “anti-dictator. ” Our test is “adaptive, ” makes q queries, has completeness 1, and soundness O(q 3) · 2 −q. Previously, a dictatorship test that has sound...|$|E
40|$|Many {{researches}} {{to testing}} object-oriented programs (OOPs) {{have been proposed}} for the past decade. However, most of these researches have centered only around the class-level testing instead of the whole profiles of OOP development. This paper presents a framework to test OOPs from em formal specification to em test data generation. The formal specification of object-oriented program is specified in Z notation which is a mathematically rigorous specification language. A state transition diagram (STD) derived from Z specification provides a complementary representation of the dynamic behavior of a given OOP. In addition, the STD {{can be used to}} generate a test data which consists of anticipated operation sequences of OOPs. Moreover, a testing algorithm modeled by em finite state machine is proposed to run against test data. Two important testing criteria, consistency and <b>completeness</b> (<b>C</b> & C), are used to evaluate testing result. The final test result shows that the testing framework is executable and pragmatic. 1...|$|E
40|$|This paper {{presents}} a general space-efficient method for error reduction for unitary quantum computation. Consider a polynomial-time quantum computation with <b>completeness</b> <b>c</b> and soundness s, either {{with or without}} a witness (corresponding to QMA and BQP, respectively). To convert this computation into a new computation with error at most 2 ^{-p}, the most space-efficient method known requires extra workspace of O(p*log(1 /(c-s))) qubits. This space requirement is too large for scenarios like logarithmic-space quantum computations. This paper shows an errorreduction method for unitary quantum computations (i. e., computations without intermediate measurements) that requires extra workspace of just O(log(p/(c-s))) qubits. This in particular gives the first method of strong amplification for logarithmic-space unitary quantum computations with two-sided bounded error. This also leads to a number of consequences in complexity theory, such as the uselessness of quantum witnesses in bounded-error logarithmic-space unitary quantum computations, the PSPACE upper bound for QMA with exponentially-small completeness-soundness gap, and strong amplification for matchgate computations...|$|E
40|$|We will {{consider}} cardinality {{constraints of the}} form M,! K, 2 f=;; g, stating that exactly, at most, at least) K elements out of a set M have to be chosen". We will show how a set C of constraints can be represented {{by means of a}} positivedisjunctive deductive database P C, such that the models of P C correspond to the solutions of C. This allows for embedding cardinality constraints into applications dealing with incomplete knowledge. We will also present a sound calculus represented by a denite logic program P cc=, which allows for directly reasoning with sets of exactlycardinality constraints. Reasoning with P cc= is very ecient, and it can be used for performance reasons before P C is evaluated. For obtaining <b>completeness,</b> however, P <b>C</b> is necessary, since we show the theoretical result that a sound and complete calculus for exactly cardinality constraints does not exist. Keywords disjunctive logic programming, constraint logic programming, cardinality constraints, [...] ...|$|R
40|$|The Minimal Supersymmetric Standard Model {{contains}} {{in general}} sources of tau lepton flavour violation which induce the rare decays tau [...] > mu gamma and tau [...] > e gamma. We argue {{in this paper}} that the observation of both rare processes would imply a lower bound on the radiative muon decay of the form BR(mu [...] > e gamma) > C BR(tau [...] > mu gamma) BR(tau [...] > e gamma). We estimate {{the size of the}} constant C without specifying the origin of the tau flavour violation in the supersymmetric model and we discuss the implications of our bound for future searches of rare lepton decays. In particular, we show that, for a wide class of models, present B-factories could discover either tau [...] > mu gamma or tau [...] > e gamma, but not both. We also derive for <b>completeness</b> the constant <b>C</b> in the most general setup, pursuing an effective theory approach. Comment: 18 pages, 6 figure...|$|R
40|$|The main {{objective}} of the paper {{is to develop a}} new method to estimate the maximum magnitude (M (max)) considering the regional rupture character. The proposed method has been explained in detail and examined for both intraplate and active regions. Seismotectonic data has been collected for both the regions, and seismic study area (SSA) map was generated for radii of 150, 300, and 500 km. The regional rupture character was established by considering percentage fault rupture (PFR), which is the ratio of subsurface rupture length (RLD) to total fault length (TFL). PFR is used to arrive RLD and is further used for the estimation of maximum magnitude for each seismic source. Maximum magnitude for both the regions was estimated and compared with the existing methods for determining M (max) values. The proposed method gives similar M (max) value irrespective of SSA radius and seismicity. Further seismicity parameters such as magnitude of <b>completeness</b> (M (<b>c)),</b> ``a'' and ``aEuro parts per thousand b `` parameters and maximum observed magnitude (M (max) (obs)) were determined for each SSA and used to estimate M (max) by considering all the existing methods. It is observed from the study that existing deterministic and probabilistic M (max) estimation methods are sensitive to SSA radius, M (c), a and b parameters and M (max) (obs) values. However, M (max) determined from the proposed method is a function of rupture character instead of the seismicity parameters. It was also observed that intraplate region has less PFR when compared to active seismic region...|$|R
40|$|The {{result of}} the paper can be deduced from already known results in [ST 06] and [BHR 05]. 1 Main After {{publishing}} the paper in ECCC, {{it has come to}} my attention that the result proven in the paper, lower bounds for adaptive linearity tests, can be deduced from already known results. As stated in the paper, the lower bound for non-adaptive linearity tests was proven by Samorodnitsky and Trevisan in [ST 06], where they prove that the Complete Graph Test is optimal among all non-adaptive linearity tests. Ben-Sasson, Harsha and Raskhodnikov have proved in [BHR 05] that any adap-tive linearity test with query complexity q, <b>completeness</b> <b>c</b> and soundness s can be transformed into a non-adaptive linearity test with the same query complexity, perfect completeness and soundness s ′ = s+ 1 − c. The combination of the two results shows optimality of the Complete Graph Test among adaptive tests as well...|$|E
40|$|The goal of {{this work}} is a {{systematic}} presentation of some classes of mixed weak formulations, for general multi-dimensional dipolar gradient elasticity (fourth order) boundary value problems. The displacement field main variable is accompanied by the double stress tensor and the Cauchy stress tensor (case 1 or mu - tau - u formulation), the double stress tensor alone (case 2 or mu - u formulation), the double stress, the Cauchy stress, the displacement second gradient and the standard strain field (case 3 or mu - tau - kappa - epsilon - u formulation) and the displacement first gradient, along with the equilibrium stress (case 4 or u - theta - gamma formulation). In all formulations, the respective essential conditions are built {{in the structure of}} the solution spaces. For cases 1, 2 and 4, one-dimensional analogues are presented for the purpose of numerical comparison. Moreover, the standard Galerkin formulation is depicted. It is noted that the standard Galerkin weak form demands C- 1 -continuous conforming basis functions. On the other hand, up to first order derivatives appear in the bilinear forms of the current mixed formulations. Hence, standard C- 0 -continuous conforming basis functions may be employed in the finite element approximations. The main purpose of this work is to provide a reference base for future numerical applications of this type of mixed methods. In all cases, the associated quadratic energy functionals are formed for the purpose of <b>completeness.</b> (<b>C)</b> 2008 Elsevier Ltd. All rights reserved...|$|E
40|$|We use N_t, {{the number}} of exoplanets {{observed}} in time t, as a science metric to study direct-search missions like Terrestrial Planet Finder. In our model, N has 27 parameters, divided into three categories: 2 astronomical, 7 instrumental, and 18 science-operational. For various " 27 -vectors" of those parameters chosen to explore parameter space, we compute design reference missions to estimate N_t. Our treatment includes the recovery of <b>completeness</b> <b>c</b> after a search observation, for revisits, solar and antisolar avoidance, observational overhead, and follow-on spectroscopy. Our baseline 27 -vector has aperture D = 16 m, inner working angle IWA = 0. 039 ", mission time t = 0 - 5 years, occurrence probability for earthlike exoplanets η = 0. 2, and typical values for the remaining 23 parameters. For the baseline case, a typical five-year design reference mission has an input catalog of ∼ 4700 stars with nonzero completeness, ∼ 1300 unique stars observed in ∼ 2600 observations, of which ∼ 1300 are revisits, and it produces N_ 1 ∼ 50 exoplanets after one year and N_ 5 ∼ 130 after five years. We explore offsets from the baseline for ten parameters. We find that N depends strongly on IWA and only weakly on D. It also depends only weakly on zodiacal light for Z 0. 2, and scattered starlight for ζ < 10 ^- 10. We find that observational overheads, completeness recovery and revisits, solar and antisolar avoidance, and follow-on spectroscopy are all important factors in estimating N. Comment: 37 pages, 11 figures, 11 tables; ApJ accepted versio...|$|E
40|$|International audienceSince 1990, we have {{maintained}} {{a database of}} historical information on landslides and floods in Italy, known as the National Research Council's AVI (Damaged Urban Areas) archive. The database was originally designed {{to respond to a}} request of the Minister of Civil Protection, and was aimed at helping the regional assessment of landslide and flood risk in Italy. The database was compiled in 1991 - 1992 to cover the period 1917 to 1990, and then updated to cover systematically the period 1917 to 2000, and non-systematically the periods 1900 to 1916 and 2001 to 2002. The database currently contains information on more than 32000 landslide events occurred at more than 21000 sites, and on more than 29000 flood events occurred at more than 14000 sites. Independently from the AVI archive, we have obtained other databases containing information on damage caused by mass movements and inundations, daily discharge measurements and solid-transport measurements at selected gauging stations, bibliographical and reference information on landslides and inundations, and a catalogue of National legislation on hydrological and geological hazards and risk in Italy. The databases are part of an information system known as SICI (an Italian acronym for Sistema Informativo sulle Catastrofi Idrogeologiche, Information System on Hydrological and Geomorphological Catastrophes), which is currently the largest single repository of historical information on landslides and floods in Italy. After an outline of the history and evolution of the AVI Project archive, we present and discuss: (a) the structure of the SICI information system, including the hardware and software solutions adopted to maintain, manage, update, use and disseminate the information stored in the various databases, (b) the type and amount of information stored in each database, including an estimate of their <b>completeness,</b> and (<b>c)</b> examples of recent applications of the information system, including a web-based GIS system to show the location of sites historically affected by landslides and floods, and an estimate of geo-hydrological (i. e. landslide and flood) risk in Italy based on the available historical information...|$|R
40|$|We {{present the}} “C 4 Cluster Catalog”, a new sample of 748 {{clusters}} of galaxies {{identified in the}} spectroscopic sample of the Second Data Release (DR 2) of the Sloan Digital Sky Survey (SDSS). The C 4 cluster–finding algorithm identifies clusters as overdensities in a seven-dimensional position and color space, thus minimizing projection effects that have plagued previous optical cluster selection. The present C 4 catalog covers ∼ 2600 square degrees of sky and ranges in redshift from z = 0. 02 to z = 0. 17. The mean cluster membership is 36 galaxies (with redshifts) brighter than r = 17. 7, but the catalog includes a range of systems, from groups containing 10 members to massive clusters with over 200 cluster members with redshifts. The catalog provides {{a large number of}} measured cluster properties including sky location, mean redshift, galaxy membership, summed r–band optical luminosity (Lr), velocity dispersion, as well as quantitative measures of substructure and the surrounding large-scale environment. We use new, multi-color mock SDSS galaxy catalogs, empirically constructed from the ΛCDM Hubble Volume (HV) Sky Survey output, to investigate the sensitivity of the C 4 catalog to the various algorithm parameters (detection threshold, choice of passbands and search aperture), as well as to quantify the purity and <b>completeness</b> of the <b>C</b> 4 cluster catalog. These mock catalog...|$|R
40|$|High {{levels of}} {{skeletal}} articulation and completeness in fossil crocodyliforms are commonly attributed to rapid burial, with decreasing articulation and completeness thought {{to result from}} prolonged decay of soft tissue {{and the loss of}} skeletal connectivity during 2 ̆ 7 bloat and float. These interpretations are based largely on patterns of decay in modern mammalian and avian dinosaur carcasses. To address this issue, we assessed the decay of buried and unburied juvenile Crocodylus porosus carcasses in a controlled freshwater setting. The carcasses progressed through typical vertebrate decay stages (fresh, bloated, active decay, and advanced decay), reaching the final skeletal stage on average 55 days after death. Unburied carcasses commenced floating five days postmortem during the bloated stage, and one buried carcass only commenced floating 12 days post-mortem. While floating, skeletal elements remained articulated within the still coherent dermis, except for thoracic ribs, ischia and pubic bones. The majority of disarticulation occurred at the sediment-water interface after the carcasses sank during the advanced decay stage, - 36 days post-mortem. Based on these results we conclude that fossil crocodyliform specimens displaying high levels of articulation are not the result of prolonged subaerial and subaqueous decay in a low-energy, aqueous environment. Using extant juvenile C porosus as a proxy for fossil crocodyliforms, rapid burial in an aquatic setting would have to occur prior to the carcass floating, and would also have to continually negate the positive buoyancy associated with bloating. Rapid burial {{does not have to be}} the only avenue to preservation of articulation, as other mechanisms such as physical barriers and internal physiological chemistry could prevent carcasses from floating and subsequently disarticulating upon sinking. The inference that a large proportion of skeletal elements could drift from floating carcasses in a low energy setting with minimal scavenging, thereby causing a loss of <b>completeness,</b> seems unlikely. (<b>c)</b> 2014 Elsevier B. V. All rights reserved...|$|R
40|$|The Mopex {{software}} is {{used at the}} Spitzer Science Center (SSC) to produce co-added and mosaicked images from sets of individually processed Spitzer images. Until now, quantitative studies {{of the performance of}} Mopex's outlier-detection methods had never been performed. This particular study focuses only on Mopex's multiframe outlier-detection algorithm, and future studies are still needed to characterize its so-called box and dual methods. The performance of the multiframe method varies with two adjustable parameters, ΓTOP and ΓRM. For a given ΓTOP value, we computed the <b>completeness</b> (<b>C)</b> and reliability (R) of the outlier detection for 101 discrete values of ΓRM uniformly distributed in the full range of possible settings for this parameter, which lie continuously between 0 and 1, inclusive. We characterized the C and R performance as a function of ΓRM in this manner for ΓTOP values of 2, 2. 5, 3, 5, and 10 for image data in all four IRAC channels (infrared passbands). Not surprisingly, the performance for IRAC channel 3 is relatively poor because the image data for this channel are markedly noisier. The best performance was obtained for a ΓTOP value of 3, and this applies to all four channels. Generally, setting ΓRM low will maximize completeness at the expense of reliability, and vice versa for setting ΓRM high. For example, for IRAC channel 1 and ΓTOP = 3, setting ΓRM = 0. 3 gives C = 83 % and R = 41 %, and setting ΓRM = 0. 8 gives C = 52 % and R = 86 %...|$|E
40|$|Abridged] We {{present a}} {{homogeneous}} and complete catalogue of optical groups {{identified in the}} purely flux limited (17. 5 <=I<= 24. 0) VIMOS-VLT Deep Survey (VVDS). We use mock catalogues extracted from the MILLENNIUM simulation, to correct for potential systematics that might affect the overall distribution {{as well as the}} individual properties of the identified systems. Simulated samples allow us to forecast the number and properties of groups that can be potentially found in a survey with VVDS-like selection functions. We use them to correct for the expected incompleteness and also to asses how well galaxy redshifts trace the line-of-sight velocity dispersion of the underlying mass overdensity. In particular, we train on these mock catalogues the adopted group-finding technique (the Voronoi-Delaunay Method, VDM). The goal is to fine-tune its free parameters, recover in a robust and unbiased way the redshift and velocity dispersion distributions of groups and maximize the level of <b>completeness</b> (<b>C)</b> and purity (P) of the group catalogue. We identify 318 VVDS groups with at least 2 members within 0. 2 <=z<= 1. 0, among which 144 (/ 30) with at least 3 (/ 5) members. The sample has globally C= 60 % and P= 50 %. Nearly 45 % of the groups with at least 3 members are still recovered if we run the algorithm with a parameter set which maximizes P (75 %). We exploit the group sample to study the redshift evolution of the fraction f_b of blue galaxies (U-B<= 1) within 0. 2 <=z<= 1. We find that f_b is significantly lower in groups than in the whole ensemble of galaxies irrespectively of their environment. These quantities increase with redshift, with f_b in groups showing a marginally significant steeper increase. We also confirm that, at any explored redshift, f_b decreases for increasing group richness, and we extend towards fainter luminosities the magnitude range over which this result holds. Comment: Submitted to A&A, revised version after referee comments, Table 5 adde...|$|E
40|$|We {{investigate}} {{the use of}} optical photometric variability to select and identify blazars in large-scale time-domain surveys, in part {{to aid in the}} identification of blazar counterparts to the ~ 30 % of gamma-ray sources in the Fermi 2 FGL catalog still lacking reliable associations. Using data from the optical LINEAR asteroid survey, we characterize the optical variability of blazars by fitting a damped random walk model to individual light curves with two main model parameters, the characteristic timescales of variability (tau), and driving amplitudes on short timescales (sigma). Imposing cuts on minimum tau and sigma allows for blazar selection with high efficiency E and <b>completeness</b> <b>C.</b> To test the efficacy of this approach, we apply this method to optically variable LINEAR objects that fall within the several-arcminute error ellipses of gamma-ray sources in the Fermi 2 FGL catalog. Despite the extreme stellar contamination at the shallow depth of the LINEAR survey, we are able to recover previously-associated optical counterparts to Fermi AGN with E > 88 % and C = 88 % in Fermi 95 % confidence error ellipses having semimajor axis r < 8 '. We find that the suggested radio counterpart to Fermi source 2 FGL J 1649. 6 + 5238 has optical variability consistent with other gamma-ray blazars, and {{is likely to be the}} gamma-ray source. Our results suggest that the variability of the non-thermal jet emission in blazars is stochastic in nature, with unique variability properties due to the effects of relativistic beaming. After correcting for beaming, we estimate that the characteristic timescale of blazar variability is ~ 3 years in the rest-frame of the jet, in contrast with the ~ 320 day disk flux timescale observed in quasars. The variability-based selection method presented will be useful for blazar identification in time-domain optical surveys, and is also a probe of jet physics. Comment: version resubmitted to ApJ with incorporated referee's comments. 12 pages, 10 figure...|$|E
40|$|We {{present the}} C 4 Cluster Catalog, a new sample of 748 {{clusters}} of galaxies {{identified in the}} spectroscopic sample of the Second Data Release (DR 2) of the Sloan Digital Sky Survey (SDSS). The C 4 cluster-finding algorithm identifies clusters as overdensities in a seven-dimensional position and color space, thus minimizing projection effects that have plagued previous optical cluster selection. The present C 4 catalog covers ~ 2600 deg 2 of sky and ranges in redshift from z= 0. 02 to 0. 17. The mean cluster membership is 36 galaxies (with measured redshifts) brighter than r= 17. 7, but the catalog includes a range of systems, from groups containing 10 members to massive clusters with over 200 cluster members with measured redshifts. The catalog provides {{a large number of}} measured cluster properties including sky location, mean redshift, galaxy membership, summed r-band optical luminosity (Lr), and velocity dispersion, as well as quantitative measures of substructure and the surrounding large-scale environment. We use new, multicolor mock SDSS galaxy catalogs, empirically constructed from the ??CDM Hubble Volume (HV) Sky Survey output, to investigate the sensitivity of the C 4 catalog to the various algorithm parameters (detection threshold, choice of passbands, and search aperture), as well as to quantify the purity and <b>completeness</b> of the <b>C</b> 4 cluster catalog. These mock catalogs indicate that the C 4 catalog is ~= 90 % complete and 95 % pure above M 200 = 1 ?? 1014 h- 1 Msolar and within 0. 03 <=z<= 0. 12. Using the SDSS DR 2 data, we show that the C 4 algorithm finds 98 % of X-ray-identified clusters and 90 % of Abell clusters within 0. 03 <=z<= 0. 12. Using the mock galaxy catalogs and the full HV dark matter simulations, we show that the Lr of a cluster is a more robust estimator of the halo mass (M 200) than the galaxy line-of-sight velocity dispersion or the richness of the cluster. However, if we exclude clusters embedded in complex large-scale environments, we find that the velocity dispersion of the remaining clusters is as good an estimator of M 200 as Lr. The final C 4 catalog will contain ~= 2500 clusters using the full SDSS data set and will represent {{one of the largest and}} most homogeneous samples of local clusters. ...|$|R
40|$|A {{property}} tester is a fast, randomized algorithm {{that reads}} {{only a few}} entries of the input, {{and based on the}} values of these entries, it distinguishes whether the input has a certain property or is "different" from any input having this property. Furthermore, we say that a property tester has <b>completeness</b> <b>c</b> and soundness s if it accepts all inputs having the property with probability at least c and accepts "different" inputs with probability at most s + o(1). In this thesis we present two property testers for boolean functions on the boolean cube f 0; 1 gn. We summarize our contribution as follows. We present a new dictatorship test that determines whether the function is a dictator (of the form f(x) = xi for some coordinate i), or a function that is an "anti-dictator. " Our test is "adaptive," makes q queries, has completeness 1, and soundness O(q 3) 2 ??q. Previously, a dictatorship test that has soundness (q + 1). 2 -q is achieved by Samorodnitsky and Trevisan, but their test has completeness strictly less than 1. Furthermore, the previously best known dictatorship test from the PCP literature with completeness 1 has soundness [...] .. Our contribution lies in achieving perfect completeness and low sound- ness simultaneously. We consider properties of functions that are invariant under linear transformations of the boolean cube. Previous works, such as linearity testing and low-degree testing, have focused on linear properties. (cont.) The one exception is a test due to Green for "triangle freeness": a function f satisfies this property if f(x); f(y); f(x + y) do not all equal 1, for any pair x; y 2 f 0; 1 gn. We extend this test to a more systematic study and consider non-linear properties that are described by a single forbidden pattern. Specifically, let M denote an r by k matrix over f 0; 1 g. We say that a function f is M-free if there are no x = (x 1, [...] .,xk), where x 1, [...] .,xk 2 f 0; 1 gn such that f(x 1), [...] .,f(xk) = 1 and M x = 0. If M can be represented by an underlying graph, we can analyze a test that determines whether a function is M-free or " from one. Our test makes k queries, has completeness 1, and soundness bounded away from 1. The technique from our work leads to alternate proofs that some previously studied linear properties are testable, albeit with worse parameters. Our results, though quite different in terms of context, are connected by similar techniques. Our analysis of the algorithms relies on the machinery of the Gowers uniformity norm, a recent and powerful tool in additive combinatorics. by Victor Yen-Wen Chen. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, Dept. of Mathematics, 2009. This electronic version was submitted by the student author. The certified thesis is available in the Institute Archives and Special Collections. Includes bibliographical references (p. 65 - 68) ...|$|E

