12|179|Public
50|$|The Joly Screen {{process had}} some problems. First and foremost, {{although}} the colored lines were reasonably fine (about 75 sets of three colored {{lines to the}} inch) they were still disturbingly visible at normal viewing distances and nearly intolerable when enlarged by projection. This problem was exacerbated {{by the fact that}} each screen was individually ruled on a machine which used three pens to apply the transparent colored inks, resulting in irregularities, high reject rates and high cost. The glass used for photographic plates at the time was not perfectly flat, and lack of uniform good contact between the screen and the image gave rise to areas of degraded color. Poor contact also caused false colors to appear if the sandwich was viewed at an angle. Although much simpler than the Kromskop system, the Joly system was not inexpensive. The starter kit of plate holder, <b>compensating</b> <b>filter,</b> one taking screen and one viewing screen cost $30 (the equivalent of at least $750 in 2010 dollars) and additional viewing screens were $1 each (the equivalent of at least $25 in 2010 dollars). This system, too, soon died of neglect, although in fact it pointed the way to the future.|$|E
40|$|In {{the case}} of chest tomography, {{particularly}} the mediastinum and the pulmonary hilar tomography are difficult to the matter without exposure time factor. For the reason, these region are much used rectilinear tomography for the shortening of exposure time. But, as for the visuability of the morphology, it is advisable for these region to use in multi-directional tomography. However, it {{is in need of}} long exposure time. Therefore, authours experimented with stereozonography of short exposure time by circular movemant using a <b>compensating</b> <b>filter</b> and computed radiography. Shifting distance of Xray tube for stereofraphy was 10 % of distance between focus and object. The exposure time and exposure angle were 0. 6 sec, 2 θ= 3 ° respectively. As a result, this stereozonography was excellent in the visuabilities of mediastinal region, including bronchovascular shadows, and could examine in details is on images of these region. My using the <b>compensating</b> <b>filter,</b> the improved image guality of mediastinal region was obstained. On addition, using computed radiographic system, even if artifact araised from disagreement between from of the computed radiographic system, even if artifact araised from disagreement metween form of rhe <b>compensating</b> <b>filter</b> and mediastinal region, authors could easily restored non-diagnostic image by orverall density manipulation and unsharp mask piltering...|$|E
40|$|As {{complexity}} {{for treating}} patients increases, {{so does the}} risk of error. Some pub-lications have suggested that record and verify (R&V) systems may contribute in propagating errors. Direct data transfer {{has the potential to}} eliminate most, but not all, errors. And although the dosimetric consequences may be obvious in some cases, a detailed study does not exist. In this effort, we examined potential errors in terms of scenarios, pathways of occurrence, and dosimetry. Our goal was to prioritize error prevention according to likelihood of event and dosimetric impact. For conventional photon treatments, we investigated errors of incorrect source-to-surface distance (SSD), energy, omitted wedge (physical, dynamic, or universal) or <b>compensating</b> <b>filter,</b> incorrect wedge or <b>compensating</b> <b>filter</b> orientation, improper rotational rate for arc therapy, and geometrical misses due to incorrect gantry, col-limator or table angle, reversed field settings, and setup errors. For electron beam therapy, errors investigated included incorrect energy, incorrect SSD, along with geometric misses. For special procedures we examined errors for total body irra-diation (TBI, incorrect field size, dose rate, treatment distance) and LINA...|$|E
50|$|The {{cold spot}} is mainly {{anomalous}} because it stands out {{compared to the}} relatively hot ring around it; {{it is not unusual}} if one only considers the size and coldness of the spot itself. More technically, its detection and significance depends on using a <b>compensated</b> <b>filter</b> like a Mexican hat wavelet to find it.|$|R
40|$|A {{suboptimal}} dynamic compensator {{to be used}} {{in conjunction}} with the ordinary discrete-time Kalman filter is derived. The resultant <b>compensated</b> Kalman <b>filter</b> has the property that steady-state bias estimation errors, resulting from modelling errors, are eliminated. The implementation of the <b>compensated</b> Kalman <b>filter</b> involves the use of accumulators in the residual channels in addition to the nominal dynamic model of the stochastic system...|$|R
40|$|This is {{the third}} {{in a series of}} papers that develop a new and {{flexible}} model to predict weak-lensing (WL) peak counts, which {{have been shown to be}} a very valuable non-Gaussian probe of cosmology. In this paper, we compare the cosmological information extracted from WL peak counts using different filtering techniques of the galaxy shear data, including linear filtering with a Gaussian and two <b>compensated</b> <b>filters</b> (the starlet wavelet and the aperture mass), and the nonlinear filtering method MRLens. We present improvements to our model that account for realistic survey conditions, which are masks, shear-to-convergence transformations, and non-constant noise. We create simulated peak counts from our stochastic model, from which we obtain constraints on the matter density Ω_m, the power spectrum normalisation σ_ 8, and the dark-energy parameter w_ 0. We use two methods for parameter inference, a copula likelihood, and approximate Bayesian computation (ABC). We measure the contour width in the Ω_m-σ_ 8 degeneracy direction and the figure of merit to compare parameter constraints from different filtering techniques. We find that starlet filtering outperforms the Gaussian kernel, and that including peak counts from different smoothing scales helps to lift parameter degeneracies. Peak counts from different smoothing scales with a <b>compensated</b> <b>filter</b> show very little cross-correlation, and adding information from different scales can therefore strongly enhance the available information. Measuring peak counts separately from different scales yields tighter constraints than using a combined peak histogram from a single map that includes multiscale information. Our results suggest that a <b>compensated</b> <b>filter</b> function with counts included separately from different smoothing scales yields the tightest constraints on cosmological parameters from WL peaks. Comment: 14 pages, 12 figures, published versio...|$|R
40|$|This work {{considers}} {{the implementation of}} recursive identification algorithms based on hyperstability concepts with polyphase structures. It is shown that the SPR condition required for convergence of these schemes can always be met by using a sufficiently high polyphase expansion factor M. ForagivenM, the degree of persistent excitation required for parameter convergence is obtained. When a priori knowledge about the unknown system is available, a <b>compensating</b> <b>filter</b> can be designed to avoid {{the need for a}} high M...|$|E
40|$|By {{conventional}} tomography using a <b>compensating</b> <b>filter,</b> and {{by these}} procedures and/or CT following pneumomediastinography in the evalution of heilar and mediastinal abnormality, it was confrimed that {{the well and}} lumen of trachea, main bronchus, tracheal bifurcation, upper lobe bronchus, a part of upper segmental bronchus and blood veaaels in hilar and mediastinal region were demonstrated clearly and in detail. Consequently, the mediastinal and hilar informations were evaluated more accurately : the lymph node involvement (swelling), the extension, orgination of mediastinal and hilar lesion and/or the presence and extent of adhesion or cancerous lesion...|$|E
40|$|Abstract-In {{a recent}} paper [I] a method has been {{presented}} {{for the design of}} amplitude- and phase-compensating IIR filters. The procedure is based on inversion of the transfer function, identified in the:-domain. Unstable poles are reflected into the unit circle, and the spoiled phase response is equalized again using allpass sections. This paper points to the fact that this method is not optimal {{in the sense that it}} does not reach the minimum error for a given order (or provides the desired error compensation by using a too high order). By refitting the <b>compensating</b> <b>filter,</b> a lower order filter is obtained with the same performance. In the investigated case the order is reduced from 34 / 34 to 26 / 26. I...|$|E
40|$|Motivated by motion <b>compensated</b> <b>filtering</b> in image {{processing}} {{we consider the}} problem of sampling and reconstruction of signals with sam-pling rates below the Nyquist rate. It is assumed that temporal depen-dence can be induced via motion. This way, the data consists of both spatial and temporal sampling and we analyze here the conditions for reconstruction {{for a number of}} typical motions. Extensive simulation ex-periments are also provided which further support the analysis. ...|$|R
40|$|This paper {{introduces}} a high-quality, low-cost video converter for {{the conversion of}} interlaced TV signals into progressive display formats of the same or higher frame repetition rate. This conversion is performed by motion <b>compensated</b> <b>filtering</b> which is preceded by motion estimation. The applied motion estimation algorithm operates on blocks sized 4 × 4 pixels. For each of these blocks, a candidate vector is chosen out of temporal and spatial predecessors using a displayed field differences error criterion. The optimal candidate is updated {{by a series of}} pixel recursive steps. The filter algorithm employs a motion <b>compensating</b> median <b>filter</b> whose shape depends on the motion vector. A fallback mode is implemented to deal with areas for which no accurate motion vectors could be derived. Single-chip integration of the whole format conversion system is feasible...|$|R
30|$|The motion {{estimation}} step {{is followed by}} the wavelet decomposition step and by motion <b>compensated</b> <b>filtering,</b> which is performed in the wavelet domain, using a variable number of motion hypotheses (depending on their reliability) and data dependent weighted averaging. The weights used for temporal filtering are derived from the {{motion estimation}} reliabilities and from the noise standard deviation estimate. The remaining noise is removed using the spatial filter from [20], which operates in wavelet domain and uses luminance to restore lost details in the corresponding depth image.|$|R
40|$|Total skin {{electron}} beam is a specialized technique that involves irradiating the entire {{skin from the}} skin surface to only a few millimetres in depth. In the Stanford technique, the patient is in a standing position and six different directional positions are used during treatment. Our technique uses large {{electron beam}}s in six directions with an inclinable couch on motorized table and a <b>compensating</b> <b>filter</b> was also used to spread the electron beam and move its intensity peak. Dose uniformity measurements were performed using Gafchromic films which indicated that the surface dose was 2. 04 ± 0. 05 Gy. This technique can ensure the dose reproducibility because the patient is fixed in place using an inclinable couch on a motorized table...|$|E
40|$|This paper {{describes}} a new speaker adaptation strategy that we term speaker specific compensation. The basic {{idea is to}} transform speech of a speaker {{in a way that}} renders it recognizable by a speaker dependent classifier built for another speaker. The <b>compensating</b> <b>filter</b> is learnt as a cepstral vector using labeled speech samples of the speaker. Using some ideas about combining multiple pattern classifiers, we present a new speaker independent speech recognition system that uses a few speaker dependent classifiers along with a bank of cepstral compensating vectors learnt for a large number of other speakers. Each of the speaker dependent classifiers is trained on the given speech samples of only one speaker and is never retrained or adapted thereafter. We present some results to illustrate the effectiveness of this speaker specific compensation idea...|$|E
40|$|A broad-beam-delivery {{system for}} heavy-charged-particle {{radiotherapy}} often employs multiple collimators and a range-compensating filter, which potentially offer complex beam customization. In treatment planning, it is however {{difficult for a}} conventional pencil-beam algorithm {{to deal with these}} structures due to beam-size growth during transport. This study aims to resolve the problem with a novel computational model. The pencil beams are initially defined at the range <b>compensating</b> <b>filter</b> with angular-acceptance correction for the upstream collimators followed by the range compensation effects. They are individually transported with possible splitting near the downstream collimator edges to deal with its fine structure. The dose distribution for a carbon-ion beam was calculated and compared with existing experimental data. The penumbra sizes of various collimator edges agreed between them to a submillimeter level. This beam-customization model will complete an accurate and efficient dose-calculation algorithm for treatment planning with heavy charged particles. Comment: 11 pages, 5 figure...|$|E
40|$|C-arm based tomographic 3 D imaging {{is applied}} in an {{increasing}} number of minimal invasive procedures. Due to the limited acquisition speed for a complete projection data set required for tomographic reconstruction, breathing motion is a potential source of artifacts. Intra-scan motion estimation and compensation is required. Here, a scheme for projection based local breathing motion estimation is combined with an anatomy adapted interpolation strategy and subsequent motion <b>compensated</b> <b>filtered</b> back projection. This approach is applied in animal experiments on a flat panel C-arm system delivering improved image quality in 3 D liver tumor imaging...|$|R
30|$|Compared to {{the method}} of [27], the number of {{operations}} performed in a search step is approximately the same, since we calculate similarity measures using two imaging modalities and choose a set of best candidate blocks, while in [27] search is performed twice, using only depth information, first time on noisy depth pixels and second time on hard-thresholded depth estimates. Similarly, the proposed motion <b>compensated</b> <b>filtering</b> does not add much overhead, since filtering weights are calculated during the motion estimation step. In total, number of the operations performed by the proposed algorithm and the method from [27] is comparable.|$|R
3000|$|... is {{the size}} of the motion {{estimation}} block and Nblocks is the number of blocks in the frame. Then, we perform the wavelet transform and motion <b>compensated</b> temporal <b>filtering</b> in the wavelet domain. This step requires [...]...|$|R
40|$|Author {{attempted}} tomography with a <b>compensating</b> <b>filter</b> and {{computed tomography}} (CT) after pneumomediastinography (PMG) to assess mediastinum-related lesions. Author found the results as follows. 1. The tomography with a compensatonf filter after PMG was especially useful {{for evaluating the}} left paratracheal, tracheobronchial lymphnodes swelling. 2. On the lateral tomogram after PMG, thymus and pretracheal, retrotracheal stripe were well demonstrated. 3. Comparative studies with plain and/or enhanced CT and CT after PMG were made on the invasions and/or adhesions to the neighboring structures. On the plain and/or enhanced CT, we could diagnose with 61 % sensitivity, 100 % specificity, and 74 % accuracy. On the other hand, we could diagnose with 80 % sensitivity, 100 % soecificity, and 90 % accuracy on the CT after PMG. Therefore, this method proved to be vary useful for diagnosing the mediastinal invasions and/or adhesions. 4. Visualization of the pleura by this method is useful to diggerentiatepulmonay lesions from mediastional lesions. These results suggest that this method would {{be considered to be}} advantageous and advisable in evaluating mediastinum-related lesions...|$|E
40|$|The work {{presented}} in this thesis investigates the characterization of laser produced plasmas and develops applications in opacity experiments using plasma emission as a back-lighting source. A diagnostic is developed to analyse bremsstrahlung emission from hot electrons produced in the laser plasma interaction. Combining a <b>compensating</b> <b>filter</b> technique with an x-ray diode array allows for the hot electron temperature to be deduced with good accuracy (± 0. 5 keV). A layered target comprising 0. 8 um Al and 1. 0 um Fe is used to investigate the opacity of iron plasma. A laser of modest irradiance (~ 10 ^ 15 W cm^- 2) is fired onto the aluminium surface, producing Al K-alpha emission (1. 5 keV) {{which is used to}} measure the opacity of the conductively heated iron layer. The aluminium plasma is characterized using source broadened spectroscopy and continuum emission analysis. The experimental transmission data is in good agreement with 2 D modelling using opacities from the Ionised Materials Package. A line focus back-lighter, produced using a high power laser system, is characterized through imaging the time and spectrally integrated emission profile of the plasma using a crossed-slit camera. The emission profile is used to infer a spatially dependant electron temperature profile. Finally, a Ti K-alpha back-lighter is used to investigate the temporal evolution of the Rayleigh-Taylor instability in a laser produced plasma. A target, seeded with an initial perturbation between layers of copper and plastic exhibited Rayleigh-Taylor growth within the first 100 ps of the interaction with a growth rate of 10 ± 2 ns^- 1. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|This paper {{introduces}} the <b>compensated</b> Kalman <b>filter,</b> a suboptimal state estimator {{which can be}} used to eliminate steady-state bias errors when it is used in conjunction with the mismatched steady-state (asymptotic) time-invariant Kalman-Bucy filter. The uncompensated mismatched steady state Kalman-Bucy filter exhibits bias errors whenever the nominal plant parameters used in the filter design are different from the actual plant parameters. The approach used relies on the utilization of the residual (innovations) process of the mismatched filter to estimate, via a Kalman-Bucy filter, the state estimation errors and subsequent improvements of the state estimate. The <b>compensated</b> Kalman <b>filter</b> augments the mismatched steady state Kalman-Bucy filrby the introduction of additional dynamics and feedforward integral compensation channels...|$|R
30|$|In this article, we have {{presented}} {{a method for}} removing spatially variable and signal dependent noise in depth images acquired using a depth camera based on the time-of-flight principle. The proposed method operates in the wavelet domain and uses multi hypothesis motion estimation to perform temporal filtering. One of the important novelties of the proposed method is that the motion estimation is performed on both depth and luminance sequences {{in order to improve}} the accuracy of the estimated motion. Another important novelty is that we use motion estimation reliabilities derived from both the depth and the luminance to derive coefficients for motion <b>compensated</b> <b>filtering</b> in wavelet domain. Finally, our temporal noise suppression is locally adaptive, to account for the non-stationary character of the noise in depth sensors.|$|R
40|$|Multi-Hypothesis motion <b>compensated</b> <b>filter</b> (MHMCF) {{utilizes}} {{a number}} of hypotheses (temporal predictions) to estimate the current pixel which is corrupted with noise. While showing remarkable denoising results, MHMCF is computationally intensive as full search is employed in the expectation of finding good temporal predictions {{in the presence of}} noise. In the frame of MHMCF, a fast denoising algorithm FMHMCF is proposed in this paper. With edge preserved low-pass prefiltering and noise-robust fast multihypothesis search, FMHMCF could find reliable hypotheses while checking very few search locations, so that the denoising process can be dramatically accelerated. Experimental results show that FMHMCF can be 10 to 14 times faster than MHMCF, while achieving the same or even better denoising performance with up to 1. 93 dB PSNR (peak-signal-noise-ratio) improvement...|$|R
40|$|This paper {{discusses}} {{the use of}} differential structures for active functions at microwaves. Starting from {{the example of a}} single-ended LNA structure, we show the advantages of using a differential approach with the design examples of a LNA, a floating negative resistance and a differential <b>compensated</b> LC <b>filter</b> structur...|$|R
40|$|In {{the very}} near future, weak lensing surveys will map the {{projected}} density of the universe in an unbiased way over large regions of the sky. In order {{to interpret the results}} of studies it is helpful to develop an understanding of the errors associated with quantities extracted from the observations. In a generalization of one of our earlier works, we present estimators of the cumulants and cumulant correlators of the weak lensing convergence field, and compute the variance associated with these estimators. By restricting ourselves to so-called <b>compensated</b> <b>filters</b> we are able to derive quite simple expressions for the errors on these estimates. We also separate contributions from cosmic variance, shot noise and intrinsic ellipticity of the source galaxies. Comment: 12 pages, including 5 figures, uses mn. sty. Substantially revised version accepted by MNRA...|$|R
40|$|Results are {{presented}} of noise reduction by motion <b>compensated</b> temporal <b>filtering</b> in a noisy IR image sequence and of moving target detection in an air-to-ground IR image sequence. In {{the case of}} motion <b>compensated</b> temporal <b>filtering</b> our approach consists of estimating the optical flow between successive frames and subsequently averaging {{a small number of}} images. Moving targets are detected by first estimating the optical flow between successive frames. Target detection amounts to comparing a predicted frame, based on the estimated optical flow, to the actual frame. Thus, it is possible to detect targets without making assumptions on their appearance. The particular motion estimator used was found to be especially useful in the case of IR imagery, because the estimator is relatively insensitive to noise and global brightness variation...|$|R
50|$|This {{implementation}} uses a {{high-pass filter}} at the non-inverting input {{to generate the}} phase shift and negative feedback to <b>compensate</b> for the <b>filter's</b> attenuation.|$|R
40|$|The {{operation}} of the digital fuzzy controller in the control system of a <b>filter</b> <b>compensating</b> device. Defined the parameters for the digital. Describes the dependence of {{the efficiency of the}} work of the regulator from the parameters necessary to configure it. Given the vectorial methods of optimization of objective functions of the fuzzy control system of a <b>filter</b> <b>compensating</b> device. It is offered to use optimization methods for the selection of the optimal parameters for setting the digital fuzzy controller...|$|R
40|$|A 4 -D wavelet-based {{transform}} is {{used for}} efficient and scalable compression of multi-view video data. It is composed of a 1 -D temporal wavelet transform, namely Motion <b>Compensated</b> Temporal <b>Filtering</b> (MCTF), a 1 -D view-directional wavelet transform, namely Disparity <b>Compensated</b> View <b>Filtering</b> (DCVF), and a 2 -D spatial transform. The latter {{is the subject of}} study in this paper. Whereas usually fixed isotropic wavelet or wavelet packet transforms have been used in the past for the spatial decomposition of the temporal-view-directional highpass bands, we now introduce the usage of adaptive anisotropic wavelet packet transforms as a generalization of wavelet and wavelet packet transforms. An efficient algorithm to adaptively find the rate-distortion optimal joint anisotropic basis for temporal-view-directional multi-view video subbands is derived. It is shown that the adaptive anisotropic transform performs best, compared with conventional wavelet or wavelet packet transforms...|$|R
40|$|Denoising {{is one of}} {{the most}} common and {{important}} task in video processing systems and abundant efforts have been made on video denoising nowadays. Multihypothesis motion <b>compensated</b> <b>filter</b> (MHMCF) is an effective video denoising method, which combines multiple hypotheses obtained from motion estimation through a number of reference frames by weighted average to suppress noise. However, MHMCF only considers denoising of grayscale video signal. In this paper, we apply MHMCF to color video denoising, where the RGB video is first transformed to the luminance-color difference space before denoising. Instead of using traditional YCbCr color conversion, we propose a novel color conversion matrix which is adaptive to the noise variance in R, G, B channels. Simulation results demonstrate that our proposed color space conversion method can successfully improve the denoising performance for color video. © 2010 IEEE...|$|R
40|$|Abstract—In many {{communication}} and signal processing systems, {{it is highly}} desirable to implement an efficient narrow-band filter that decimate or interpolate the incoming signals. This paper presents hardware efficient <b>compensated</b> CIC <b>filter</b> over a narrow band frequency that increases the speed of down sampling by using multiplierless decimation filters with polyphase FIR filter structure. The proposed work analyzed the performance of <b>compensated</b> CIC <b>filter</b> on the bases of the improvement of frequency response with reduced hardware complexity in terms of no. of adders and multipliers and produces the filtered results without any alterations. CIC compensator filter demonstrated that by using compensation with CIC filter improve the frequency response in passed of interest 26. 57 % with the reduction in hardware complexity 12. 25 % multiplications per input sample (MPIS) and 23. 4 % additions per input sample (APIS) w. r. t. FIR filter respectively...|$|R
40|$|Abstract- This paper {{presents}} an innovative solution to power quality problems using power quality-improving (PQI) appliances. PQI appliances conduct currents that supplement and correct {{the sum of}} the other load currents within a premise. From the utility side, the premise housing a PQI appliance thus becomes an improved, if not ideal, utility customer. The PQI appliance improves both harmonic power quality and power factor while performing its normal function, such as heating water. In this paper, the water heater PQI appliance is used as an example to demonstrate the control circuit design and function. Both computer simulation results and laboratory experiment results are presented to demonstrate the effectiveness of the approach. The estimated costs of the PQI controller and of harmonic <b>compensating</b> <b>filters</b> are compared to show that the PQI appliance may be an economic way to provide power quality improvement at the building level. 1 I...|$|R
40|$|This paper {{presents}} a novel arbitrary shape {{region of interest}} (ROI) coding for scalable wavelet video codec. The motion information of the ROIs is estimated by macro block padding and polygon matching. The derived motion vectors are set as the motion trajectory of the samples to generate a one-dimensional temporal signal. This signal is then filtered to reduce the temporal redundancy using motion <b>compensated</b> temporal <b>filtering</b> for arbitrary shape ROI. Compared to traditional non-ROI coding, the reconstructed quality of the ROI coding can be significantly improved at low bit-rates. The efficiency of motion <b>compensated</b> temporal <b>filtering</b> (MCTF) based on arbitrary ROI is also {{compared with that of}} the video object coding in MPEG- 4. Based on large number of experiments, the ability of the MCTF to reduce the temporal redundancy is demonstrated as better than (or at least comparable to) that of MPEG- 4. (C) 2011 Published by Elsevier B. V...|$|R
40|$|Abstract — In this paper, a novel {{framework}} for scalable multiview video coding is described. A well known wavelet based scalable coding scheme for single-view video sequences {{has been adopted}} and extended to match {{the specific needs of}} scalable multi-view video coding. Motion <b>compensated</b> temporal <b>filtering</b> (MCTF) is applied to each video sequence of each camera. The use of a wavelet lifting structure guarantees perfect invertibility of this step, and as a consequence of its open-loop architecture, SNR and temporal scalability are attained. Correlations between the temporal subbands of adjacent cameras are reduced by a novel disparity <b>compensated</b> view <b>filtering</b> (DCVF), method which is also lifting based and open-loop to enable view scalability. Spatial scalability and entropy coding are achieved by the JPEG 2000 spatial wavelet transform and EBCOT coding, respectively. Rate allocation along the temporal-view-filtered subbands is done by means of an RD-optimal algorithm. Experimental results show the high scaling capability in terms of SNR, temporal and view scalability. I...|$|R
40|$|Abstract — In this paper, {{we propose}} two wavelet-based {{frameworks}} which allow fully scalable multi-view video coding. Using a 4 -D wavelet transform, both schemes generate a bitstream {{that can be}} truncated to achieve a temporally, view-directionally, and/or spatially downscaled representation of the coded multiview video sequence. Well-known wavelet-based scalable coding schemes for single-view video sequences have been adopted and extended to match {{the specific needs of}} scalable multi-view video coding. Motion <b>compensated</b> temporal <b>filtering</b> (MCTF) is applied to each video sequence of each camera to exploit temporal correlation and inter-view dependencies are exploited with disparity <b>compensated</b> view <b>filtering</b> (DCVF). A spatial wavelet transform is utilized either before and after temporal-viewdirectional decomposition (2 D+T+V+ 2 D scheme) or only after the temporal-view-directional decomposition (T+V+ 2 D scheme) for spatial decorrelation. The influence of the two different approaches on spatial scalability is shown in this paper as well as the superior coding efficiency of both codecs compared with simulcast coding. I...|$|R
40|$|Abstract—This paper {{presents}} a novel framework to achieve scalable multi-view image compression and view synthesis. The open-loop wavelet lifting scheme for geometric filtering {{has been exploited}} to achieve SNR scalability and view-type scalability (mono-, stereo-, or multi-view). Spatial scalability is achieved by employing in-band prediction which removes correlations among subbands (level-by-level) via shift-invariant references obtained by Overcomplete Discrete Wavelet Transforms (ODWT). We propose a novel In-band Disparity <b>Compensated</b> View <b>Filtering</b> (I-DCVF) approach, akin to the Motion <b>Compensated</b> Temporal <b>Filtering</b> (MCTF), for achieving a scalable multi-view codec. In our codec, hybrid prediction is proposed to deal with occlusions, and a novel cost function in dynamic programming (DP) for disparity estimation is introduced to improve view synthesis quality. Experiments show comparable results at full resolution and significant improvements at coarser resolutions compared to a conventional spatial prediction scheme. View synthesis efficiency is extensively improved by utilizing disparity estimation from the proposed DP approach. Index Terms—Multi-view image, disparity compensation, occlusion, overcomplete discrete wavelet transform, dynamic programming, scalability. I...|$|R
