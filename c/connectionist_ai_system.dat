0|1415|Public
40|$|In {{this work}} a {{discrete}} counterpart to the continuous harmonic potential field approach is suggested. The extension to the discrete case {{makes use of}} the strong relation HPF-based planning has to <b>connectionist</b> artificial intelligence (<b>AI).</b> <b>Connectionist</b> <b>AI</b> <b>systems</b> are networks of simple, interconnected processors running in parallel {{within the confines of}} the environment in which the planning action is to be synthesized. It is not hard to see that such a paradigm naturally lends itself to planning on weighted graphs where the processors may be seen as the vertices of the graph and the relations among them as its edges. Electrical networks are an effective realization of <b>connectionist</b> <b>AI.</b> The utility of the discrete HPF (DHPF) approach is demonstrated in three ways. First, the capability of the DHPF approach to generate new, abstract, planning techniques is demonstrated by constructing a novel, efficient, optimal, discrete planning method called the M* algorithm. Also, its ability to augment the capabilities of existing planners is demonstrated by suggesting a generic solution to the lower bound problem faced by the A* algorithm. The DHPF approach is shown to be useful in solving specific planning problems in communication. It is demonstrated that the discrete HPF paradigm can support routing on-the-fly while the network is still in a transient state. It is shown by simulation that if a path to the target always exist and the switching delays in the routers are negligible, a packet will reach its destination despite the changes in the network which may simultaneously take place while the packet is being routed...|$|R
40|$|Described {{within this}} paper is an {{adaptive}} hypermedia system (AHS) that utilises symbolic <b>AI</b> and <b>connectionist</b> <b>AI</b> to provide generic student modelling. The needs for generic tutoring systems are discussed, {{in terms of a}} system that is applicable to a multitude of teaching domains, whilst maintaining diagnostic facilities of the student...|$|R
40|$|The {{ubiquity of}} systems using {{artificial}} intelligence or "AI" has brought increasing {{attention to how}} those systems should be regulated. The choice of how to regulate <b>AI</b> <b>systems</b> will require care. <b>AI</b> <b>systems</b> {{have the potential to}} synthesize large amounts of data, allowing for greater levels of personalization and precision than ever before [...] -applications range from clinical decision support to autonomous driving and predictive policing. That said, there exist legitimate concerns about the intentional and unintentional negative consequences of <b>AI</b> <b>systems.</b> There are many ways to hold <b>AI</b> <b>systems</b> accountable. In this work, we focus on one: explanation. Questions about a legal right to explanation from <b>AI</b> <b>systems</b> was recently debated in the EU General Data Protection Regulation, and thus thinking carefully about when and how explanation from <b>AI</b> <b>systems</b> might improve accountability is timely. In this work, we review contexts in which explanation is currently required under the law, and then list the technical considerations that must be considered if we desired <b>AI</b> <b>systems</b> that could provide kinds of explanations that are currently required of humans...|$|R
40|$|The {{focus of}} this thesis is to provide an {{artificial}} intelligence (<b>AI)</b> <b>system</b> that can develop spatial intelligence. A MATLAB application of a genetic algorithm <b>AI</b> <b>system</b> has been implemented. The <b>AI</b> <b>system</b> will incorporate three dimensional (3 D) objects that can learn to maneuver in 3 D space {{so that they may}} be assembled with each other. As an example, two rectangles with holes and a nut will maneuver itself onto a screw. The performance of the <b>AI</b> <b>system</b> will then be recorded as video results...|$|R
40|$|The new {{paradigm}} of service-oriented architecture leads to new requirements for software systems. These requirements must be met when integrating <b>AI</b> <b>systems</b> into a SOA landscape, too. However, most <b>AI</b> <b>systems</b> today are monolithic. In this paper we develop a general approach to provide artificial intelligence based {{on the concept of}} intelligent agents as services in a SOA landscape. Here the <b>AI</b> <b>system</b> itself and each intelligent agent will act as services. 1...|$|R
50|$|Unlike consumer-faced <b>AI</b> {{recommendations}} <b>systems</b> {{which have}} a high tolerance for false positives and negatives, even a very low rate of false positives or negatives rate may cost the total credibility of <b>AI</b> <b>systems.</b> Industrial <b>AI</b> applications are usually dealing with critical issues related to safety, reliability, and operations. Any failure in predictions could incur a negative economic and/or safety impact on the users and discourage them to rely on <b>AI</b> <b>systems.</b>|$|R
40|$|NASA's Office of Aeronautics and Space Technology (OAST) has {{instituted a}} 'Systems Autonomy' {{technology}} development {{program aimed at}} the creation of increasingly sophisticated <b>AI</b> <b>systems.</b> Expert system production will in due course be followed by fully autonomous <b>AI</b> <b>systems</b> capable of the complex mission objectives of the Space Station, {{as well as of}} lunar and Martian exploration. <b>AI</b> <b>systems</b> must demonstrate the degree of their autonomy in a real-time operational environment, and then undergo mating to robotics technologies. OAST's Systems Autonomy Program and Robotics Program are discussed with a view to anticipated difficulties in their integration...|$|R
40|$|The {{hypothesis}} is considered that: Once an <b>AI</b> <b>system</b> with roughly human-level general intelligence is created, an “intelligence explosion ” involving the relatively rapid creation of increasingly more generally intelligent <b>AI</b> <b>systems</b> will very likely ensue, {{resulting in the}} rapid emergence of dramatically superhuman intelligences. Various arguments against this hypothesis are considered and found wanting. ...|$|R
5000|$|Sol, the onboard {{computer}} system. Described as a 'Mark IV' <b>A.I.</b> <b>system.</b>|$|R
50|$|From the {{cognitive}} science perspective, every natural intelligent system is hybrid because it performs mental operations {{on both the}} symbolic and subsymbolic levels. For {{the past few years}} there has been an increasing discussion of the importance of <b>A.I.</b> <b>Systems</b> Integration. Based on notions that there have already been created simple and specific <b>AI</b> <b>systems</b> (such as systems for computer vision, speech synthesis, etc., or software that employs some of the models mentioned above) and {{now is the time for}} integration to create broad <b>AI</b> <b>systems.</b> Proponents of this approach are researchers such as Marvin Minsky, Ron Sun, Aaron Sloman, and Michael A. Arbib.|$|R
40|$|Computer {{systems that}} are {{designed}} explicitly to exhibit intentionality embody a phenomenon of increasing cultural importance. In typical discourse about arti�cial intelligence (<b>AI)</b> <b>systems,</b> system intentionality is often seen as a technical and ontological property of a program, resulting from its underlying algorithms and knowledge engineering. Infuenced by hermeneutic approaches to text analysis and drawing from the areas of actor-network theory and philosophy of mind, this paper proposes a humanistic framework for analysis of <b>AI</b> <b>systems</b> stating that system intentionality is narrated and interpreted by its human creators and users. We {{pay special attention to}} the discursive strategies embedded in source code and technical literature of software systems that include such narration and interpretation. Finally, we demonstrate the utility of our theory with a close reading of an <b>AI</b> <b>system,</b> Hofstadter and Mitchell's Copycat...|$|R
50|$|For {{all public}} AI Services, Products, Libraries, Frameworks, {{if you can}} exploit {{vulnerabilities}} to make the <b>AI</b> <b>system</b> or component stop working, or lead the <b>AI</b> <b>system</b> or component make wrong decisions, please register. The target areas include Computer Vision, Voice Recognition, Natural Language Processing, Autonomous Driving, Malware Detection, etc. The target AI frameworks include mainstream frameworks like TensorFlow, TorchNet, Caffe, etc.|$|R
40|$|When <b>AI</b> <b>systems</b> {{interact}} with humans in the loop, {{they are often}} called on to provide explanations for their plans and behavior. Past work on plan explanations primarily involved the <b>AI</b> <b>system</b> explaining the correctness of its plan and the rationale for its decision {{in terms of its}} own model. Such soliloquy is wholly inadequate in most realistic scenarios where the humans have domain and task models that differ significantly from that used by the <b>AI</b> <b>system.</b> We posit that the explanations are best studied in light of these differing models. In particular, we show how explanation {{can be seen as a}} "model reconciliation problem" (MRP), where the <b>AI</b> <b>system</b> in effect suggests changes to the human's model, so as to make its plan be optimal with respect to that changed human model. We will study the properties of such explanations, present algorithms for automatically computing them, and evaluate the performance of the algorithms...|$|R
5000|$|Lynnanne Zager as {{the voice}} of G.I.N.A., the Woods family {{personal}} home <b>A.I.</b> <b>system</b> and helper.|$|R
40|$|A {{rule-based}} {{expert system}} is demonstrated {{to have both}} a symbolic computational network representation and a sub-symbolic connectionist representation. These alternate views enhance {{the usefulness of the}} original system by facilitating introduction of connectionist learning methods into the symbolic domain. The connectionist representation learns and stores metaknowledge in highly connected subnetworks and domain knowledge in a sparsely connected expert network superstructure. The total connectivity of the neural network representation approximates that of real neural systems and hence avoids scaling and memory stability problems associated with other connectionist models. Keywords. symbolic <b>AI,</b> <b>connectionist</b> <b>AI,</b> connectionism, neural networks, learning, reasoning, expert networks, expert systems, symbolic models, sub-symbolic models. y Paper given to the symposium Approaches to Cognition, the fifteenth annual Symposium in Philosophy held at the University of North Carolina, Gree [...] ...|$|R
50|$|The core idea of <b>A.I.</b> <b>systems</b> {{integration}} is making individual software components, such as speech synthesizers, interoperable with other components, such as common sense knowledgebases, {{in order to}} create larger, broader and more capable <b>A.I.</b> <b>systems.</b> The main methods that have been proposed for integration are message routing, or communication protocols that the software components use to communicate with each other, often through a middleware blackboard system.|$|R
5000|$|In the {{intelligence}} explosion scenario inspired by Good's hypothetical, recursively self-improving <b>AI</b> <b>systems</b> quickly transition from subhuman general intelligence to superintelligent. Nick Bostrom's 2014 book Superintelligence: Paths, Dangers, Strategies sketches out Good's argument in greater detail, while making a broader case for expecting <b>AI</b> <b>systems</b> to eventually outperform humans across the board. Bostrom cites writing by Yudkowsky on inductive value learning {{and on the}} risk of anthropomorphizing advanced <b>AI</b> <b>systems,</b> e.g.: [...] "AI might make an apparently sharp jump in intelligence purely {{as the result of}} anthropomorphism, the human tendency to think of 'village idiot' and 'Einstein' as the extreme ends of {{the intelligence}} scale, instead of nearly indistinguishable points on the scale of minds-in-general." ...|$|R
5000|$|... 70 {{styles and}} 17 drummers are modeled {{in-depth}} in real-time by an <b>A.I.</b> <b>system</b> and not static patterns ...|$|R
5000|$|Autonomous <b>AI</b> <b>systems</b> may be {{assigned}} the wrong goals by accident. Two AAAI presidents, Tom Dietterich and Eric Horvitz, {{note that this}} is already a concern for existing systems: [...] "An important aspect of any <b>AI</b> <b>system</b> that interacts with people is that it must reason about what people intend rather than carrying out commands literally." [...] This concern becomes more serious as AI software advances in autonomy and flexibility.|$|R
5000|$|Yudkowsky {{writes on}} the {{importance}} of friendly artificial intelligence in smarter-than-human systems. This informal goal is reflected in MIRI's recent publications as the requirement that <b>AI</b> <b>systems</b> be [...] "aligned with human interests". Following Bostrom and Steve Omohundro, MIRI researchers believe that autonomous generally intelligent <b>AI</b> <b>systems</b> will have default incentives to treat human operators as competitors, obstacles, or threats if they are not specifically designed to promote their operators' goals.|$|R
40|$|Theory {{grounding}} {{is suggested}} {{as a way}} to address the unresolved cognitive science issues of systematicity and productivity. Theory grounding involves grounding the theory skills and knowledge of an embodied artificially intelligent (<b>AI)</b> <b>system</b> by developing theory skills and knowledge from the bottom up. It is proposed that theory grounded <b>AI</b> <b>systems</b> should be patterned after the psychological developmental stages that infants and young children go through in acquiring naïve theories. Systematicity and productivity are properties of certain representational systems indicating the range of representations the systems can form. Systematicity and productivity are likely outcomes of theory grounded <b>AI</b> <b>systems</b> because systematicity and productivity are theoretical concepts. Theory grounded systems should be well oriented to acquire and develop these theoretical concepts...|$|R
5000|$|Intelligent {{agents have}} been defined many {{different}} ways. According to Nikola Kasabov <b>AI</b> <b>systems</b> should exhibit the following characteristics: ...|$|R
50|$|Psyclone is a {{software}} platform, or an <b>AI</b> operating <b>system</b> (AIOS), developed by Communicative Machines Laboratories {{for use in}} creating large, multi modal <b>A.I.</b> <b>systems.</b> The system is an implementation of a blackboard system that supports the OpenAIR message protocol. Psyclone is available for free for non-commercial purposes and has therefore often been used by research institutes on low budgets and novice A.I. developers.|$|R
40|$|We {{consider}} {{the problem of}} efficient "on the fly" tuning of existing, or legacy, Artificial Intelligence (<b>AI)</b> <b>systems.</b> The legacy <b>AI</b> <b>systems</b> are allowed to be of arbitrary class, albeit the data they are using for computing interim or final decision responses should posses an underlying structure of a high-dimensional topological real vector space. The tuning method that we propose enables dealing with errors without the need to re-train the system. Instead of re-training a simple cascade of perceptron nodes {{is added to the}} legacy system. The added cascade modulates the <b>AI</b> legacy <b>system's</b> decisions. If applied repeatedly, the process results in a network of modulating rules "dressing up" and improving performance of existing <b>AI</b> <b>systems.</b> Mathematical rationale behind the method is based on the fundamental property of measure concentration in high dimensional spaces. The method is illustrated with an example of fine-tuning a deep convolutional network that has been pre-trained to detect pedestrians in images...|$|R
30|$|Taken as an {{individual}} aspect {{within the context of}} the AI-based judgments, both the description of functional contexts and the input of case descriptions far surpass the reception and recollection capacities of human beings. The computer-based combinatory handling of function and case descriptions exceeds the capabilities of the human brain in terms of processing speed. As everywhere in the utilization of computers, high storage capacity and data processing speed are also key features of <b>AI</b> <b>systems.</b> An <b>AI</b> <b>system,</b> however, cannot establish a content connection between knowledge/experience and the current problem situation, only a formal context: the inference mechanism of the <b>AI</b> <b>system</b> diagnoses concordance or non-concordance of a problem based on a canon of rules, case descriptions, statistics, etc., conclusions are drawn and the respective robot actions are generated.|$|R
40|$|Sophisticated {{autonomous}} AI {{may need}} to base its behavior on fuzzy concepts such as well-being or rights. These concepts cannot be given an explicit formal definition, but obtaining desired behavior still requires a way to instill the concepts in an <b>AI</b> <b>system.</b> To solve the problem, we review evidence suggesting that the human brain generates its concepts using a relatively limited set of rules and mechanisms. This suggests {{that it might be}} feasible to build <b>AI</b> <b>systems</b> that use similar criteria for generating their own concepts, and could thus learn similar concepts as humans do. Major challenges to this approach include the embodied nature of human thought, evolutionary vestiges in cognition, the social nature of concepts, and the need to compare conceptual representations between humans and <b>AI</b> <b>systems...</b>|$|R
40|$|The paper {{presents}} a network modeling tool called Net-Clause Language (NCL), integrating some connectionist-like and some symbolic processing features in a unified computational environment. Unlike the other connectionist symbol processing approaches, NCL re{{presents a}}nd processes symbols not by implementing them as patterns {{of activity in}} a traditional neural network, rather it uses some connectionist ideas to organize symbolic computation in a more flexible way. The paper presents two examples (deductive inference and image processing) which show how the connectionist-like and symbolic features of NCL can benefit each from the other. 1 Introduction The long standing symbolic versus <b>connectionist</b> debate in <b>AI</b> has proved {{that none of the}} approaches taken in isolation could be fruitful. Now it is widely acknowledged that instead of looking for a single universal formalism an <b>AI</b> <b>system</b> should be built out of diverse components, some connectionist and some symbolic. This view is well expr [...] ...|$|R
40|$|Abstract- The {{push for}} {{real-time}} autonomous <b>AI</b> <b>systems</b> {{has been sought}} for decades. The DoD has spent considerable R&D budgets looking for systems that can operate with no or little supervision. These systems must process incredible amounts of heterogeneous information looking for information. In order to achieve these goals, we must affect real learning, or “learning with experience, ” in autonomous <b>AI</b> <b>systems</b> [10]. The goal of having machines that learn with experience {{is one of the}} most intriguing problems in computer science and computer engineering. As the types of problems we would like <b>AI</b> <b>systems</b> to solve get more complex and more diverse, it is becoming a necessary task as well. Unfortunately, by its nature, learning is somewhat fuzzy, and random in nature, for information comes at us in stochastic fashion [22]. In fact, the overall goal is to learn things we do not yet know, and in doing so find patterns that we can learn. This constitutes not patter matching, or pattern recognition, but is, in fact, pattern discovery. Nonetheless, we would like a mathematical framework for machine learning to aid in our understanding and improve our ability to make progress toward autonomous <b>AI</b> <b>systems.</b> ...|$|R
5000|$|In {{the video}} game, Mass Effect 2, the {{character}} Joker makes {{a reference to}} the <b>AI</b> <b>system</b> [...] "EDI" [...] singing Daisy Bell.|$|R
5000|$|Arnold Simulator - a {{software}} platform, in experimental stage of development, designed for rapid prototyping of <b>AI</b> <b>systems</b> with highly dynamic neural network topologies.|$|R
40|$|Artifical Intelligence (AI) is useful. AI {{can deliver}} more {{functionality}} for reduced cost. AI {{should be used}} more widely but won’t be unless developers can trust adapative, nondeterministic, or complex <b>AI</b> <b>systems.</b> Verification and validation is one method used by software analysts to gain that trust. <b>AI</b> <b>systems</b> have features that make them hard to check using conventional V&V methods. Nevertheless, as we show in this article, there are enough alternative readily-available methods that enable the V&V of AI software...|$|R
40|$|Enhanced, more reliable, {{and better}} {{understood}} {{than in the}} past, artificial intelligence (<b>AI)</b> <b>systems</b> can make providing healthcare more accurate, affordable, accessible, consistent, and efficient. However, AI technologies have not been as well integrated into medicine as predicted. In order to succeed, medical and computational scientists must develop hybrid systems that can effectively and efficiently integrate the experience of medical care professionals with capabilities of <b>AI</b> <b>systems.</b> After providing a general overview of artificial intelligence concepts, tools, and techniques, Medical A...|$|R
40|$|Variable binding {{has long}} been a {{challenge}} to connectionists. Attempts to perform variable binding using localist and distributed connectionist representations are discussed, and problems inherent in each type of representation are outlined. Keywords: Neural networks, localist representations, distributed representations, systematicity, variable binding, unification, inference. 3 1. Introduction In recent years research on intelligent systems has split into two paradigms, the classical symbolic artificial intelligence (AI) paradigm and the <b>connectionist</b> <b>AI</b> paradigm. Researchers working in symbolic AI maintain that the correct level at which to model intelligent systems (including the human mind) is that of the symbol. The symbol is an entity in a computational system that can have arbitrary designations and is used to refer to an entity in the outside world, and the main assumptions on which this paradigm rests were coherently outlined by Newell and Simon (Newell & Simon, 1980) und [...] ...|$|R
25|$|To {{support the}} drop-in/drop-out co-op, 343 Industries built a dynamic <b>AI</b> <b>system</b> for the fireteam companions {{to assist the}} player when a session is played alone.|$|R
30|$|There is no uniform {{definition}} of “artificial intelligence (AI)”. With {{regard to the}} contexts discussed here, it is relevant that the robots equipped with <b>AI</b> <b>systems</b> have a knowledge base and sensor systems to record environmental data. Relationships between environmental information and the knowledge base are established {{with the help of}} an inference mechanism. In this manner, the robot is able to make “decisions” without the individual decision-making paths and the results of decisions having been anticipated by the programmer of the <b>AI</b> <b>system.</b>|$|R
30|$|This {{is crucial}} {{because not all}} {{abnormalities}} are representative of disease and must be actioned. <b>AI</b> <b>systems</b> learn on a case-by-case basis. However, unlike CAD systems, which just highlight {{the presence or absence}} of image features known to be associated with a disease state [15, 16], <b>AI</b> <b>systems</b> look at specific labelled structures and also learn how to extract image features either visible or invisible to the human eye. This approach mimics human analytical cognition, allowing for better performance than that obtained with old CAD software [17].|$|R
