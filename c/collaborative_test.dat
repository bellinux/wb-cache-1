29|174|Public
50|$|Enterprise test {{software}} (ETS) {{is a type}} {{of software}} that electronics and other manufacturers use to standardize product testing enterprise-wide, rather than simply in the test engineering department. It is designed to integrate and synchronize test systems to other enterprise functions such as research and development (R&D), new product introduction (NPI), manufacturing, and supply chain, overseeing the <b>collaborative</b> <b>test</b> processes between engineers and managers in their respective departments.|$|E
50|$|Panaya is an Israeli {{software}} as {{a service}} (SaaS) company that provides cloud-based quality management services for enterprise applications worldwide. Its solutions include Change impact analysis, automated code remediation, <b>collaborative</b> <b>test</b> management and test-execution, and ALM acceleration. The company’s solutions enable companies to save their information technology applications life cycle costs and minimize the risks associated with system changes. Panaya’s service runs on the Amazon Elastic Compute Cloud.|$|E
40|$|For {{this study}} we {{examined}} collaborative assessment in counseling 820 German students who were going abroad and {{who were exposed to}} the Test to Measure Intercultural Competence (TMIC). A randomized pretest–posttest control group design was used. The control group did not get any test feedback. The remaining groups received written feedback or written plus oral <b>collaborative</b> <b>test</b> feedback. Repeated measures linear mixed effects modeling showed that <b>collaborative</b> <b>test</b> feedback positively influenced students’ self-appraisal of their intercultural competence (TMIC-SA); their values on three stages of change; as well as their self-understanding, self-confidence, and perceived benefit from test participation. It is concluded that collaborative assessment and feedback can enhance self-appraised intercultural competence, thereby showing its potential in intercultural training...|$|E
40|$|University of Minnesota Ph. D. dissertation. August 2012. Major: Educational Psychology. Advisors: Joan Garfield, Michelle Everson. 1 {{computer}} file (PDF); xiii, 237 pages, appendices A-C. The {{purpose of this}} study was to explore how <b>collaborative</b> <b>tests</b> could be implemented successfully in online introductory statistics courses. The research questions set forth were (1) What is the impact of using <b>collaborative</b> <b>tests</b> in an online statistics course on students´ learning? (2) What is the effect of using <b>collaborative</b> <b>tests</b> on students’ attitudes towards statistics? and (3) How does using a required consensus on <b>collaborative</b> <b>tests</b> vs. a nonconsensus approach affect group discussions? Three <b>collaborative</b> <b>tests</b> were implemented in two online sections of the EPSY- 3264 Basic and Applied Statistics course offered at the University of Minnesota. The two sections were identical in terms of the instructor, assignments, assessments, and lecture notes used. The only difference between the two sections was in terms of the format of the <b>collaborative</b> <b>tests</b> that were used. In the consensus section, students worked together in groups and submitted one answer per group. In the nonconsensus section, students worked on the test together in groups but submitted tests individually. Students were randomly assigned to a consensus (n= 32) or a nonconsensus (n= 27) section of the course. The Comprehensive Assessment of Important Outcomes in Statistics (CAOS) test was used to measure students´ learning, both at the beginning and at the end of the course. The Survey Of Attitudes Toward Statistics (SATS- 36) instrument was used to measure students’ change in attitudes towards statistics. Another instrument designed by the instructor to measure students’ perspective towards <b>collaborative</b> <b>testing</b> was also used. Students’ discussions during the three <b>collaborative</b> <b>tests</b> were reviewed using the Pozzi, Manca, Persico, & Sarti, (2007) framework to evaluate and monitor computersupported collaborative learning. Discussions were coded using three dimensions, iv (Social, Teaching and Cognitive) and their indicators from the framework and then converted to quantitative variables that were used in the data analysis. No significant relationship was found between different sections and students’ scores on the CAOS. There was no significant difference in students’ attitudes towards statistics between the two sections. However, for both sections, students’ attitudes increased in terms of their intellectual knowledge, skills, and interest towards statistics after taking the three <b>collaborative</b> <b>tests.</b> The effects of using a required consensus on <b>collaborative</b> <b>tests</b> vs. a nonconsensus approach on group discussions did not seem to be significantly different. The two formats of the <b>collaborative</b> <b>tests</b> that were used seemed to support students’ discussion more in terms of the Cognitive dimension compared to the Social and Teaching dimensions. Overall, the results suggest that the difference between using two different formats of <b>collaborative</b> <b>tests</b> is not significant. However, the results support what research on <b>collaborative</b> <b>tests</b> in face-to-face courses have demonstrated before such as an increase in students’ attitudes towards learning (e. g., Giraud & Enders, 2000; Ioannou & Artion, 2010). Instructors and researchers should continue to use and experiment with <b>collaborative</b> <b>tests</b> in online introductory statistics courses. The study here is just the beginning in terms of conducting empirical research into what teaching methods and assessments should be used in an effort to create quality and effective online statistics courses...|$|R
50|$|In addition, {{there are}} {{numerous}} other smaller <b>collaborative</b> <b>tests,</b> on {{a whole range of}} consumer products from anti-wrinkle creams to athletic shoes.|$|R
40|$|Introduction: <b>Collaborative</b> <b>testing</b> is a {{learning}} strategy that {{provides students with}} the opportunity to learn and practice collaboration. This study aimed to determine the effect of <b>collaborative</b> <b>testing</b> on test performance and retention of course content in nursing students of Shiraz University of Medical Sciences, Shiraz, Iran. Methods: This quasi-experimental study was carried out on 84 students enrolled in the course of Medical-Surgical 2 in Spring 2013 and Fall 2013 semesters. The control group consisting of 39 students participated in the first mid-term exam in an individual format. The intervention group, on the other hand, consisted of 45 students who took the test in a two-stage process. The first stage included an individual testing, while the second stage was a collaborative one given in groups of five individuals chosen randomly. Four weeks later, in order to investigate retention of the course content, both groups took part in the second mid-term exam held individually. Results: The study findings showed significant difference between the mean scores in the intervention group in the Fall 2013 semester (p= 0. 001). Besides, a statistically significant difference was found between the two groups regarding the tests mean scores (p= 0. 001). Moreover, retention of course content improved in the collaborative group (p= 0. 001). Conclusion: The results indicated an increase in test performance and a long-term learning enhancement in <b>collaborative</b> <b>testing</b> compared with the traditional method. <b>Collaborative</b> <b>testing,</b> as an active learning technique and a valuable assessment method, can help nursing instructors provide the alumni with strong problemsolving and critical thinking abilities at healthcare environments...|$|R
40|$|This {{paper is}} a {{continuation}} of a 2009 paper presented at the 7 th International Workshop on Structural Health Monitoring that described various wind turbine condition-monitoring techniques. This paper presents the results obtained by various condition- monitoring techniques from a damaged Gearbox Reliability <b>Collaborative</b> <b>test</b> gearbox...|$|E
40|$|The {{analytical}} procedure {{described in this}} document is a tentative method for the determination of uranium isotopes in soil and air samples. It is being collaboratively tested according to an interagency agreement. Data from the <b>collaborative</b> <b>test</b> will be examined and information on the precision and accuracy of the method will be obtained...|$|E
40|$|A <b>collaborative</b> <b>test</b> on the {{determination}} of natamycin in cheese rind was carried out. The described method comprises: 1) sampling, 2) homogination, 3) extraction, 4) clean up, 5) concentration, 6) determination - spectroscopy - HPLC-UV. For practical reasons the steps 3 to 6 only could be incorporated in this test. Eight laboratories participated. Three samples of decreasing levels were distributed...|$|E
40|$|<b>Collaborative</b> <b>testing</b> {{has been}} shown to improve {{performance}} but not always content retention. In this study, we investigated whether <b>collaborative</b> <b>testing</b> could improve both performance and content retention in a large, introductory biology course. Students were semirandomly divided into two groups based on their performances on exam 1. Each group contained equal numbers of students scoring in each grade category (“A”–“F”) on exam 1. All students completed each of the four exams of the semester as individuals. For exam 2, one group took the exam a second time in small groups immediately following the individually administered test. The other group followed this same format for exam 3. Individual and group exam scores were compared to determine differences in performance. All but exam 1 contained a subset of cumulative questions from the previous exam. Performances on the cumulative questions for exams 3 and 4 were compared for the two groups to determine whether there were significant differences in content retention. Even though group test scores were significantly higher than individual test scores, students who participated in <b>collaborative</b> <b>testing</b> performed no differently on cumulative questions than students who took the previous exam as individuals...|$|R
40|$|This {{research}} is creating acoustic communications (acomms) technologies enabling underwater sensor networks and distributed systems. Figure 1. Project MISSION involves the <b>collaborative</b> <b>testing</b> of NPS Seaweb and NUS Unet underwater acoustic {{networks in the}} shallow (15 - 40 meter), noisy waters of Singapor...|$|R
40|$|Large component-based {{systems are}} often built {{from many of}} the same components. As {{individual}} component-based software systems are developed, tested and maintained, these shared components are repeatedly manipulated. As a result there are often significant overlaps and synergies across and among the different test efforts of different component-based systems. However, in practice, testers of different systems rarely collaborate, taking a test-all-by-yourself approach. As a result, redundant effort is spent testing common components, and important information {{that could be used to}} improve testing quality is lost. The goal of this research is to demonstrate that, if done properly, testers of shared software components can save effort by avoiding redundant work, and can improve the test effectiveness for each component as well as for each component-based software system by using information obtained when testing across multiple components. To achieve this goal I have developed <b>collaborative</b> <b>testing</b> techniques and tools for developers and testers of component-based systems with shared components, applied the techniques to subject systems, and evaluated the cost and effectiveness of applying the techniques. The dissertation research is organized in three parts. First, I investigated current testing practices for component-based software systems to find the testing overlap and synergy we conjectured exists. Second, I designed and implemented infrastructure and related tools to facilitate communication and data sharing between testers. Third, I designed two testing processes to implement different <b>collaborative</b> <b>testing</b> algorithms and applied them to large actively developed software systems. This dissertation has shown the benefits of <b>collaborative</b> <b>testing</b> across component developers who share their components. With <b>collaborative</b> <b>testing,</b> researchers can design algorithms and tools to support collaboration processes, achieve better efficiency in testing configurations, and discover inter-component compatibility faults within a minimal time window after they are introduced...|$|R
40|$|Collaborative {{testing has}} been shown to enhance student {{performance}} compared to individual testing. It is suggested that collaborative testing promotes higher order thinking, but research has yet to explore this assumption directly. The aim {{of this study was to}} explore the benefits of collaborative testing on overall performance, as well as performance on higher order thinking questions. It was hypothesised that, compared to individual test results, students would perform better overall and on higher order thinking questions under collaborative testing conditions. It was expected that these differences would be equal when comparing students of different academic abilities (i. e. ‘upper’, ‘middle’ and ‘lower’ performers). Undergraduate students completed an individual followed by a <b>collaborative</b> <b>test</b> as part of summative assessment. Analyses revealed that with the exception of upper performers, students performed better overall on the <b>collaborative</b> <b>test.</b> Additionally, regardless of their academic abilities, students performed better on the higher order thinking questions under collaborative conditions. This improvement was equal across different academic abilities, suggesting that collaborative testing promotes higher order thinking even when taking into account previous academic achievement. The acceptability and application of collaborative testing is discussed. Full Tex...|$|E
40|$|Abstract. Open Source Software (OSS) {{projects}} have the {{unique opportunity to}} reach an unprecedented level of software quality by tapping into its community and collaborative power. However, the community process of collaborative software Quality Assessment (QA) may not reach its full potential or worse be easily jeopardised by malevolent entities {{because there is a}} lack of protection mechanisms, easy-to-use enabling mechanisms and clear incentives. We propose such mechanisms as part of a decentralised <b>collaborative</b> <b>test</b> and QA framework centred on the OSS actors. ...|$|E
40|$|An IEA {{working group}} on ferritic/martensitic steels for fusion applications, {{consisting}} of researchers from Japan, European Union, USA, and Switzerland, met {{at the headquarters of}} the Joint European Torus, Culham, UK. At the meeting, preliminary data generated on the large heats of steels purchased for the IEA program and on other heats of steels were presented and discussed. Second purpose of the meeting was to continue planning and coordinating the <b>collaborative</b> <b>test</b> program in progress on reduced-activation ferritic/martensitic steels. The majority of this report consists of viewographs for the presentations...|$|E
50|$|The UNH-IOL {{operates}} testing {{programs on}} an annual membership basis called consortia. Each consortium is a collaboration between equipment vendors, test equipment manufacturers, industry forums, and the UNH-IOL in a particular technology. The <b>collaborative</b> <b>testing</b> model is intended to distribute {{the costs associated with}} maintaining a high-quality testing program among the consortium members.|$|R
50|$|The {{mission of}} All-America Selections is “to promote new garden seed {{varieties}} with superior garden performance judged in impartial trials in North America.” To accomplish this, the All-America Selections (AAS) organization oversees a <b>collaborative</b> <b>testing</b> program involving horticulture professionals {{all across the}} continent. AAS is governed by a Board consisting of four officers and six directors.|$|R
40|$|Individual testing, often via {{multiple}} choice questions, {{tends to be}} the norm in post secondary education. <b>Collaborative</b> <b>testing</b> has been proposed as an alternative and the benefits have been widely documented (Lusk 2 ̆ 6 Conklin, 2003; Sandahl, 2009, Cortright et al, 2003). In the first stage of <b>collaborative</b> <b>testing,</b> students write the exam individually. In the second stage students write the same exam in groups (ideally 4) where they are required to come to consensus. Our own experience with these 2 -stage collaborative midterm exams in very different programs (Nursing and Engineering) resulted in improved performance and overwhelmingly positive student review, consistent with the results of Gilley 2 ̆ 6 Clarkston (2014), whose 2 -stage model we followed. Using the Immediate Feedback Assessment Technique (IF-AT) scratch cards (Epstein, 2001), we created the opportunity to dispel misconceptions, allow students to discover correct answers during testing, and provide partial credit for multiple-choice questions. In addition to the improved self-confidence and metacognitive skill development promoted by collaboration (Carvalho, 2010), we hypothesize that delivering the guided answers using IF-AT cards, at a time when students are most receptive, aids in retention of the tested concepts. We will present preliminary data providing some support for this hypothesis from our recent study to assess whether collaboration with immediate feedback or collaboration alone is superior in encouraging retention. A hands-on simulation of this procedure will be included, along with tips for implementing this form of testing in other science (and non-science) classrooms. Carvalho, M. K. F. (2010). Assessing changes in performance and monitoring processes in individual and <b>collaborative</b> <b>tests</b> according to students 2 ̆ 7 metacognitive skills. European Journal of Cognitive Psychology, 22 (7), 1107 - 1136. Cortright, R. N., Collins, H. L., Rodenbaugh, D. W. 2 ̆ 6 Di Carlo, S. E. (2003). Student Retention of Course Content Is Improved By Collaborative-Group Testing. Advances in Physiology Education, 27 : 102 - 108 Epstein, M. L., Epstein B. B., and Brosvic, G. M. (2001). Immediate Feedback During Academic Testing. Psychological Reports, 88 (3 Pt 1), 889 - 894. Gilley, B. H. 2 ̆ 6 Clarkston, B. (2014). Collaborative Testing: Evidence of Learning in a Controlled In-Class study of Undergraduate Students. Journal of College Science Teaching, 43 (3), 83 - 91. Lusk, M., 2 ̆ 6 Conklin, L. (2003). <b>Collaborative</b> <b>testing</b> to promote learning. The Journal of nursing education, 42 (3), 121 - 124. Sandahl, S. S. (2009). <b>Collaborative</b> <b>testing</b> as a learning strategy in nursing education: A review of the literature. Nursing education perspectives, 30 (3), 171 - 175...|$|R
40|$|Abstract- Although several {{approaches}} for automated test case generation {{have been proposed}} {{over the last few}} years, automated test case generation for object oriented application based on their behavior is still in an infant stage. This thesis proposed a new framework for automated test case generation from UML diagrams for object oriented applications based on the behavior. This thesis implements this framework with the development of a tool integrates all types of automated test case generation from four UML diagrams (Class Diagram, Sequence diagram and State-chart Diagram and Use case Diagram) in an object oriented application. Parsing the Petal files to generate the <b>collaborative</b> <b>Test</b> cases from these UML Diagrams using Pattern Discovery and Information Retrieval is discussed...|$|E
40|$|A {{transonic}} wing, {{designed to}} accentuate key transition physics, is tested at cryogenic {{conditions at the}} National Transonic Facility at NASA Langley. The <b>collaborative</b> <b>test</b> between Boeing and NASA is aimed at assessing the facility for high-Reynolds number testing of configurations with significant regions of laminar flow. The test shows a unit Reynolds number upper limit of 26 M/ft for achieving natural transition. At higher Reynolds numbers turbulent wedges emanating from the leading edge bypass the natural transition process and destroy the laminar flow. At lower Reynolds numbers, the transition location is well correlated with the Tollmien-Schlichting-wave N-factor. The low-Reynolds number {{results suggest that the}} flow quality is acceptable for laminar flow testing if the loss of laminar flow due to bypass transition can be avoided...|$|E
40|$|In this paper, {{the results}} of an {{international}} <b>collaborative</b> <b>test</b> case relative to the production of a Direct Numerical Simulation and Lagrangian Particle Tracking database for turbulent particle dispersion in channel flow at low Reynolds number are presented. The objective of this test case is to establish a homogeneous source of data relevant to the general problem of particle dispersion in wall-bounded turbulence. Different numerical approaches and computational codes have been used to simulate the particle-laden flow and calculations have been carried on long enough to achieve a statistically-steady condition for particle distribution. In such stationary regime, a comprehensive database including both post-processed statistics and raw data for the fluid and for the particles has been obtained. The complete datasets can be downloaded from the web a...|$|E
50|$|AppScan version 2 was {{released}} in February 2001, adding policy recognition engine and knowledge database, an automatic and customizable crawler engine and attack simulator. Version 3 {{was released}} in April 2002, adding <b>collaborative</b> <b>testing</b> capabilities, where different tasks can be assigned to different testers; {{and a number of}} user interface enhancements in both the scanning and reporting sections of the program. By 2003 AppScan was used by over 500 enterprise customers and revenues reached $30m.|$|R
50|$|Since January 2012, Avnu Alliance {{has worked}} with its test house, the University of New Hampshire InterOperability Laboratory (UNH-IOL), to test {{interoperability}} and provide validation for its certified products. Avnu’s certification testing officially began at UNH-IOL in February 2013. The UNH-IOL is a neutral, independent testing service that works with other audio/video industry consortiums, including the Ethernet Alliance, Wi-Fi Alliance and IPv6 Forum, to provide third-party verification, lower costs through <b>collaborative</b> <b>testing,</b> and help guide industry acceptance of a technology standard.|$|R
40|$|In {{collaborative}} two-stage exams students {{complete a}} test {{as individuals and}} then immediately complete the same, or very similar, test in groups. We compared two-stage <b>collaborative</b> <b>testing</b> to individual testing to determine which format has a greater effect on student learning in an undergraduate Earth and Ocean Science course. A cross-over design allowed {{students to participate in}} both the control (individual) and treatment (collaborative) conditions. In both the individual and collaborative conditions students completed the same set of questions twice, which controlled for any potential performance gain caused by more frequent testing. Learning was measured as the change in students’ individual performance on questions given in the individual stage and after the midterm, calculated as percent change and normalized change. When students were tested in groups they showed significantly greater improvement on subsequent individual testing then when tested only as individuals. There {{was no significant difference in}} the amount of improvement experienced by ‘upper’, ‘middle’ or ‘lower’ achieving students as categorized by their first-stage midterm score. Our results demonstrate that students of all achievement levels can learn from each other while simultaneously being assessed, and we believe <b>collaborative</b> <b>testing</b> is a powerful tool that can be applied to any undergraduate science classroom...|$|R
40|$|Keywords:NASTRAN {{finite element}} analysis; Experiment test; Structural {{strength}} analysis of loader Abstract. This paper main aim {{to explain the}} finite element analysis and experimental test on loading machine structure strength analysis method of finite element analysis, test data as the input and boundary conditions of finite element calculation model of <b>collaborative</b> <b>test</b> accuracy. The finite element analysis and calculation conditions of test conditions {{can be used as}} a supplement and perfection. Introductions In the ordinary mechanical industry, mechanical structure strength analysis method with numerical simulation and experiment analysis method. The numerical analysis method is mainly finite element analysis method, this method is based on the physical and technical information to establish the finite element model, using the general finite element software calculation an...|$|E
40|$|Considerable recent {{attention}} {{has been focused on}} efficiency investigation of ultrasonic impact treatment being one of the post [...] weld treatments improving reliability and life of welded joints. Investigations were carried out at different times in Russia, Ukraine, France, Japan, Norway, Sweden and USA. In each specific case independent problems were solved by these investigations which were performed in accordance with various appropriate procedures. The work under review is intended to determine and study UIT efficiency according to IIW <b>Collaborative</b> <b>Test</b> Program on Improvement Methods, Doc. XIII-WG 2 - 30 - 94, by comparison with well-known methods for increasing fatigue strength of welded joints. The paper presents comparative fatigue test results for specimens in the as-welded condition and specimens treated by hammer peening, shot peening, TIG dressing, ultrasonic impact treatment and a combination of TIG dressing and ultrasonic impact treatment. Welded specimens were fabricated from WELDOX 420 steel at SSAB. Also, the advisability of applying UIT to improve fatigue limit and life of welded joints is shown...|$|E
40|$|Learning through collaboration, even in {{a testing}} situation, has many {{benefits}} stemming from peer‐to‐peer interactions. A <b>collaborative</b> <b>test,</b> hereafter called a two stage exam, typically has the following format (Stearns 1996,) : 1 st Stage: Students write exam as individuals. 2 Two‐stage exams are reported to improve retention of concepts by individual students (Cortright et al. 2003) in addition participants report reduction in test anxiety (Russo and Warren 1999), greater motivation to study and think critically during a two stage exam (Shindler 2004). No previous study has tested for retention while controlling for the additional “time‐on‐task ” of a two‐stage exam format, in which students {{are exposed to the}} same questions twice. nd Stage: Groups of 3 – 5 students immediately complete a second identical (or very similar) exam. The 2 nd Stage typically takes much less time. Research Questions 1) Does collaboration during a two‐stage exam increase students’ retention of concepts more than a test written individually? 2) What, if any, specific effects does collaboration during a test have on students ’ retention of concepts...|$|E
40|$|The {{purpose of}} this study is to {{investigate}} how learners work collaboratively during their astronomy quiz preparation. Research has been conducted extensively on cooperative and collaborative learning, however not much has been done to investigate learners preparations in quizzes. There has been very little research done on learners‟ interactions and behaviour but more on comparing learners‟ performance individually with when they work collaboratively. The subjects of this study were five schools from the Gauteng West district comprising of teams made up of four learners. These five teams were observed and two members per team interviewed. The teams were observed in order to understand how they interact with each other and the roles assumed by each member and the teacher. The use of interviews was to get more understanding of the benefits of the team and also find out if the quiz contributed in their astronomy content gain. The results show that all teams had an understanding of their roles and responsibilities as members, however only two teams displayed more commitment to attain the goal by advancing to further rounds of the astronomy quiz. In view of the findings I concluded that <b>collaborative</b> <b>testing</b> is an effective strategy for improving learning and potentially for the retention of content. Keywords <b>Collaborative</b> learning <b>Collaborative</b> <b>testing</b> Peer collaboration Astronomy qui...|$|R
40|$|The main {{challenge}} of Embrapa {{is to develop}} a model of genuine Brazilian tropical agriculture and livestock. To get this task, the quality of laboratories results is mandatory, increasing the demand for reference materials. Projects were proposed to produce reference materials to support the national agriculture laboratories and consolidate a network able to perform reliable and reproducible analytical testing laboratory within the internationally standards required. Reference materials were produced and available to interested laboratories and <b>collaborative</b> <b>tests</b> were conducted to obtain consensus values. The results and statistical evaluations were performed with the use of software developed by Embrapa Pecuária Sudeste. 201...|$|R
5000|$|<b>Testing</b> <b>collaborative</b> {{production}} of HTA information for national adaptation and reporting ...|$|R
40|$|This paper {{focuses on}} one of the {{features}} of e-testing, which has not been discussed until now, called <b>collaborative</b> <b>test</b> construction by several test-authors in distant places. It is well known that collaborative work has many advantages. The main idea {{of this paper is to}} improve test qualities by applying the unique advantages of collaborative work to e-testing construction and using prediction tools while constructing test. The analysis of collaborative e-testing construction identified the number of test-authors as the most important factor in test validity, while test reliability depends more on the participation of an expert. Based on these findings, a collaborative e-testing construction system was developed that using several prediction tools to improve the reliability of tests constructed by novice test-authors. An experiment in which a novice and an expert test-author each constructed tests by using these prediction tools in one case and not using in the other showed that those constructed using them were more reliable, although those constructed by the expert had even higher reliability...|$|E
30|$|Established {{language}} {{testing and}} measurement models devote {{a great amount}} of attention to appropriately modeling student interaction with tasks in a given Assessment Situation. The task, however, is an auxiliary tool to externalize ability features of interest, obtain individual scores, afford generalizable ability interpretations, etc. As educational measurement and language assessment professionals embrace the use of more complex test tasks, incorporate technology to allow for <b>collaborative</b> <b>test</b> interactions, experiment with nontraditional test administration conditions, shift from documenting past achievement to characterizing students’ potential for learning, which are all critical elements of dynamic assessment, consideration should be given to the increasingly complex and inextricable nature of person and task interaction. Next, we revisit dynamic assessment, focusing specifically on game-based learning and GBAs. We will articulate reasons why a reciprocal ability-in language user-in context orientation provides a more apt conceptual fit for the claims and inferences inherent in dynamic assessments. Finally, we will again consider that interaction at the reciprocal level refutes the claim that ability can be neatly separated from task in the assessment situation (i.e., scoring, interpretation, and use processes) and highlight the unique measurement challenges it presents.|$|E
40|$|Approved {{for public}} release; {{distribution}} is unlimited. Modern Test and Evaluation has long supported acquisition of warfighting {{systems in the}} United States Navy. As the complexity and long-term supportability of these systems has dramatically increased, the need to successfully, and incrementally test and evaluate families of systems, including their interfaces, has become even more critical. Long established techniques and methodologies for T&E may still apply, but new factors must be addressed. As the Navy continues to grapple with acquisition reform, and also looks to transform itself in the future, the Warfighters’ needs have essentially remained the same – delivery of the best, most effective weapons, delivered as soon as possible, and made easy to operate and maintain. Without an equally effective developmental and operational test and evaluation process, the United States Navy cannot satisfy this need. This thesis examines T&E today and where it must go in the future. It provides recommendations for T&E enhancements, and explores several areas where the Navy, and in many cases, Joint Services, are already looking towards future, integrated and <b>collaborative</b> <b>test</b> and evaluation. Civilian, Naval Surface Warfare Center, Corona Division, United States Nav...|$|E
40|$|The National Bureau of Standards (NBS) Data Management Technology Program is discussed. The NBS Data Management Technology Program {{addresses}} {{major problems}} encountered {{during the following}} stages of an application's lifetime: requirements analysis and data base design, system selection and implementation, operations management and conversion. Products developed include standard software specifications, guides to best practice, standard data elements and representations, and reports documenting the experiences of other organizations {{as they attempt to}} improve the management of their computing resources. Data base Laboratory facilities are maintained for the investigation and analysis of state of the art data base technology. These facilities support <b>collaborative</b> <b>testing</b> with researchers, vendors, users, and standards developers...|$|R
40|$|Service-Oriented Architecture (SOA) and Web Services (WS) have {{received}} significant attention recently. Even though WS {{are based on}} open standards and support software interoperability, but the trustworthy issues of WS has actually limited the growth of WS applications as organizations do not trust those WS developed by other vendors {{and at the same}} time they do not have access to the source code. This paper addressed this issue by proposing several solutions including specification-based verification and validation, <b>collaborative</b> <b>testing,</b> and group testing. The key concept is that it is possible to provide a comprehensive evaluation of WS even if their source code is not available...|$|R
5000|$|Synchronous {{usability}} testing methodologies involve {{video conferencing}} or employ remote application sharing {{tools such as}} WebEx. WebEx and GoToMeeting are {{the most commonly used}} technologies to conduct a synchronous remote usability test. However, synchronous remote testing may lack the immediacy and sense of [...] "presence" [...] desired to support a <b>collaborative</b> <b>testing</b> process. Moreover, managing inter-personal dynamics across cultural and linguistic barriers may require approaches sensitive to the cultures involved. Other disadvantages include having reduced control over the testing environment and the distractions and interruptions experienced by the participants' in their native environment. One of the newer methods developed for conducting a synchronous remote usability test is by using virtual worlds.|$|R
