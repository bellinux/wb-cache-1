535|166|Public
25|$|The team {{semantics}} for dependence {{logic is}} {{a variant of}} Wilfrid Hodges' <b>compositional</b> <b>semantics</b> for IF logic. There exist equivalent game-theoretic semantics for dependence logic, {{both in terms of}} imperfect information games and in terms of perfect information games.|$|E
2500|$|Graph-theoretic methods, {{in various}} forms, have proven {{particularly}} useful in linguistics, since natural language often lends itself well to discrete structure. Traditionally, syntax and <b>compositional</b> <b>semantics</b> follow tree-based structures, whose expressive power {{lies in the}} principle of compositionality, modeled in a hierarchical graph. More contemporary approaches such as head-driven phrase structure grammar model the syntax of natural language using typed feature structures, which are directed acyclic graphs.|$|E
50|$|This {{treatment}} of quantifiers has been essential in achieving a <b>compositional</b> <b>semantics</b> for sentences containing quantifiers.|$|E
40|$|In {{distributional}} semantics studies, {{there is}} a growing attention in compositionally determining the distributional meaning of word sequences. Yet, compositional distributional models depend on a large set of parameters that have not been explored. In this paper we propose a novel approach to estimate parameters for a class of compositional distributional models: the additive models. Our approach leverages on two main ideas. Firstly, a novel idea for extracting <b>compositional</b> distributional <b>semantics</b> examples. Secondly, an estimation method based on regression models for multiple dependent variables. Experiments demonstrate that our approach outperforms existing methods for determining a good model for <b>compositional</b> distributional <b>semantics.</b> ...|$|R
40|$|We {{define a}} {{hierarchy}} of <b>compositional</b> formal <b>semantics</b> of algebraic polynomial systems over F-algebras by abstract interpretation. This generalizes classical formal language theoretical results and contextfree grammar flow-analysis algorithms in the same uniform framework of universal algebra and abstract interpretation...|$|R
40|$|This paper {{describes}} ongoing work {{in developing}} a <b>compositional</b> trace-based <b>semantics</b> and proof system for a real-time language. The semantics models distributed processes communicating over asynchronous FIFO communication channels. Sending processes can specify time-out periods for individual messages. Messages not received within their time-out period are `lost'. Program behavior is modeled as traces of events, including events (such as asynchronous messages) which occur after termination. The proof system uses specification triples with explicit variables for time and program traces. 1 Introduction This paper describes ongoing work {{in developing a}} <b>compositional</b> trace-based <b>semantics</b> and proof system for a real-time language. The semantics models distributed processes communicating over asynchronous FIFO communication channels. Sending processes can specify time-out periods for individual messages. Messages not received within their time-out period are `lost'. Additional work in r [...] ...|$|R
50|$|The team {{semantics}} for dependence {{logic is}} {{a variant of}} Wilfrid Hodges' <b>compositional</b> <b>semantics</b> for IF logic. There exist equivalent game-theoretic semantics for dependence logic, {{both in terms of}} imperfect information games and in terms of perfect information games.|$|E
5000|$|Evans {{received}} his PhD in linguistics from Georgetown University in 2000. His research {{relates to the}} domains of space and time. He also works on lexical and <b>compositional</b> <b>semantics,</b> as well as figurative language, abstract thought, and digital communication, especially Emoji.|$|E
50|$|The denotational <b>compositional</b> <b>semantics</b> {{presented}} {{above is}} very general {{and can be}} used for functional, imperative, concurrent, logic, etc. programs (see 2008a). For example it easily provides denotation semantics for constructs that are difficult to formalize using other approaches such as delays and futures.|$|E
40|$|In {{previous}} work with J. Hedges, we formalised a generalised quantifiers theory of natural language in categorical <b>compositional</b> distributional <b>semantics</b> {{with the help}} of bialgebras. In this paper, we show how quantifier scope ambiguity can be represented in that setting and how this representation can be generalised to branching quantifiers. Comment: In Proceedings SLPCS 2016, arXiv: 1608. 0101...|$|R
40|$|It {{is widely}} {{believed}} that low-level languages with jumps must be difficult to reason about by being inherently non-modular. We have recently argued that this in untrue and proposed a novel method for developing <b>compositional</b> natural <b>semantics</b> and Hoare logics for low-level languages and demonstrated its viability on {{the example of a}} simple low-level language with expressions (Saabas & Uustalu 2005). The central idea is to use the implicit structure of finite disjoint unions present in low-level code as an (ambiguous) phrase structure. Here we apply our method to a stack-based language and develop it further. We define a <b>compositional</b> natural <b>semantics</b> and Hoare logic for this language and go then on to show that, in addition to Hoare logics, one can also derive compositional type systems as weaker specification languages with the same method. We describe type systems for stackerror freedom and secure information flow...|$|R
40|$|Herein, we give {{algebraic}} {{foundations for}} compositional model checking of Moore machines. We define composition of Moore machines and maximal closing environments as categorial constructions. Afterwards, we design a language to specify Moore machines (MSL) and define a <b>compositional</b> operational <b>semantics</b> for this language. We compare GNOME objects with MSL objects. Finally, we present techniques for checking safety properties of MSL specifications...|$|R
50|$|The {{combination}} of a <b>compositional</b> <b>semantics</b> with a syntax that mirrors such a semantics makes concatenative languages highly amenable to algebraic manipulation of programs; {{although it may be}} difficult to write mathematical expressions directly in them. Concatenative languages can be implemented in an efficient way with a stack machine, and are a common strategy to program virtual machines.|$|E
50|$|Lonclass {{is derived}} from the Universal Decimal Classification (UDC), itself a {{reworking}} of the earlier Dewey Decimal Classification (DDC). Lonclass dates from the 1960s, whereas UDC was created from DDC in the late 19th century. The BBC adaptation of UDC preserves the core features that distinguish UDC from DDC: an emphasis on a <b>compositional</b> <b>semantics</b> that allows new items to be expressed in terms of relationships between known items.|$|E
50|$|LOTH {{hinges on}} the belief that the mind works like a computer, always in {{computational}} processes. The theory believes that mental representation has both a combinatorial syntax and <b>compositional</b> <b>semantics.</b> The claim is that mental representations possess combinatorial syntax and compositional semanticâ€”that is, mental representations are sentences in a mental language. Alan Turing's work on physical machines implementation of causal processes that require formal procedures was modeled after these beliefs.|$|E
40|$|A {{scheme for}} syntax-directed truslation that mirrors <b>compositional</b> model-theoretic <b>semantics</b> is discussed. The scheme {{is the basis}} for an English {{translation}} system called PATR and was used to specify a semantically interesting fragment of English, including such constructs as tense, aspect, module, and various iexicafiy controlled verb complement structures. PATR was embedded in a question-answering system that replied appropriately to questions requiring the computation of logical entailments...|$|R
40|$|We {{show that}} the Kolmogorov {{extension}} theorem and the Doob martingale convergence theorem are two aspects of a common generalization, namely a colimit-like construction in a category of Radon spaces and reversible Markov kernels. The construction provides a <b>compositional</b> denotational <b>semantics</b> for standard iteration operators in programming languages, e. g. Kleene star or while loops, as a limit of finite approximants, {{even in the absence}} of a natural partial order...|$|R
40|$|International audienceWe {{view the}} vectors of a {{distributional}} semantic model as vectors of a semi-module over the semi-ring {{of the real}} interval I = [0, 1]. We show that the quantum logic of projectors is distributive and includes a Boolean sublattice. The <b>compositional</b> functional <b>semantics</b> of pregroup grammars interprets words and sentences as vectors such that the first order formula, the pregroup vector and the semantic vector interpreting a sentence are equivalent...|$|R
50|$|The deliberative agent {{requires}} symbolic representation with <b>compositional</b> <b>semantics</b> (e. g. data tree) in {{all major}} functions, for its deliberation {{is not limited}} to present facts, but construes hypotheses about possible future states and potentially also holds information about past (i.e. memory). These hypothetic states involve goals, plans, partial solutions, hypothetical states of the agent's beliefs, etc. It is evident, that deliberative process may become considerably complex and hardware killing.|$|E
5000|$|Semantic {{relations}} between words are of many kinds, for example homonymy, antonymy, meronymy, and paronymy. Semantics as specifically involved in lexicological work is called lexical semantics. Lexical semantics is somewhat {{different from the}} semantics of larger units such as phrases, sentences, and complete texts (or discourses), {{because it does not}} involve the same degree of <b>compositional</b> <b>semantics</b> complexities; however, the notion of [...] "word" [...] can be extremely complex, particularly in agglutinative languages.|$|E
50|$|Another {{appealing}} {{aspect of}} categorial grammars {{is that it}} is often easy to assign them a <b>compositional</b> <b>semantics,</b> by first assigning interpretation types to all the basic categories, and then associating all the derived categories with appropriate function types. The interpretation of any constituent is then simply the value of a function at an argument. With some modifications to handle intensionality and quantification, this approach can be used to cover a wide variety of semantic phenomena.|$|E
40|$|This paper {{develops}} a <b>compositional</b> vector-based <b>semantics</b> of relative pronouns within a categorical framework. Frobenius algebras {{are used to}} formalise the operations required to model the semantics of relative pronouns, including passing information between the relative clause and the modified noun phrase, as well as copying, combining, and discarding parts of the relative clause. We develop two instantiations of the abstract semantics, one based on a truth-theoretic approach and one based on corpus statistics. ...|$|R
40|$|A more {{expressive}} temporal semantics for Basic LOTOS, i. e. LOTOS without value passing, is presented. The semantics {{is given}} {{as an attempt}} to provide a <b>compositional</b> temporal <b>semantics</b> expressive with respect to maximal trace equivalence. It is however shown that, though it is actually more expressive than a similar semantics given in a previous report, it is not enough expressive. The capability of the given semantics to be used to verify interesting properties of programs is also investigated...|$|R
40|$|We {{present a}} new {{framework}} for <b>compositional</b> distributional <b>semantics</b> {{in which the}} distributional contexts of lexemes are {{expressed in terms of}} anchored packed dependency trees. We show that these structures have the potential to capture the full sentential contexts of a lexeme and provide a uniform basis for the composition of distributional knowledge in a way that captures both mutual disambiguation and generalization. Comment: To appear in Special issue of Computational Linguistics - Formal Distributional Semantic...|$|R
5000|$|Foundational {{considerations}} of game semantics {{have been more}} emphasised by Jaakko Hintikka and Gabriel Sandu, especially for Independence-friendly logic (IF logic, more recently Information-friendly logic), a logic with branching quantifiers. It was thought that the principle of compositionality fails for these logics, so that a Tarskian truth definition could not provide a suitable semantics. To get around this problem, the quantifiers were given a game-theoretic meaning. Specifically, the approach {{is the same as}} in classical propositional logic, except that the players do not always have perfect information about previous moves by the other player. Wilfrid Hodges has proposed a <b>compositional</b> <b>semantics</b> and proved it equivalent to game semantics for IF-logics.|$|E
5000|$|Three main {{approaches}} {{have been proposed}} for {{the definition of the}} semantics of IF logic. The first two, based respectively on games of imperfect information and on Skolemization, are mainly used in the definition of IF sentences only. The former generalizes a similar approach, for first-order logic, which was based instead on games of perfect information.The third approach, team semantics, is a <b>compositional</b> <b>semantics</b> in the spirit of Tarskian semantics. However, this semantics does not define what it means for a formula to be satisfied by an assignment (rather, by a set of assignments).The first two approaches were developed in earlier publications on if logic (...) the third one by Hodges in 1997 (...) [...]|$|E
50|$|Another {{controversial}} dichotomy is {{the question}} of whether human language is solely human or on a continuum with (admittedly far removed) animal communication systems. Studies in ethology have forced researchers to reassess many claims of uniquely human abilities for language and speech. For instance, Tecumseh Fitch has argued that the descended larynx is not unique to humans. Similarly, once held uniquely human traits such as formant perception, combinatorial phonology and <b>compositional</b> <b>semantics</b> are now thought to be shared with at least some nonhuman animal species. Conversely, Derek Bickerton and others argue that the advent of abstract words provided a mental basis for analyzing higher-order relations, and that any communication system that remotely resembles human language utterly relies on cognitive architecture that co-evolved alongside language.|$|E
40|$|Abstract Pair-sharing {{analysis}} of object-oriented programs determines those pairs of program variables bound at run-time to overlapping data structures. This information {{is useful for}} program parallelisation and analysis. We follow a similar construction for logic programming and formalise the property, or abstract domain, Sh of pair-sharing. We prove that Sh induces a Galois insertion w. r. t. the concrete domain of program states. We define a <b>compositional</b> abstract <b>semantics</b> for the static analysis over Sh, and prove it correct. ...|$|R
40|$|Concurrent {{constraint}} {{programming is}} classically based on asynchronous communication via a shared store. This paper presents {{new version of}} the ask and tell primitives which features synchronicity. Our approach is based on the idea of telling new information just in the case that a concurrently running process is asking for it. An operational and an algebraic semantics are defined. The algebraic semantics is proved to be sound and complete with respect to a <b>compositional</b> operational <b>semantics</b> which is also presented in the paper...|$|R
40|$|Starting from a <b>compositional</b> {{operational}} <b>semantics</b> {{of transition}} P Systems we have previously defined, {{we face the}} problem of developing an axiomatization that is sound and complete with respect to some behavioural equivalence. To achieve this goal, we propose to transform the systems into a normal form with an equivalent semantics. As a first step, we introduce axioms which allow the transformation of membrane structures into flat membranes. We leave as future work the further step {{that leads to the}} wanted normal form...|$|R
50|$|Graph-theoretic methods, {{in various}} forms, have proven {{particularly}} useful in linguistics, since natural language often lends itself well to discrete structure. Traditionally, syntax and <b>compositional</b> <b>semantics</b> follow tree-based structures, whose expressive power {{lies in the}} principle of compositionality, modeled in a hierarchical graph. More contemporary approaches such as head-driven phrase structure grammar model the syntax of natural language using typed feature structures, which are directed acyclic graphs. Within lexical semantics, especially as applied to computers, modeling word meaning is easier when a given word is {{understood in terms of}} related words; semantic networks are therefore important in computational linguistics. Still other methods in phonology (e.g. optimality theory, which uses lattice graphs) and morphology (e.g. finite-state morphology, using finite-state transducers) are common in the analysis of language as a graph. Indeed, the usefulness of this area of mathematics to linguistics has borne organizations such as TextGraphs, as well as various 'Net' projects, such as WordNet, VerbNet, and others.|$|E
5000|$|A formal grammar {{is a set}} {{of rules}} for {{rewriting}} strings, along with a [...] "start symbol" [...] from which rewriting starts. Therefore, a grammar is usually thought of as a language generator. However, it can also sometimes be used as the basis for a [...] "recognizer" [...] - a function in computing that determines whether a given string belongs to the language or is grammatically incorrect. To describe such recognizers, formal language theory uses separate formalisms, known as automata theory. One of the interesting results of automata theory {{is that it is not}} possible to design a recognizer for certain formal languages.Parsing is the process of recognizing an utterance (a string in natural languages) by breaking it down to a set of symbols and analyzing each one against the grammar of the language. Most languages have the meanings of their utterances structured according to their syntax - a practice known as <b>compositional</b> <b>semantics.</b> As a result, the first step to describing the meaning of an utterance in language is to break it down part by part and look at its analyzed form (known as its parse tree in computer science, and as its deep structure in generative grammar).|$|E
5000|$|Neale is an intentionalist and a pragmatist {{about the}} {{interpretation}} of speech and writing, and to this extent his work is rooted firmly in the Gricean tradition. While probably a Quinean in his attitude towards indeterminacy {{in the realm of}} meaning, Neale is a Chomskyan and a Fodorian in his stance on what they say (for example, when they use incomplete definite descriptions); (6) inappropriate reliance on formal notions of context deriving from indexical logics, (7) unwarranted faith in transcendent notions of [...] "what is said", [...] "what is implied" [...] and [...] "what is referred to"; and metaphysics, theory of legal interpretation, and literary theory. Philosophical problems about interpretation, context, information content, structure, and representation form the nexus of Neale's work. He has vigorously defended Russell's Theory of Descriptions, descriptive theories of anaphora, Paul Grice's intention-based theory of meaning, and a general approach to meaning and interpretation he calls [...] "linguistic pragmatism". His most influential work to date has been on the underdetermination and indeterminacy associated with uses of so-called attitude towards syntax and mental representation. Aspects of syntactic theory (8) a quite general overestimation of the role traditional <b>compositional</b> <b>semantics</b> can play in explanations of how humans use language to represent the world and communicate.|$|E
40|$|Financial and {{insurance}} contracts do {{not sound like}} promising territory for functional programming and formal semantics, but in fact we have discovered that insights from programming languages bear directly on the complex subject of describing and valuing a large class of contracts. We introduce a combinator library {{that allows us to}} describe such contracts precisely, and a <b>compositional</b> denotational <b>semantics</b> that says what such contracts are worth. We sketch an implementation of our combinator library in Haskell. Interestingly, lazy evaluation plays a crucial role...|$|R
40|$|We {{present a}} model for <b>compositional</b> {{distributional}} <b>semantics</b> related to the framework of Coecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors. We introduce a new learning method for tensors, generalising the approach of Baroni and Zamparelli (2010). We evaluate it on two benchmark data sets, and find it to outperform existing leading methods. We argue in our analysis {{that the nature of}} this learning method also renders it suitable for solving more subtle problems compositional distributional models might face. ...|$|R
40|$|We {{present a}} factorized <b>compositional</b> {{distributional}} <b>semantics</b> {{model for the}} representation of transitive verb constructions. Our model first produces (subject, verb) and (verb, object) vector representations based on the similarity of the nouns in the construction {{to each of the}} nouns in the vocabulary and the tendency of these nouns to take the subject and object roles of the verb. These vectors are then combined into a final (subject,verb,object) representation through simple vector operations. On two established tasks for the transitive verb construction our model outperforms recent previous work...|$|R
