0|32|Public
2500|$|In 1987, {{the former}} Panama Canal (Zone) Council was {{consolidated}} {{and made a}} part of the Direct Service Council, in a similar way that other Councils were consolidated or merged to form larger local Councils {{in other areas of the}} world. An [...] "official" [...] 12th <b>CSP</b> <b>issued</b> by the former Council for its youth to wear featured the words [...] "Direct Service" [...] in addition to the words [...] "Canal Zone." [...] While not officially created by the BSA, the patch was worn by DSC youth and adults living in the Zone until the middle 90s.|$|R
50|$|All <b>CSP</b> troopers are <b>issued</b> an ASP baton and OC, with Tasers started {{being used}} in 2011.|$|R
5000|$|After {{the removal}} of trust in DigiNotar, there are now four Certification Service Providers (<b>CSP)</b> that can <b>issue</b> {{certificates}} under the PKIoverheid hierarchy: ...|$|R
40|$|Micro {{manufacturing}} {{is steadily}} advancing from research labs {{to spin off}} companies. Products formerly manufactured by thin film technology using semiconductor processes are being replaced by more efficient full 3 D micro manufacturing techniques. Materials have expanded to polymers, metals and ceramics. From being wafer based structures of tens of microns in width and a thickness of up to a micron, feature sizes are now a few microns wide and hundreds of microns deep. This puts very high demands on the geometrical dimensional and roughness measurement tools of the future. This paper gives a review of current metrology tools available for dimensional and surface characterization, and their limitations. It will also present the requirements of future instruments and discuss potential techniques to solve these <b>issues.</b> <b>QC</b> 201202174 M Multi Material Micro Manufacture Network of Excellenc...|$|R
40|$|Quantum Computing (QC) {{research}} {{has gained a}} lot of momentum recently due to several theoretical analyses that indicate that QC is significantly more efficient at solving certain classes of problems than classical computing. While experimental validation will ultimately be required, the primitive nature of current QC hardware leaves practical testing limited to trivial examples. Thus, a robust simulator is needed to study complex <b>QC</b> <b>issues.</b> Most <b>QC</b> simulators model ideal operations, and thus cannot predict the actual time required to execute an algorithm or quantify the effects of errors in the calculation. We have developed a novel QC simulator that models physical hardware implementations. This simulator not only allows the accurate simulation of quantum algorithms on various hardware implementations, but also takes an important step towards providing a framework to determine their true performance and vulnerability to errors. ...|$|R
40|$|Advances in {{information}} technology produce large sets of data for decision makers. In both military and civilian efforts to achieve decision superiority, decision makers have to act agilely with proper, adequate and relevant information available. Information fusion is a process aimed to support decision makers’ situation awareness. This involves a process of combining data and information from disparate sources with prior information or knowledge to obtain an improved state estimate about an agent or other relevant phenomena. The important issue in decision making is not only assessing the current situation but also envisioning how a situation may evolve. In this work {{we focus on the}} prediction part of decision making called predictive situation awareness. We introduce new methodology where simulations and plan recognition are tools for achieving improved predictive situation awareness. Plan recognition is the term given to the process of inferring an agent’s intentions from a set of actions and is intended to support decision making. Beside its main task that is to support decision makers’ predictive situation awareness, plan recognition could also be used for coordination of actions and for developing computer-game agents that possess cognitive ability to recognize other agents’ behaviour. Successful plan recognition is heavily dependent on the data that is supplied. Therefore we introduce a bridge between plan recognition and sensor management where results of our plan recognition are reused to the control of, to give focus of attention to, the sensors that are expected to acquire the most important/relevant information. Our methodologies include knowledge representation, embedded stochastic simulations, microeconomics, imprecise knowledge and statistical inference <b>issues.</b> <b>QC</b> 2010092...|$|R
40|$|Bisimulation and its {{asymmetric}} variant (simulation) {{are widely}} used in CCS to compare the behaviour of processes. In <b>CSP,</b> correctness <b>issue</b> is addressed by introducing an ordering relation between an implementation and a specification. This report presents a new operational semantics for CSP, where two closure transitions are added to give a calculus in which simulation and refinement are identical. He Jifeng is a senior research-fellow of UNU/IIST, He is also a professor of computer science at East China Normal University and Shanghai Jiao Tong University. His research interests include the mathematical theory of programming and refined methods, the design techniques for the mixed software/hardware systems. Email: hjf@iist. unu. edu Copyright c fl 2003 by UNU/IIST, He Jifeng Contents i Contents...|$|R
50|$|He has {{performed}} solo and in various groupings throughout North America, Europe, United Kingdom, and Japan. His solo concerts {{have been at}} places such as New Museum (New York), Arnolfini (Bristol, UK), Musée des Beaux-Arts de Montréal, (Montréal, <b>QC),</b> <b>Issue</b> Project Room (New York), Guggenheim (New York), and Cafe OTO (London, UK). His solo and collaborative recordings have been released by labels such as Table of the Elements, Polyvinyl Records, Type Recordings, Jagjaguwar, Hometapes, Important Records, Taiga Records, and many others.|$|R
40|$|In {{this paper}} we compare and {{contrast}} two alternative semantics {{as a means of}} combining CSP with Object-Z. The purpose of this combination is to more effectively specify complex, concurrent systems: while CSP is ideal for modelling systems of concurrent processes, Object-Z is more suitable for modelling the data structures often needed to model the processes themselves. The first semantics, the finite trace model, is compatible with the standard CSP semantics but does not allow all forms of unbounded nondeterminism to be modelled (i. e. where a choice is made from an infinite set of options). The second semantics, the infinite trace model, overcomes this limitation but is no longer compatible with the standard <b>CSP</b> semantics. <b>Issues</b> involving specification, refinement and modelling fairness are discussed. Keywords CSP, Object-Z, concurrent systems, combining FDTs, semantics, refinement 1 INTRODUCTION CSP [15] is a process algebra developed for the formal specification of concurrent [...] ...|$|R
40|$|Structural Health Monitoring (SHM) is {{a helpful}} tool for {{engineers}} {{in order to}} control and verify the structural behaviour. SHM also guides the engineers and owners of structures in decision making concerning the maintenance, economy and safety of structures. Sweden has not a very sever tradition in monitoring, as countries with strong seismic and/or aerodynamic activities. Anyway, several large scale monitoring projects have taken place in recent years and SHM is slowly making entrance as an essential implement in managing structures by engineers as well as owners. This licentiate thesis presents a state-of-the art-review of health monitoring activities and over sensory technologies for monitoring infrastructure constructions like bridges, dams, off-shore platforms, historical monuments etc. related to civil engineering. The fibre optic equipment is presented with special consideration. The permanent monitoring system of the New Årsta Bridge consists of 40 fibre optic sensors, 20 strain transducers, 9 thermocouples, 6 accelerometers and one LVDT. The aims of the static study are: to control the maximal strains and stresses; to detect cracking in the structure; to report strain changes under construction, testing period and in the coming 10 years; and to compare conventional system with fibre optic system. The system installation started in January 2003 and was completed October 2003. The measurements took place from the very beginning and are suppose to continue for at least 10 years of operation. At the construction phase the measurements were performed manually and later on automatically through broad band connection between the office and central data acquisition systems located inside the bridge. The monitoring project of the New Årsta Railway Bridge is described from the construction phase to the testing phase of the finished bridge. Results of the recorded statistical data, crack detection and loading test are presented and a comparison between traditional techniques like strain transducers and fibre optic sensors is done. Various subjects around monitoring and sensor technologies that were found under the project are brought up in order to give the reader a good understanding, as well of the topics, techniques and of the bridge. Example of few applications is given with the aim of a deeper insight into monitoring related <b>issues.</b> <b>QC</b> 2010111...|$|R
40|$|This paper overviews {{the basic}} {{principles}} and recent advances in the emerging field of Quantum Computation (QC), highlighting its potential application to Artificial Intelligence (AI). The paper provides a very brief introduction to basic <b>QC</b> <b>issues</b> like quantum registers, quantum gates and quantum algorithms and then it presents references, ideas and research guidelines on how QC {{can be used to}} deal with some basic AI problems, such as search and pattern matching, as soon as quantum computers become widely available. Comment: 9 pages. Presented at PCI- 2007 : 11 th Panhellenic Conference in Informatics, 18 - 20 May 2007, Patras, Greec...|$|R
40|$|Due {{to growing}} {{concerns}} regarding global energy security and environmental sustainability {{it is becoming}} increasingly important to increase the energy efficiency of the transport sector. The internal combustion engine will probably continue to be the main propulsion system for road transportation for many years to come. Hence, much effort must be put in reducing the fuel consumption of the internal combustion engine to prolong a future decline in fossil fuel production and to reduce greenhouse gas emissions. Turbocharging and variable valve actuation applied to any engine has shown great benefits to engine efficiency and performance. However, using a turbocharger on an engine gives some drawbacks. In an attempt to solve some of these issues and increase engine efficiency further this thesis deals with the investigation of a novel gas exchange concept called divided exhaust period (DEP). The core idea of the DEP concept is to utilize variable valve timing technology on the exhaust side in combination with turbocharging. The principle of the concept is to let the initial high energy blow-down pulse feed the turbocharger, but bypass the turbine during {{the latter part of the}} exhaust stroke when back pressure dominates the pumping work. The exhaust flow from the cylinder is divided between two exhaust manifolds of which one is connected to the turbine, and one bypasses the turbine. The flow split between the manifolds is controlled with a variable valve train system. The DEP concept has been studied through simulations on three heavy-duty diesel engines; one without exhaust gas recirculation (EGR), one with short route EGR and one with long route EGR. Simulations show a potential improvement to pumping work, due to reduced backpressure, with increased overall engine efficiency as a result. Although, the efficiency improvement is highly dependent on exhaust valve size and configuration due to issues with choked flow in the exhaust valves. The EGR system of choice also proves to have a high impact on the working principle of the DEP application. Furthermore, the DEP concept allows better control of the boost pressure and allows the turbine to operate at higher efficiency across the whole load and speed range. The option of discarding both wastegate and variable geometry turbine is apparent, and there is little need for a twin-entry type turbine since pulse interference between cylinders is less of an <b>issue.</b> <b>QC</b> 20130108 </p...|$|R
40|$|By {{concentrated}} {{solar radiation}} thermal energy {{can be obtained}} {{to be used for}} power generation or for thermal process engineering to produce e. g. solar fuels. There are manifold materials issues comprising specific functionalities on the one hand (e. g. high reflectivity for mirrors, high absorptivity for solar receivers, or suitable reactivity for thermochemical heat storage or thermochemical fuel production). On the other hand, long-term stability of CSP materials under cyclic thermal load and harsh environmental conditions is crucial for future <b>CSP</b> applications. Material <b>issues,</b> such as degradation of SiC Absorbers under desert conditions and cyclic redox reactions employed for future solar thermochemical water splitting or chemical heat storages are discussed in detail...|$|R
40|$|For {{many years}} chiral {{effective}} theory (ChEFT) has enabled and supported lattice QCD calculations of hadron observables by allowing systematic effects from unphysical lattice parameters to be controlled. In {{the modern era}} of precision lattice simulations approaching the physical point, ChEFT techniques remain valuable tools. In this review we discuss the modern uses of ChEFT applied to lattice studies of hadron structure {{in the context of}} recent determinations of important and topical quantities. We consider muon g- 2, strangeness in the nucleon, the proton radius, nucleon polarizabilities, and sigma terms relevant to the prediction of dark-matter-hadron interaction cross-sections, among others. Comment: Journal of Physics G: Nuclear and Particle Physics focus <b>issue</b> on Lattice <b>QC...</b>|$|R
40|$|This chapter overviews {{the quality}} control (<b>QC)</b> <b>issues</b> for SNP-based {{genotyping}} methods used in genome-wide association studies. The main metrics {{for evaluating the}} quality of the genotypes are discussed followed by a worked out example of QC pipeline starting with raw data and finishing with a fully filtered dataset ready for downstream analysis. The emphasis is on automation of data storage, filtering, and manipulation to ensure data integrity throughput the process and on how to extract a global summary from these high dimensional datasets to allow better-informed downstream analytical decisions. All examples will be run using the R statistical programming language followed by a practical example using a fully automated QC pipeline for the Illumina platform...|$|R
40|$|Content Security Policy (CSP) is an {{emerging}} W 3 C standard introduced {{to mitigate the}} impact of content injection vulnerabilities on websites. We perform a systematic, largescale analysis of four key aspects that impact {{on the effectiveness of}} CSP: browser support, website adoption, correct configuration and constant maintenance. While browser support is largely satisfactory, with the exception of few notable issues, our analysis unveils several shortcomings relative to the other three aspects. CSP appears to have a rather limited deployment as yet and, more crucially, existing policies exhibit a number of weaknesses and misconfiguration errors. Moreover, content security policies are not regularly updated to ban insecure practices and remove unintended security violations. We argue that many of these problems can be fixed by better exploiting the monitoring facilities of <b>CSP,</b> while other <b>issues</b> deserve additional research, being more rooted into the CSP design...|$|R
40|$|Cloud {{computing}} is {{a typical}} example of distributed computing and emerged as a new paradigm that moves computing and data away from desktop and portable PCs into large data centers. Cloud services have three broad categories based on the fundamental nature of the cloud-based solution they provide: infrastructure-as-a-service (IaaS), platform-as-aservice (PaaS), or software-as-aservice (SaaS). In this work we {{have focused on the}} SaaS services provided by the Cloud service providers (<b>CSP).</b> The <b>issue</b> with SaaS is data security and confidentiality that makes the cloud user reluctant towards the cloud services. Data confidentiality can be achieved by encrypted outsourced content before outsourcing to cloud servers. But due to excessive computation of existing cryptographic algorithms and distributed nature of cloud computing, there is a need of a light weight cryptographic technique that has less computational overhead and high throughput. In this paper a light weight encryption technique is proposed which has less computational time and overall good performance. In order to prove it, the proposed algorithm is compared with the existing encryption techniques and results are analyzed. The key distribution of the shared key and secret key between the two group members is also handled efficiently using the same algorithm. Enormous overhead due to the large key size has been effectively ruled out in this paper. Light weight nature of proposed algorithm is well suited for distributed nature of cloud servers for an efficient processing with greatly enhanced user‘s confidence in cloud computing...|$|R
40|$|Motivation: Bisulfite {{sequencing}} (BS-seq) {{has emerged}} as the gold standard to study genome-wide DNA methylation at single-nucleotide resolution. Quality Control (QC) is a critical step in the analysis pipeline to ensure that BS-seq data are of high quality and suitable for subsequent analysis. Although several QC tools are available for next generation sequencing (NGS) data, most of them were not designed to handle <b>QC</b> <b>issues</b> specific to BS-seq protocols. Therefore, there is a strong need for a dedicated QC tool to evaluate and remove potential technical biases in BS-seq experiments. Results: We developed a package named BSeQC to comprehensively evaluate the quality of BS-seq experiments and automatically trim nucleotides with potential technical biases that may result in inaccurate methylation estimation. BSeQC takes standard SAM/BAM files as input and generates bias-free SAM/BAM files for downstream analysis. Evaluation based on real BS-seq data indicates that the use of the bias-free SAM/BAM file substantially improves the quantification of methylation level. Availability and implementation: BSeQC is freely available at...|$|R
40|$|Railway track {{stiffness}} (vertical track load {{divided by}} track deflection) {{is a basic}} parameter oftrack design which influences the bearing capacity, the dynamic behaviour of passing vehiclesand, in particular, track geometry quality {{and the life of}} track components. Track stiffness is abroad topic and in this thesis some aspects are treated comprehensively. In the introductionpart of the thesis, track stiffness and track stiffness measurements are put in their propercontext of track maintenance and condition assessment. The first aspect is measurement of track stiffness. During the course of this project, Banverkethas developed a new device for measurement of dynamic track stiffness called RSMV(Rolling Stiffness Measurement Vehicle). The RSMV is capable of exciting the trackdynamically through two oscillating masses above one wheelset. The dynamic stiffness is acomplex-valued quantity where magnitude is the direct relation between applied load anddeflection (kN/mm) and phase is a measure of deflection-delay by comparison with force. Thephase has partial relationship with damping properties and ground vibration. The RSMVrepeatability is convincing and both overall measurements at higher speeds (up to 50 km/h) and detailed investigations (below 10 km/h) can be performed. The measurement systemdevelopment is described in Paper A and B. The second aspect is evaluation of track stiffness measurements along the track from a trackengineering perspective. Actual values of stiffness as well as variations along the track areimportant, but cannot always answer maintenance and design related questions alone. InPaper D track stiffness is studied in combination with measurements of track geometryquality (longitudinal level) and ground penetrating radar (GPR). The different measurementsare complementary and a more reliable condition assessment is possible by the combinedanalysis. The relation between soft soils and dynamic track stiffness measurements is studiedin Paper C. Soft soils are easily found and quantified by stiffness measurements, in particularif the soft layer is in {{the upper part of the}} substructure. There are also possibilities to directlyrelate substructure properties to track stiffness measurements. Environmental vibrations areoften related to soft soils and partly covered in Paper C. One explanation of the excitationmechanism of train induced environmental vibrations is short waved irregular supportconditions. This is described in Paper E, where track stiffness was evinced to have normalvariations of 2 – 10 % between adjacent sleepers and variations up to 30 % were found. Anindicative way of finding irregular support conditions is by means of filtering longitudinallevel, which is also described in the paper. Train-track interaction simulation is used in PaperH to study track stiffness influence on track performance. Various parameters of trackperformance are considered, e. g. rail sectional moment, rail displacement, forces at wheel-railinterface and on sleepers, and vehicle accelerations. Determining optimal track stiffness froman engineering perspective is an important task as it impacts all listed parameters. The third aspect, efficient maintenance, is only partially covered. As track stiffness relates toother condition data when studied from a maintenance perspective, vertical geometricaldefects (longitudinal level and corrugation/roughness) are studied in paper F. The generalmagnitude dependency of wavelength is revealed and ways of handling this in conditionassessment are proposed. Also a methodology for automated analysis of a large set ofcondition data is proposed in Paper G. A case study where dynamic track stiffness,longitudinal level and ground penetrating radar are considered manifests the importance oftrack stiffness measurements, particularly for soil/embankment related <b>issues.</b> <b>QC</b> 2010062...|$|R
40|$|Thesis (Ph. D.) [...] University of Washington, 1982 Interprocess {{communication}} via {{shared memory}} has received considerable {{attention in the}} past. More recently, {{there has been a}} growing interest in communication in distribution environments. This dissertation examines distributed communication and attempts to integrate it with shared memory communication. A kernel is presented which provides simple tools to facilitate communication in these environments, and allows definition of new communication mechanisms. The kernel consists of features for synchronization, data transfer and locking. By combining the synchronization and data transfer facilities, distributed communication may be modelled. Shared memory communication normally requires synchronization and locking. Some applications require both shared memory and distributed communication. Such "hybrid" applications typically use the kernel features for synchronization, data transfer and locking. The kernel operations, though somewhat low level, provide flexibility in designing efficient mechanisms well suited for specific applications. Several examples illustrate the use of the kernel in programming solutions to a variety of communication problems, as well as in modelling some programming language mechanisms (including Ada and CSP). Several situations are identified which involve both distributed and shared communication. It is argued that in programming such hybrid problems, a unified approach to the different communication paradigms should be used, while permitting the programmer {{to take advantage of the}} benefits offered by the underlying physical architectures. Such an approach is demonstrated through examples using the kernel. The kernel has been implemented using the UNIX operating system. This implementation has been used to program a number of problem solutions, as well as in the construction of a compiler for the <b>CSP</b> language. <b>Issues</b> pertaining to a network implementation of the kernel are addressed. Measurements obtained from the implemented kernel have been used in a performance comparison of distributed and shared memory programs...|$|R
40|$|Abstract Background Gene {{expression}} profiling using microarrays {{has become}} an important genetic tool. Spotted arrays prepared in academic labs {{have the advantage of}} low cost and high design and content flexibility, but are often limited by their susceptibility to quality control (<b>QC)</b> <b>issues.</b> Previously, we have reported a novel 3 -color microarray technology that enabled array fabrication QC. In this report we further investigated its advantage in spot-level data QC. Results We found that inadequate amount of bound probes available for hybridization led to significant, gene-specific compression in ratio measurements, increased data variability, and printing pin dependent heterogeneities. The impact of such problems can be captured through the definition of quality scores, and efficiently controlled through quality-dependent filtering and normalization. We compared gene expression measurements derived using our data processing pipeline with the known input ratios of spiked in control clones, and with the measurements by quantitative real time RT-PCR. In each case, highly linear relationships (R 2 > 0. 94) were observed, with modest compression in the microarray measurements (correction factor Conclusion Our microarray analytical and technical advancements enabled a better dissection of the sources of data variability and hence a more efficient QC. With that highly accurate gene expression measurements can be achieved using the cDNA microarray technology. </p...|$|R
40|$|Developing complex {{engineering}} systems requires theconsolidation {{of models}} {{from a variety}} of domains such aseconomics, mechanics and software engineering. These modelsare typically created using differing formalisms and bystakeholders that have varying views on the same problemstatement. The challenging question is: what is needed to makesure that all of these different models remain consistent duringthe design process? A review of the related literature revealsthat this is still an open challenge and has not yet beeninvestigated at a fundamental level within the context ofModel-Based Systems Engineering (MBSE). Therefore, thispaper specifically focuses on examining the fundamentals ofconsistency management. We show that some inconsistenciescannot be detected and {{come to the conclusion that}} it isimpossible to say whether or not a system is fully consistent. Inthis paper, we first introduce a mathematical foundation todefine consistency in a formal manner. A decision-basedapproach to design is then studied and applied to thedevelopment of a real-world example. The research revealsseveral distinct types of inconsistencies that can occur duringthe design and development of a system. We show that theseinconsistencies can be further classified into two groups:internal and external consistency. From these insights, theontology of inconsistencies is constructed. Finally,requirements for possible tool support and methods to identifyand manage specific types of consistency <b>issues</b> are proposed. <b>QC</b> 20120109 </p...|$|R
40|$|ObjectiveDiscussing {{end-of-life}} (EOL) care {{is challenging}} when {{death is not}} imminent, contributing to poor decision-making and EOL quality-of-life. A communication support program (<b>CSP)</b> targeting these <b>issues</b> may facilitate discussions. We aimed to qualitatively explore responses to a nurse-led CSP, incorporating a question prompt list (QPLbooklet of questions patients/caregivers can ask clinicians), promoting life expectancy and EOL-care discussions. MethodsParticipants met a nurse-facilitator to explore an EOL-focussed QPL. Prognosis and advance care planning (ACP) QPL content was highlighted. Thirty-one transcribed meetings were analysed using thematic text analysis before reaching data saturation. ResultsThirty-one advanced cancer patients (life expectancy < 12 months) and 11 family caregivers were recruited from six medical oncology clinics in Sydney, Australia. Intent to use the QPL related to information needs, involvement in care and readiness to discuss EOL issues. Many participants did not want life expectancy estimates, citing unreliable estimates, unknown treatment outcomes, or coping by not looking ahead. Most displayed interest in ACP, often motivated by a loved one's EOL experiences, clear treatment preferences, concerns about caregivers or recognition that ACP is valuable regardless of life expectancy. Timing emerged as a reason not to discuss EOL issues; many maintaining it was too early. ConclusionPatients and caregivers appear ambivalent about acknowledging approaching death by discussing life expectancy but value ACP. Given heterogeneity in responses, individualised approaches are required to guide EOL discussion conduct and content. Further exploration {{of the role of}} prognostic discussion in ACP is warranted. Copyright (c) 2014 John Wiley & Sons, Lt...|$|R
40|$|Federal {{quality control}} (QC) {{programs}} for the measurement of payment allocation errors in federally funded Family Assistance Programs {{have been at the}} center of a hot controversy. In 1986, Congress directed the Department of Health and Human Services to contract with the National Academy of Sciences to study these QC programs. The Academy’s Panel on Quality Control of Family Assistance Programs has recently completed its work on the study. Existing federal QC methodology employs a two-phase sampling scheme, giving rise to a regression estimator of the allotment error rate. A key <b>issue</b> in the <b>QC</b> debate is the appropriateness of the regression estimator in view of marked departure of the QC data from the standard model-based conditions for linear regression-linearity, constancy of variance, and normality of error. In this study, we employ bootstrap resampling methods to investigate the effects of these departures on the efficiency of the regresscon estimator of the error rate. The analysis is based upon actual QC data from the Oregon Food Stamp Program. The origin of quality control procedures in federal Family Assistance<A Programs (FAPs) may be traced back to the Aid to Families with Dependent Children (AFDC) program in the early 1970 s. Today, three FAPs- food stamps, Medicaid, and AFDC- are all subject to quality control procedures that are descendent from the early AFDC program. Regardless of the initial intent of these quality control programs, and possibly contrary t...|$|R
40|$|This {{article has}} two aims. On the one hand, I want {{to clarify the}} use of {{architectural}} quality as a key concept used by architects. That is, I {{want to look at}} this notion from a professional perspective. The use of quality terms, the design, and the way architecture is judged all depend on one another. On the other hand, I examine how architectural quality is tested for and verified in practice. As a typical example, I look at how a jury in an architectural competition arrives at a decision on a winning entry. The jury has to identify the best solution for the task-a prize-winning design -and only one entry can be chosen. In the decision-making process, quality is strongly connected with values, and behind this thinking lays the assumption that good and bad solutions are manifest in the design. The assumption by the architects, again, is that a professional eye can detect quality in a design.   Seventeen professionals with first-hand competition experience were interviewed for this research. Among them were representatives of an architect's organization, the competition organizers, and the competitors. These interviewees were selected based on their professional expertise and skill in judging architectural competition entries.   From the interviews, we gain a good picture of how the concept of quality in design is understood in practice. The concept has different meanings, and appears to be ambiguous and even confusing. As is typical of practitioners, professional architects combine an aesthetic and artistic perspective with technical and practical points of view.  We see a genuine uncertainty prevailing in the field of architectural design: there is no single answer to the questions of architectural quality issues, but instead, in architectural and urban design proposals there are always several good solutions to be recognized.   In architectural competitions, the jury's task is to select the best proposal among the entries submitted [...] and to single out one winner. Disagreement over the final decision is seen as a failure for the jury. In case of divergence of opinion among the jury members, the competition might result in that the winning entry is not built. Competence and consensus are therefore two key factors in making the jury feel comfortable with the final selection of winners in architectural competitions. Consequently, the architects sitting in a jury must be professionals, with a good judgment in quality <b>issues.</b>   <b>QC</b> 20160621 </p...|$|R
40|$|In {{the last}} decade, {{diffusion}} MRI (dMRI) {{studies of the}} human and animal brain {{have been used to}} investigate a multitude of pathologies and drug-related effects in neuroscience research. Study after study identifies white matter (WM) degeneration as a crucial biomarker for all these diseases. The tool of choice for studying WM is dMRI. However, dMRI has inherently low signal-to-noise ratio and its acquisition requires a relatively long scan time; in fact, the high loads required occasionally stress scanner hardware past the point of physical failure. As a result, many types of artifacts implicate the quality of diffusion imagery. Using these complex scans containing artifacts without quality control (QC) can result in considerable error and bias in the subsequent analysis, negatively affecting the results of research studies using them. However, dMRI QC remains an under-recognized issue in the dMRI community as there are no user-friendly tools commonly available to comprehensively address the <b>issue</b> of dMRI <b>QC.</b> As a result, current dMRI studies often perform a poor job at dMRI QC. Thorough QC of diffusion MRI will reduce measurement noise and improve reproducibility, and sensitivity in neuroimaging studies; this will allow researchers to more fully exploit the power of the dMRI technique and will ultimately advance neuroscience. Therefore, in this manuscript, we present our open-source software, DTIPrep, as a unified, user friendly platform for thorough quality control of dMRI data. These include artifacts caused by eddy-currents, head motion, bed vibration and pulsation, venetian blind artifacts, as well as slice-wise and gradient-wise intensity inconsistencies. This paper summarizes a basic set of features of DTIPrep described earlier and focuses on newly added capabilities related to directional artifacts and bias analysis...|$|R
40|$|Solar {{resource}} forecasting is {{very important}} for the operation and management of solar power plants. Solar radiation is highly variable because it is driven mainly by synoptic and local weather patterns. This high variability presents challenges to meeting power production and demand curves, notably in the case of photovoltaic (PV) power plants, which have little or no storage capacity. For concentrating solar power (<b>CSP)</b> plants, variability <b>issues</b> are partially mitigated by the thermal inertia of the plant, including its heat transfer fluid, heat exchangers, turbines and, potentially, coupling with a heat storage facility; however, temporally and spatially varying irradiance introduces thermal stress in critical system components and plant management issues that can result in the degradation of the overall system’s performance and reduction of the plant’s lifetime. The variability can also result in lower plant efficiencies compared to operation in stable conditions because optimally operating the plant is more challenging. For PV power plants that have battery storage, forecasts are helpful to schedule the charging process of the batteries at the most appropriate time, optimize the fractions of electricity delivered and stored at any instant, and thus avoid the loss of usable energy. Solar radiation forecasting anticipates the solar radiation transients and the power production of solar energy systems, allowing for the setup of contingency mechanisms to mitigate any deviation from the required production. With the expected integration of large shares of solar power, reliable predictions of solar power production are becoming increasingly important as a basis for efficient management and operation strategies as well as for solar energy trading. Today, solar power prediction systems are an essential part of electric grid management in countries that have substantial shares of solar power generation, among which Germany is a paradigmatic case. For example, in 2016 Germany had an installed PV power capacity of more than 40 GWpeak, supplying more than 40...|$|R
40|$|Spin torque {{oscillator}} (STO) {{technology has}} a unique blend of features, including {{but not limited to}} octave tunability, GHz operating frequency, and nanoscaled size, which makes it highly suitable for microwave and radar applications. This thesis studies the fundamentals of STOs, utilizes the state-of-art STO's advantages, and proposes two STO-based microwave systems targeting its microwave applications and measurement setup, respectively. First, based on an investigation of possible STO applications, the magnetic tunnel junction (MTJ) STO shows a great suitability for microwave oscillator in multi-standard multi-band radios. Yet, it also imposes a large challenge due to its low output power, which limits it from being used as a microwave oscillator. In this regard, different power enhancement approaches are investigated to achieve an MTJ STO-based microwave oscillator. The only possible approach is to use a dedicated CMOS wideband amplifier to boost the output power of the MTJ STO. The dedicated wideband amplifier, containing a novel Balun-LNA, an amplification stage and an output buffer, is proposed, analyzed, implemented, measured and used to achieve the MTJ STO-based microwave oscillator. The proposed amplifier core consumes 25. 44 mW from a 1. 2 V power supply and occupies an area of 0. 16 mm 2 in a 65 nm CMOS process. The measurement results show a S 21 of 35 dB, maximum NF of 5 dB, bandwidth of 2 GHz - 7 GHz. This performance, as well as the measurement results of the proposed MTJ STO-based microwave oscillator, show that this microwave oscillator has a highly-tunable range and is able to drive a PLL. The second aspect of this thesis, firstly identifies the major difficulties in measuring the giant magnetoresistance (GMR) STO, and hence studying its dynamic properties. Thereafter, the system architecture of a reliable GMR STO measurement setup, which integrates the GMR STO with a dedicated CMOS high frequency IC to overcome these difficulties in precise characterization of GMR STOs, is proposed. An analysis of integration methods is given and the integration method based on wire bonding is evaluated and employed, as a first integration attempt of STO and CMOS technologies. Moreover, a dedicated high frequency CMOS IC, which is composed of a dedicated on-chip bias-tee, ESD diodes, input and output networks, and an amplification stage for amplifying the weak signal generated by the GMR STO, is proposed, analyzed, developed, implemented and measured. The proposed dedicated high frequency circuits for GMR STO consumes 14. 3 mW from a 1. 2 V power supply and takes a total area of 0. 329 mm 2 in a 65 nm CMOS process. The proposed on-chip bias-tee presents a maximum measured S 12 of - 20 dB and a current handling of about 25 mA. Additionally, the proposed dedicated IC gives a measured gain of 13 dB with a bandwidth of 12. 5 GHz - 14. 5 GHz. The first attempt to measure the (GMR STO+IC) pair presents no RF signal at the output. The possible cause and other identified <b>issues</b> are given. <b>QC</b> 20140114 </p...|$|R
40|$|Aim The UK Government’s recent {{public policy}} shift, {{encapsulated}} in the Sporting Future policy document and articulated through Sport England’s strategic response Toward an Active Nation, signals {{a fundamental shift}} in the approach to engaging more physically active lifestyles. It envisages such engagement as predicated on forging partnerships outside the traditional sporting community {{as a means of}} promoting behavioral change amongst those alienated by the mainstream sporting culture. As in any policy shift, the management of funding streams becomes a key tool in the pursuit of these new priorities. In response to this shift toward physical activity broadly defined, County Sports Partnerships (CSPs), as key local coordinators-providers, must re-imagine their mission and recalibrate their objectives. The study aims to enhance a critical understanding of this ongoing process. It considers CSP responses to the waxing and waning influence of key strategic partners and the emerging dichotomy between ‘sporting’ and ‘physical’ cultures. It suggests approaches to the management of these tensions and pinpoints subsequent research priorities required to better understand the emerging physical activity landscape. Background Strategies for increasing sport participation are characterized by the traditional dichotomy between sport narrowly defined as organized/structured and physical activity broadly defined as unstructured/recreational encompassing different forms of physical expression. The division is apparent within the institutional landscape, which shapes the delivery of sport and physical activity and subsequently constrains the development of integrated approaches. The fragmentation of organizational actors along with the continual change of local sport and physical activity priorities, inhibit the development of stable collaborations between agencies involved in sport and physical activity (Lindsey, 2009). In the case of the UK, the activities of multiple stakeholders operating locally against the backdrop of a rapidly changing policy and funding environment, generates additional complexity. For example, the delivery of sport services by Local Authorities raises issues around accountability, equity, service quality and sustainability (King, 2014). At the same time, the role of CSPs is construed in a number of contrasting ways by partner agencies, creating the potential for misunderstanding over the shifting priorities for sports development (Mackintosh, 2011). Grix and Phillpots (2011) note the paradox that while CSPs were established to facilitate the delivery of sport policy at regional/local level by responding and adapting to local conditions, this has resulted in a hierarchical mode of partnership that rests on resource dependency and asymmetrical network governance between the Government and stakeholders in the sport policy network. These issues create concerns about the effectiveness of the Government’s physical activity and sport participation strategy at the local level; concerns that are brought into sharp focus at a time of rapid change. Specifically, intersecting roles/responsibilities within the sport policy network may perpetuate tensions between affiliated entities manifest as distrust and fragmentation, or misalignment over the changing sports development objectives. Consequently, the nature of sport governance, as executed from national to local level, causes operational challenges for CSPs. Such network entities, while attempting to achieve similar objectives, have different strategic and operational plans that may not facilitate a whole-of-sport perspective in relation to the creation of shared understandings across a sporting network (O’Boyle & Shilbury, 2016). Indeed, duplication of roles/responsibilities between national sport governance and the CSP network is evident in cases where efforts have sought to develop policy approaches in areas like public health, volunteering and workforce. Methodology This is an exploratory case study based around the experiences of two CSPs in the South West of England and involving their key partner organizations. The research employs a qualitative approach; based around a series of semi-structured interviews with middle and senior managers operating across a number of stakeholder bodies. A standard protocol for qualitative content analysis is used to enable pattern-matching of emerging themes through a constant comparison method and thereby extract a thematic structure detailing the <b>issues</b> <b>CSPs</b> are facing. 108 Discussion and implications The study sheds light on developing responses to manage sport partnerships in time of changing policy priorities. The mandate of creating active lifestyles requires the exploration of the policy trajectory, appropriate mode of governance and local service delivery models. Conceptually, {{there is a need to}} address the dichotomy between sport and physical activity, perhaps synthesized as physical culture. This involves the rebranding of CSPs centered on physical activity, while re-thinking roles, responsibilities/parameters and partnership-building as shaped by the funding imperative and the subsequent partnership responses to the new sports/PA environment. Within the complex sport policy environment, we need to find the means for better connecting national sport-PA participation policy with local network entities and non-sporting sectors...|$|R
40|$|Modern {{information}} and communication technology (ICT), including internet, smart phones, cloud computing, global positioning system, e-commerce, e-Health, global communications and internet of things (IoT), all rely fundamentally - for identification, authentication, confidentiality and confidence - on cryptography. However, {{there is a high}} chance that most modern cryptography protocols will be annihilated upon the arrival of quantum computers. This necessitates taking steps for making the current ICT systems secure against quantum computers. The task is a huge and time-consuming task and there is a serious probability that quantum computers will arrive before it is complete. Hence, it is of utmost importance to understand the risk and start planning for the solution now. At this moment, there are two potential paths that lead to solution. One is the path of post-quantum cryptography: inventing classical cryptographic algorithms that are secure against quantum attacks. Although they are hoped to provide security against quantum attacks for most situations in practice, there is no mathematical proof to guarantee unconditional security (`unconditional security' is a technical term that means security is not dependent on a computational hardness assumption). This has driven many to choose the second path: quantum cryptography (QC). Quantum cryptography - utilizing the power of quantum mechanics - can guarantee unconditional security in theory. However, in practice, device behavior varies from the modeled behavior, leading to side-channels that can be exploited by an adversary to compromise security. Thus, practical QC systems need to be security evaluated - i. e., scrutinized and tested for possible vulnerabilities - before they are sold to customers or deployed in large scale. Unfortunately, this task has become more and more demanding as QC systems are being built in various style, variants and forms at different parts of the globe. Hence, standardization and certification of security evaluation methods are necessary. Also, a number of compatibility, connectivity and interoperability <b>issues</b> among the <b>QC</b> systems require standardization and certification which makes it an issue of highest priority. In this thesis, several areas of practical quantum communication systems were scrutinized and tested for the purpose of standardization and certification. At the source side, the calibration mechanism of the outgoing mean photon number - a critical parameter for security - was investigated. As a prototype, the pulse-energy-monitoring system (PEMS) implemented in a commercial quantum key distribution (QKD) machine was chosen and the design validity was tested. It was found that the security of PEMS was based on flawed design logic and conservative assumptions on Eve's ability. Our results pointed out the limitations of closed security standards developed inside a company and highlighted the need for developing - for security - open standards and testing methodologies in collaboration between research and industry. As my second project, I evaluated the security of the free space QKD receiver prototype designed for long-distance satellite communication. The existence of spatial-mode-efficiency-mismatch side-channel was experimentally verified and the attack feasibility was tested. The work identified a methodology for checking the spatial-mode-detector-efficiency mismatch in these types of receivers and showed a simple, implementable countermeasure to block this side-channel. Next, the feasibility of laser damage as a potential tool for eavesdropping was investigated. After testing on two different quantum communication systems, it was confirmed that laser damage has a high chance of compromising the security of a QC system. This work showed that a characterized and side-channel free system does not always mean secure; as side-channels can be created on demand. The result pointed out that the standardization and certification process must consider laser-damage related security critical issues and ensure that it is prevented. Finally, the security proof assumptions of the detector-device-independent QKD (ddiQKD) protocol - that restricted the ability of an eavesdropper - was scrutinized. By introducing several eavesdropping schemes, we showed that ddiQKD security cannot be based on post selected entanglement. Our results pointed out that testing the validity of assumptions are equally important as testing hardware for the standardization and certification process. Several other projects were undertaken including security evaluation of a QKD system against long wavelength Trojan-horse attack, certifying a countermeasure against a particular attack, analyzing the effects of finite-key-size and imperfect state preparation in a commercial QKD system, and experimental demonstration of quantum fingerprinting. All of these works are parts of an iterative process for standardization and certification that a new technology - in this case, quantum cryptography- must go through before being able to supersede the old technology - classical cryptography. I expect that after few more iterations like the ones outlined in this thesis, security of practical QC will advance to a state to be called unconditional and the technology will truly be able to win the trust to be deployed on large scale...|$|R
40|$|The 11 th RCM North Atlantic {{was held}} in Horta (Portugal) 22 - 26 September 2014. Due to the delayed {{introduction}} of the revised DCF the European Commission decided a roll-over in 2013 meaning Member States National Programmes 2011 - 2013 remains unchanged for the period 2014 - 2017. The limitations this decision brings for coordination of current MS national programmes have allowed RCM NA to focus in three major {{different aspects of the}} data collection where a better integration –as stated by article 4 Commission Decision 665 / 2008 — is currently needed. 1. Concurrent sampling One of the major changes in the DCF that came into force in 2009 was a shift towards concurrent sampling: a sampling strategy covering the sampling of all species during sampling operations. Via this strategy the DCF is able to facilitate the data demands of the existing stock-based assessments as well as serving the revised needs for the ecosystem approach to fishery management. The requirements for concurrent length sampling were developed in PGCCDBS 07. Implementation studies were done through the following years at national level and an ICES Workshop (2008) discussed about the common problems and the way for best implementation. However it seems concurrent sampling has been under discussion in some countries since then. STECF report (STECF, 12 - 07) noted “that concurrent sampling of different fish stocks in the same catch is carried out differently in different Member States leading to inconsistent estimates of catch compositions from sampling schemes. There is a need to explain and define concurrent sampling in order to ensure consistent sampling by MS. ” RCM NA analysed the current situation. Data collected is increasingly being used by groups to provide additional information, not available in the past under historic data collection methods. RCM NA detailed the ICES Working Groups that have benefited from the introduction of concurrent sampling allowing them to provide more robust advice. Moreover, there are a large number of stocks lacking quantitative assessments and reliable estimates of stock status. RCM NA specified recent studies indicating that simple harvest control rules using information on the catch length composition and length reference points can be used to deliver catch-based advice that is risk adverse (e. g. Geromont and Butterworth 2014, Jardim et al., 2014, ICES WKLIFE). Concurrent sampling may constitute an important source of biological data for many of the data-limited stocks and the application of these simple HCRs. And historical series are in fact very recent so more results from on-going work is expected. The benefits of concurrent sampling were also highlighted regarding species specific data in species that are often grouped together, with quality that can be verified given the experience and expertise of the data collectors. In the RCM NA it was evident that not all MS were carrying out sampling in this manner. The question as to whether this variability in sampling affects the quality and utility of the data collected needs to be investigated. 2. Regional coordination Optimizing and harmonizing fisheries management across MS is dependent on improving regional coordination. This coordination is expected to improve through the use of tools as the regional data bases where on-going work is being developed. RCM NA analysed {{that there is a need}} for harmonization of métiers at level 6. This work was being accomplished since the 2008 RCM NA and was somehow abandoned last years so the problem persists. Reviewing and collating fleet descriptions, metier definitions, standardising metier coding and merging national métiers into regional metiers are fundamental steps that has to be taken by MS. RDB is currently containing big amounts of data not useful for regional coordination. The 2014 RCM NA decided to produce a reference list containing all the possible combinations for métier naming. The reference list was compared with both, data uploaded into the RDB and list of métiers as provided in the MS National Programme (NP 2011 - 2013). The results of this comparison show the need to restrict the RDB uploads and métier lists provided in the NP accordingly to the reference list and following the métier naming standards. The current list of métiers uploaded to the RDB is incomplete and definitely contains incorrect métier codes. 3. Quality checks There has been considerable discussion, guidance and recommendations about improving and reporting quality in relation to the DCF at STECF, RCMs and at ICES expert groups. This is an ongoing and collective task where specific inputs are needed. The report of RCM NA provides extensive guidelines to the MS how to implement quality assurance procedures. RCM NA focused on the quality <b>issues</b> and recommended <b>QC</b> and QA procedures at the National data capture and data processing level - those stages where the responsibility for checking the data remains firmly in the hands of the MS. This formsa simple standard QA document which can also inform data users and evaluators of the minimum checks carried out by each MS prior to any data upload to the RDB. There was not sufficient time to review the results and these will need to be done at the next RCM. The document itself will need to be reviewed as to its efficacy, whether it may form part of a Regional QA document and how it may be kept up to date if it does. Between the other issues addressed by the RCM NA it is necessary to stress the landing obligation. This represents a fundamental shift in the management approach to EU fisheries. The RCM NA considered different topics related to this new situation and discussed how it might have an impact on data. The direction of some of these implications is also unclear until the implementation of the obligation has been defined and the practical implications on the ground can be addressed. First issue considered was the access to vessels for biological sampling and potential changes in behaviour of fishing vessels. Opinion of the RCM is that scientific observers should have no mandate for the control of fishing regulations. Previous observer programmes have indicated that changes in operational behaviour already occur when an observer is on board. It is suspected that this will increase with the introduction of the landing obligation. Secondly, changes in IT systems and protocols were addressed. The landing obligation will generate changes for the collection of sampling data. One of the major changes is that the catch will be split into three catch components. As already stated in the other RCMs on-board sampling protocols will have to be adjusted to account for the new defined components of the catch. National fisheries institutes must update and adapt their existing IT systems in order to include the new catch components. Furthermore, the regional data bases and consecutively FishFrame and InterCatch need to be prepared and the uploading processes and raising and estimation procedures adapted. The third issue was the quality of data compliance of the logbooks. The quality of the data depends both on the quality of the catch information and the quality of the biological sampling. Both elements will be affected by the landing obligation. Concern is expressed by the RCM on the future quality of the catch statistics. The RCM is of the opinion that the discard plans, to be implemented in the different regions, should contain clear proposals on how different components of the catch should be monitored and that logbooks and IT systems should be adapted in a timely manner to record the different catch components. Analysis of the data call for submission data to the RDB revealed huge work must be done in order to ensure correct data are available for regional coordination and/or expert groups. Most part of countries uploaded data (only Spain –not uploaded but available to the meeting- and France –similar situation- didn’t do it) but superficial analysis showed the data uploaded was inconsistent: large differences between MS, low number of species uploaded indicating that uploads from several countries are still incomplete, incorrect name of the fishing activities making impossible check again the metier descriptions compiled in the past, etc. It is not the task of the RCM NA to check every data upload, so it was clear a new data call should be established to ensure MS upload correct data. Nevertheless RCM NA see big improvements in the work MS are doing regarding these data calls coming from a situation where some countries didn’t provide the data to a new scenario where everyone is providing data and worries concern the quality, which is a large step forward. Other items on the agenda were the consideration of the follow up of relevant recommendations made last year by Liaison Meeting; consideration of the cost sharing proposal received from RCM NS&EA; evaluation of the ICES data quality transmission sheets and presentations on relevant developments from ICES, EC and SC-RD...|$|R

