24|121|Public
40|$|Modeling {{embedded}} real-time systems {{consisting of}} different components with UML-RT {{leads to a}} design model using various diagrams. Sequence diagrams describe possible interactions between system components and may be annotated with specific real-time constraints. Statechart diagrams are used for describing each component's behavior. In {{order to be able}} to get a consistent model, a <b>consistency</b> <b>concept</b> for different diagram types is needed that takes into account real-time constraints. In this paper, a <b>consistency</b> <b>concept</b> for sequence diagrams and statechart diagrams is presented which focuses on the establishment of timing constraints. Our <b>consistency</b> <b>concept</b> distinguishes between syntactical, semantic and real-time consistency and takes into account the influence of processor allocation and scheduling. Using the <b>consistency</b> <b>concept</b> we describe a method for ensuring the consistency based on worst case execution time analysis of statecharts and schedulability analysis of tasks, thereby enabling a precise answer of the question of consistency...|$|E
40|$|In {{this paper}} we {{investigate}} possible ways to define consistency of assessments in infinite signaling games, i. e. signaling {{games in which}} the sets of types, messages and answers are complete, separable metric spaces. Roughly speaking, a <b>consistency</b> <b>concept</b> is called appropriate if it implies Bayesian consistency and copies the original idea of consistency in finite extensive form games as introduced by Kreps and Wilson (Econometrica, 1982, 50, 863 - 894). We present a particular appropriate <b>consistency</b> <b>concept,</b> which we call strong consistency, and give a characterization f strongly consistent assessments, It turns out that all appropriate consistency concepts are refinements of strong consistency. Finally, we define and characterize structurally consistent assessments in infinite signaling games...|$|E
30|$|The <b>consistency</b> <b>concept</b> of the GTC {{is still}} more general than the concept of Kosow. Kosow [10] and the GTC share the {{interpretation}} that {{the perception of the}} consistency is a subjective matter. According to Kosow, scenario storylines rely on holistic consistency ‘filters’, as intuitive gut feelings, i.e., subjective consistency definitions. In the GTC, the perceived consistency depends on the subjective criteria of sameness of the actor.|$|E
40|$|Abstract. The <b>concept</b> {{of local}} <b>consistency</b> plays {{a central role}} in {{constraint}} satisfaction and has been extended to handle general constraint-based inference (CBI) problems. We propose a family of novel generalized local <b>consistency</b> <b>concepts</b> for the junction graph representation of CBI problems. These concepts are based on a general condition that depends only on the existence and property of the multiplicative absorbing element and does not depend on the other semiring properties of CBI problems. We present several local consistency enforcing algorithms and their approximation variants. Theoretical complexity analyses and empirical experimental results for the application of these algorithms to both MaxCSP and probability inference are given. We also discuss the relationship between these local <b>consistency</b> <b>concepts</b> and message passing schemes such as junction tree algorithms and loopy message propagation. ...|$|R
30|$|Amara {{considered}} {{that the second}} basic criterion of the possible image of future or scenario is its internal consistency. Kosow [10] has extended the <b>consistency</b> <b>concepts</b> to concern also consistency within a scenario sample (or scenario set), consistency between different forms of one scenario and consistency of underlying models of scenarios.|$|R
30|$|Beyond this {{apparent}} consensus, different <b>consistency</b> <b>concepts,</b> criteria {{and measures of}} consistency coexist. Van Asselt and colleagues [29] have shown that different understandings of consistency are circulating: Consistency means being in line with historical trends and developments when the “historic deterministic temporal repertoire” is used; or {{it refers to the}} internal consistency of scenarios, when the “futurist difference temporal repertoire” is taken over. In different scenario ‘schools’, diverse <b>consistency</b> <b>concepts</b> are applied: Mathematical models can be considered ‘objectively’ internally consistent by definition of their mathematical (causal) logics. 7 Storylines, however, rely on holistic consistency ‘filters’ as intuitive gut feelings, i.e., subjective consistency definitions [34]. More systematic, qualitative scenario approaches use the consistency principle to future variants with each other to form comprehensive pictures and to select scenario samples (e.g., the so-called consistency analysis, CA [35]). To this aim, different formal consistency algorithms and consistency scales have been developed. 8 The different consistency measures do apply different consistency criteria: CA is using the criterion of co-incidence or co-existence of factor developments. In contrast, CIB is using qualitative causal information considering the direction of influences between developments [36].|$|R
40|$|Abstract. Using {{the literal}} {{encoding}} of the satisfiability problem (SAT) as a bi-nary constraint satisfaction problem (CSP), we relate the path <b>consistency</b> <b>concept</b> and the row convexity of CSPs with the inference {{rules in the}} propositionallogic field. Then, we use this result to propose a measure characterizing satisfiable and unsatisfiable 3 -SAT instances. The correlation between the computational re-sults allows us to validate this measure...|$|E
40|$|Outlier {{screening}} is {{a popular}} approach for testing automotive products. In practice, developing an outlier model can be subjective, making justification of the model challenging. In this {{paper we propose a}} new concept called Consistency which provides a data-driven objective way to assess an outlier model. We study the development of outlier models in view of this new model <b>consistency</b> <b>concept</b> and report experimental findings on an automotive product line...|$|E
40|$|Abstract: John {{entered his}} office. John left his office. The order of these updates to a {{context-aware}} system {{is important to}} reflect {{the state in the}} real world. Context information obtained by sensor systems requires consistency concepts which reflect the chronological ordering in which context information has been captured. This paper introduces a <b>consistency</b> <b>concept</b> which allows to express the ordering of events which happened outside of a computer system. ...|$|E
40|$|The literal {{encoding}} of the satisfiability problem (SAT) as {{a binary}} constraint satisfaction problem (CSP) {{allows us to}} make a technical comparison between the propositional logic and the constraint network fields. Equivalencies or analogies between concepts and techniques used in the two fields are then shown. For instance it is shown that arc consistency is equivalent to unit resolution [7, 2]. This result is generalized here by relating the local <b>consistency</b> <b>concepts</b> with inference rules in the propositional logic. We relate the pure literal rule with an interesting property of constraint networks called "row convexity", and we propose an empirical characterization of satisfiable and unsatisfiable 3 -SAT instances...|$|R
5000|$|As a {{background}} of these confusions, the difficulties of maintaining the <b>consistencies</b> between <b>concepts</b> and terminologies during the rapid developments of technology, may be significant.And it is a reason why this slightly classical terminology [...] "Table-lookup synthesis" [...] is explained on here.|$|R
40|$|PURPOSE: This study {{aimed to}} {{evaluate}} the relevance and importance of two SF- 36 subscales, Vitality (VT) and Physical Function (PF), to assess concepts of energy and physical function in patients with type 2 diabetes mellitus (DM) and non-dialysis CKD-related anemia. METHODS: Patients with clinical history of DM and non-dialysis CKD-related anemia (n = 68) were identified as follows: 40 participated in concept elicitation (CE) interviews; 20 in cognitive interviews (CI), and 8 in pilot interviews. Relevance and importance ratings for SF- 36 VT and PF items were obtained. Interviews were recorded, transcribed, and patient expressions of concepts coded. Inter-rater agreement {{was used to evaluate}} coding <b>consistency.</b> <b>Concepts</b> elicited were mapped to SF- 36 VT and PF items. RESULTS: Patients (n = 64) were 65. 6...|$|R
40|$|In this paper, we {{show how}} {{different}} behavioral consis-tency concepts can be formed for sequence diagrams and statecharts. Our approach {{relies on the}} partial transla-tion of models into a semantic domain and on the defini-tion of explicit consistency conditions. Partial translations and consistency conditions can be combined to form an ex-plicit <b>consistency</b> <b>concept.</b> In order to make our approach applicable in practice, we discuss the issue of tool support and favor {{the development of a}} so-called consistency work-bench. 1...|$|E
40|$|AbstractIn {{this paper}} we discuss the <b>consistency</b> <b>concept</b> of Williams {{coherence}} for imprecise conditional previsions, presenting a variant of this notion, which we call W-coherence. It is shown that W-coherence ensures important consistency properties and is quite general and well-grounded. This is done comparing it with alternative or anyway similar known and less known consistency definitions. The common root of these concepts is that they variously extend to imprecision the subjective probability approach championed by de Finetti. The analysis in the paper is also helpful in better clarifying several little investigated aspects of these notions...|$|E
30|$|In this paper, we {{show that}} the nature of the {{solutions}} of the FFLS is completely different from the nature of the solutions of the LP and that it is insufficient to obtain all exact solutions for FFLS, using numerical examples. We employed the results from this survey as examples for LP methods in specific studies in [[32],[33]] to declare that the consistency or possible solution for FFLS is not similar to the known <b>consistency</b> <b>concept</b> in the linear system. For instance, the FFLS may yield two unique solutions or many infinite solutions despite that it is constructed by only one equation. Also, the number of solutions in non-square FFLS does not depend on the number of equations compared with the number of parameters.|$|E
40|$|Introduction and Motivation The {{intention}} of the present paper is to clarify that planning under certain circumstances is clearly learning. The necessary assumptions are, first, a well-specified problem domain of planning and, second, a formally well-based and algorithmically clear approach to plan generation 1. The main result {{will be a better}} understanding of the essentials of plan generation under the specified circumstances. Understanding the hypothetical character of generated plans has a serious impact on evaluating and using planning results properly. Furthermore, this approach allows for the development of a couple of novel planning algorithms as refinements of former solutions. As a side effect, the area of machine learning may benefit from exciting stimulations due to the peculiarities of the underlying planning domain. Among others, there is a refinement of former <b>consistency</b> <b>concepts</b> derived from those peculiariti...|$|R
40|$|This {{paper is}} an attempt to address the {{processing}} of nonlinear numerical constraints over the Reals by combining three different methods: local consistency techniques, symbolic rewriting and interval methods. To formalize this combination, we define a generic two-step constraint processing technique based on an extension of the Constraint Satisfaction Problem, called Extended Constraint Satisfaction Problem (ECSP). The first step is a rewriting step, in which the initial ECSP is symbolically transformed. The second step, called approximation step, is based on a local consistency notion, called weak arc-consistency, defined over ECSPs in terms of fixed point of contractant monotone operators. This notion is shown to generalize previous local <b>consistency</b> <b>concepts</b> defined over finite domains (arc-consistency) or infinite subsets of the Reals (arc B-consistency and interval, hull and box-consistency). A filtering algorithm, derived from AC- 3, is given and is shown to be corr [...] ...|$|R
30|$|Furthermore, we {{recommend}} future research to {{also consider the}} <b>consistency</b> between the <b>concept</b> of reputation and its operationalization more carefully (see Sect.  4.1. 4). The popular definition by Fombrun (1996) combined with the equally popular operationalization based on the Fortune ranking leads to a mismatch.|$|R
40|$|The {{paper and}} the {{electronic}} worlds provide complementary affordances and we envision a framework where there is continuous transfer of knowledge between the two environments. This paper shall discuss the automatic generation of a paper workbook using principles of graphic design, a consequence of electronic world interactions. Our approach incorporates user context models as well a graphic design principles. User interaction in the electronic world affects the user context, thus critically determining media to be selected for paper. The paper workbook creation is then informed by design principles involving layout <b>consistency,</b> <b>concept</b> dominance, contrast and proximity. The media selection and the paper layout are both fully automated. Our preliminary experimental {{results indicate that the}} paper designs are well liked and deemed relevant to prior electronic interaction...|$|E
40|$|Consistency {{modeling}} for gene {{selection is}} a new topic emerging from recent cancer bioinformatics research. The result of classification or clustering on a training set was often found {{very different from the}} same operations on a testing set. Here, we address this issue as a consistency problem. We propose a new concept of performance-based consistency and a new novel gene selection method, Genetic Algorithm Gene Selection method in terms of consistency (GAGSc). The proposed <b>consistency</b> <b>concept</b> and GAGSc method were investigated on eight benchmark microarray and proteomic datasets. The experimental results show that the different microarray datasets have different consistency characteristics, and that better consistency can lead to an unbiased and reproducible outcome with good disease prediction accuracy. More importantly, GAGSc has demonstrated that gene selection, with the proposed consistency measurement, is able to enhance the reproducibility in microarray diagnosis experiments...|$|E
40|$|Consistency {{modeling}} for gene {{selection is}} a new topic emerging from recent cancer bioinformatics research. The result of classification or clustering on a training set was often found {{very different from the}} same operations on a testing set. Here, the issue is addressed as a consistency problem. In practice, the inconsistency of microarray datasets prevents many typical gene selection methods working properly for cancer diagnosis and prognosis. In an attempt to deal with this problem, a new concept of performance-based consistency is proposed in this thesis. An interesting finding in our previous experiments is that by using a proper set of informative genes, we significantly improved the consistency characteristic of microarray data. Therefore, how to select genes in terms of consistency modelling becomes an interesting topic. Many previously published gene selection methods perform well in the cancer diagnosis domain, but questions are raised because of the irreproducibility of experimental results. Motivated by this, two new gene selection methods based on the proposed performance-based <b>consistency</b> <b>concept,</b> GAGSc (Genetic Algorithm Gene Selection method in terms of consistency) and LOOLSc (Leave-one-out Least-Square bound method with consistency measurement) were developed in this study with the purpose of identifying a set of informative genes for achieving replicable results of microarray data analysis. The proposed <b>consistency</b> <b>concept</b> was investigated on eight benchmark microarray and proteomic datasets. The experimental results show that the different microarray datasets have different consistency characteristics, and that better consistency can lead to an unbiased and reproducible outcome with good disease prediction accuracy. As an implementation of the proposed performance-based consistency, GAGSc and LOOLSc are capable of providing a small set of informative genes. Comparing with those traditional gene selection methods without using consistency measurement, GAGSc and LOOLSc can provide more accurate classification results. More importantly, GAGSc and LOOLSc have demonstrated that gene selection, with the proposed consistency measurement, is able to enhance the reproducibility in microarray diagnosis experiments...|$|E
40|$|Abstract. We {{consider}} the consequences on basic reasoning problems of allowing negated primitive concepts on left-hand-sides of inclusion dependencies in the description logic dialect CFDnc. Although earlier work {{has shown that}} this makes CQ answering coNP-complete, we show that TBox <b>consistency</b> and <b>concept</b> satisfiability remain in PTIME. We also show that knowledge base consistency and instance retrieval remain in PTIME if a CFDnc knowledge base satisfies a number of additional conditions, and that failing {{any one of these}} conditions will alone lead to intractability for these problems. ...|$|R
40|$|We {{shed light}} on the {{connections}} between different approaches to constraint satisfaction by showing that the main <b>consistency</b> <b>concepts</b> used to derive tractability results for constraint satisfaction are intimately related to certain combinatorial pebble games, called the existential k-pebble games, that were originally introduced in the context of Datalog. The crucial insight relating pebble games to constraint satisfaction is that the key concept of strong k-consistency is equivalent to a condition on winning strategies for the Duplicator player in the existential k-pebble game. We use this insight to show that strong k-consistency can be established if and only if the Duplicator wins the existential k-pebble game. Moreover, whenever strong k-consistency can be established, one method for doing this is to first compute the largest winning strategy for the Duplicator in the existential k-pebble game and then modify the original problem by augmenting it with the constraints expressed by the largest winning strategy. This basic result makes it possible to establish deeper connections between pebble games, consistency properties, and tractability of constraint satisfaction. In particular, we use existential k-pebble games to introduce the concept of k-locality and show that it constitutes a new tractable case of constraint satisfaction that properly extends the well known case in which establishing strong k-consistency implies global consistency...|$|R
40|$|This paper {{presents}} {{a progress report}} on the implementation ABox reasoner and a knowledge representation framework. We present an ABox reasoner which has been constructed for providing a basis for an optimized implementation. We compare the implementation with the <b>concept</b> <b>consistency</b> reasoner FaCT which sets the standard in current DL implementations...|$|R
40|$|Modeling of {{real time}} {{software}} systems (RTSS) consist of different components with UML 2. 0 {{leads to a}} design model using various diagrams. To get a consistent model, a <b>consistency</b> <b>concept</b> for different diagrams type is needed {{that takes into account}} real time constraints. Ensuring consistency of The Unified Modeling Language (UML) model is very crucial as it is effect to the quality of UML model and directly gives impact to good implementation of Information System. Although there are increasing researches on consistency management, there is still lack of researches of consistency driven by Use Case. With this motivation, in this paper, we have proposed few consistency rules between Use Case, Sequence and Timing diagrams which focus on the establishment of timing constraints. Elements of each diagram involved in the proposed rules are formalized. Using an example, we show how the diagrams fulfill our proposed consistency rules...|$|E
30|$|Third, {{scenario}} consistency {{depends on}} the consistency criterion applied, i.e., A and B are (in-)consistent {{with respect to a}} specific definition of consistency (x). Regarding scenario consistency, these criteria can be either intuitive (holistic) or systematic (analytic and formal). A scenario can be intuitively matching one’s ideas and its intuitive consistency can be judged by subjective assessment. On the contrary, a systematic-analytic <b>consistency</b> <b>concept</b> follows formal rules that allow for objectively decomposing and recomposing its logics; examples are coincidence and causality. We assume that different consistency criteria can conflict. A scenario pair consistent {{in the sense of the}} CA is not necessarily consistent in the sense of CIB – and it is an open question whether a scenario pair consistent with regard to a formal criterion is also intuitively perceived as a consistent one by (internal or external) users. In sum: ‘(A) and (B) are (in-)consistent under criterion (x)’, with A and B being scenario (elements) or numerical, conceptual or mental models.|$|E
40|$|We {{propose a}} {{consistency}} {{model for a}} data store in the Cloud and work towards the goal of deploying Database as a Service over the Cloud. This includes consistency across the data partitions and consistency of any replicas that exist across different nodes in the system. We target applications which need stronger consistency guarantees than the applications currently supported by the data stores on the Cloud. We propose a cost-effective algorithm that ensures distributed consistency of data without really compromising on availability for fully replicated data. This paper describes a design in progress, presents the consistency and recovery algorithms for relational data, highlights the guarantees provided by the system and presents future research challenges. We believe that the current notions of consistency for databases might not be applicable over the Cloud and a new formulation of the <b>consistency</b> <b>concept</b> may be needed keeping in mind the application classes we aim to support...|$|E
40|$|Abstract. This {{paper is}} an attempt to address the {{processing}} of nonlinear numerical constraints over the Reals by combining three di erent methods: local consistency techniques, symbolic rewriting and interval methods. To formalize this combination, we de ne a generic two-step constraint processing technique based on an extension of the Constraint Satisfaction Problem, called Extended Constraint Satisfaction Problem (ECSP). The rst step is a rewriting step, in which the initial ECSP is symbolically transformed. The second step, called approximation step, is based on a local consistency notion, called weak arc-consistency, de ned over ECSPs in terms of xed point ofcontractant monotone operators. This notion is shown to generalize previous local <b>consistency</b> <b>concepts</b> de ned over nite domains (arc-consistency) or in nite subsets of the Reals (arc B-consistency and interval, hull and box-consistency). A l-tering algorithm, derived from AC- 3, is given and is shown to be correct, con uent and to terminate. This framework is illustrated by the combination of Grobner Bases computations and Interval Newton methods. The computation of Grobner Bases for subsets of the initial set of constraints is used as a rewriting step and operators based on Interval Newton methods are used together with enumeration techniques to achieve weak arc-consistency on the modi ed ECSP. Experimental results from a prototype are presented, as well as comparisons with other systems...|$|R
40|$|In his monumental discoveries, {{the driving}} force for Einstein was, I believe, <b>consistency</b> of <b>concept</b> and {{principle}} rather than conflict with experiment. Following this Einsteinian dictum, we would first argue that homogeneity (universal character) {{of space and time}} characterizes 'no force' (absence of force) and leads to existence of a universal velocity while inhomogeneity (again a universal property) characterizes curved spacetime and presence of a universal force which is present everywhere and always. The former gives rise to Special Relativity while the latter to General Relativity. Comment: 12 pages, latex. arXiv admin note: substantial text overlap with physics/ 050509...|$|R
30|$|Since {{we think}} that the {{disadvantages}} of the use of probabilities outweigh the respective advantages – and, therefore, that no probabilities should be used – we will present an alternative approach: focusing on the most relevant scenario(s). This concept combines the <b>concept</b> of <b>consistency</b> with the <b>concepts</b> of attributes and closeness. In this context, we use an inductive approach.|$|R
40|$|Constructing complex {{software}} {{systems by}} integrating different software components is a promising and challenging approach. With the functionality of software components given by models {{it is possible}} to ensure consistency of such models before implementation in order to successfully build the system. Models consisting of different submodels, the absence of an overall formal semantics and the numerous possibilities of employing models requires the development of techniques ensuring the consistency. In this paper, we discuss the issue of consistency of models made up of different submodels proposing a concept for the management of consistency. Consistency management relies on a concept of consistency and a process for ensuring consistency of models. We introduce a <b>consistency</b> <b>concept</b> for software components modeled in the Unified Modeling Language (UML) and devise suitable consistency checks. On this basis, we propose a process how to locate and resolve inconsistencies, thus ensuring the consistency of models and by that the consistency of componentbased systems derived from those models...|$|E
40|$|Some {{preliminary}} {{results are presented}} on a novel approach {{to the analysis of}} the propagation of round-off errors in the fast transversal filter (FTF) recursive least squares (RLS) algorithm. This approach is based on the concept of backward consistency which can be applied to any recursive algorithm, e. g. to the class of Kalman filtering algorithms. The backward <b>consistency</b> <b>concept</b> is applied to the FTF algorithm. This application leads to the introduction of the FTF state variables that are backwardly consistent. In other words, each point on the FTF manifold represents a value for the FTF state variables that corresponds exactly to the solution of a prewindowed shift-invariant least-squares (LS) problem. The advantage of this approach is that the error propagation on the FTF manifold corresponds exactly (without averaging or even linearization) to the propagation of a perturbation on the input data in the LS problem. The dynamics of this perturbation are analyzed. Anglai...|$|E
30|$|Even {{with these}} limitations, however, {{we believe the}} study makes an {{important}} contribution to the emerging stream of research on serial acquisitions, research on transfer effects and organizational learning in strategic settings, as well as to empirical research on strategic consistency. Our focus was to analyze whether strategic consistency has an effect on the expected performance of serial acquisitions. To further seize the coupling of the pattern and learning streams of research of acquisition series, succeeding studies could start from here and analyze differences in patterns of acquisition direction and search for optimal patterns. Further studies of strategic consistency in acquisition programs may include divestment decisions, as they are generally an integral part of restructuring programs. We look forward to studies that extend the strategic <b>consistency</b> <b>concept</b> toward a congruency-related approach, also incorporating the fit of serial acquisitions to external contingencies and competitive changes (Lamberg et al. 2009). Methodically, the innovative approach of cumulative capital market perception offers room for further development and application beyond the context of acquisition series.|$|E
40|$|Abstract: This paper {{presents}} {{a progress report}} on the implementation of an ALCRP(D) ABox reasoner and a knowledge representation framework. We present an ALC ABox reasoner which has been constructed for providing a basis for an optimized ALCRP(D) implementation. We compare the implementation with the <b>concept</b> <b>consistency</b> reasoner FaCT which sets the standard in current DL implementations...|$|R
40|$|The <b>consistency</b> of the <b>concept</b> {{of quantum}} (quasi) {{particles}} possessing effective mass {{which is both}} position- and excitation-dependent is analyzed via simplified models. It is shown that the system may be stable even when the effective mass m=m(x,E) itself acquires negative values in a limited range of coordinates x and energies E. Comment: 17 pp., 3 fig...|$|R
40|$|The {{purpose of}} this study was to {{investigate}} the impact of using the E-learning approach as well as IT technologies in the teaching process of Economics Information System (EIS) course to graduate students. Also, this study concentrates on two trends, the first one related to the befits of using E-learning approach in teaching EIS. The benefits of this approach are: Saving time for both the instructors and students; flexible scheduling for student since the course can be taken by the employee on time without interfering with his/her work, enable the students to access the material course to be learned at any time in the case of missing his/her lecture or session and improving the <b>consistency</b> <b>concepts.</b> The second related to the effect of information technology on the economic system. For example, we shall see what information is required to make markets efficient. Also, we shall see how this demand for information should be curtailed in increase privacy at the expense of efficiency. The important conclusion of this study can be shown as: Each student can view course content of EIS as well as the examination and homework announcement. Each student can view only his/her own grade not for other student. connection between students and their instructor will be done through a feedback form or e-mail. Students can evaluate his/her self through some quiz grade and the mistake question and finally the students have a capability of surveying all the previous examinations for the same course and subjects...|$|R
