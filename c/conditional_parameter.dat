12|225|Public
40|$|In {{practical}} Bayesian optimization, we {{must often}} search over structures with dif-fering numbers of parameters. For instance, we {{may wish to}} search over neural network architectures with {{an unknown number of}} layers. To relate performance data gathered for different architectures, we define a new kernel for <b>conditional</b> <b>parameter</b> spaces that explicitly includes information about which parameters are relevant in a given structure. We show that this kernel improves model quality and Bayesian optimization results over several simpler baseline kernels. ...|$|E
40|$|We prove a no-go theorem for a {{class of}} hidden {{variables}} theories that satisfy parameter independence. Specifically, we show that, assuming two conditions, there are no non-trivial hidden variables models of the quantum predictions for product measurements on two systems in any maximally entangled state in a Hilbert space of dimension at least 3 × 3. The two conditions are parameter independence and a condition that we call <b>conditional</b> <b>parameter</b> independence. The result {{is analogous to the}} recent no-go theorems based on Leggett’s inequalities and their generalisations. ...|$|E
40|$|This paper {{separates}} <b>conditional</b> <b>parameter</b> estimation, which consistently raises {{test set}} accuracy on statistical NLP tasks, from conditional model structures, {{such as the}} conditional Markov model used for maximum-entropy tagging, which tend to lower accuracy. Error analysis on part-of-speech tagging shows that the actual tagging errors made by the conditionally structured model derive not only from label bias, but also from other {{ways in which the}} independence assumptions of the conditional model structure are unsuited to linguistic sequences. The paper presents new word-sense disambiguation and POS tagging experiments, and integrates apparently conflicting reports from other recent work. ...|$|E
40|$|Build systems {{contain a}} lot of {{configuration}} knowledge about a software system, such as under which conditions specific files are compiled. Extracting such configuration knowledge is important for many tools analyzing highly-configurable systems, but very challenging due to the complex nature of build systems. We design an approach, based on SYMake, that symbolically evaluates Make files and extracts configuration knowledge in terms of file presence conditions and <b>conditional</b> <b>parameters.</b> We implement an initial prototype and demonstrate feasibility on small examples...|$|R
40|$|Gaussian finite-mixture {{models are}} {{extended}} {{to include the}} use of auxiliary information, the dependence of component membership probabilities being modelled by a generalized linear model for polytomous responses. Among the possible applications of the proposed methodology are probabilistic classification and estimation of group <b>conditional</b> <b>parameters.</b> Identifiability features of such a model are investigated in comparison with standard finite mixtures. A full Bayesian hierarchical representation of the model is developed to implement the Gibbs sampling estimation algorithm. Two examples are presented where the methodology is applied to the analysis of real and synthetic data...|$|R
5000|$|In {{the online}} {{advertising}} industry, a Viewable Impression is a metric of ads which were actually viewable when served (in part, entirely or based on other <b>conditional</b> <b>parameters).</b> The first system to deliver reports {{based on a}} Viewable Impression metric for standard IAB (Interactive Advertising Bureau) Display ad units, called RealVu, was developed by Rich Media Worldwide and accredited by the Media Rating Council on March 9, 2010. Other companies to offer viewable impressions include DMA-Institute [...] OnScroll, C3 Metrics, Comscore, and AdYapper, while MSNBC utilizes ServeView, a proprietary system in use since 2010.|$|R
40|$|Abstract. Large {{amount of}} digital content would be stored safely in {{peer-to-peer}} network, with encrypted format. Being requested, a cipher text is downloaded from certain peer and decrypted by a delegated decryptor {{to obtain the}} clear text. Observing the need for this new kind of delegation decryption service, we propose a novel time constraint delegation scheme for decrypting p 2 p data in this paper. The new features of the delegation scheme are that: it uses a flexible secure mobile agent solution without designated delegation server; the time constraint <b>conditional</b> <b>parameter</b> is clearly bound with the protocols; and the computation complexity is greatly reduced by replacing public key computation with hash function. We elaborate the protocol design {{as well as its}} security, extensions and properties. Potential applications in content delivery network and pervasive computing scenarios are depicted. ...|$|E
40|$|This paper investigates ways {{to explore}} the between frame {{correlation}} of shape information {{within the framework of}} an operationally rate-distortion (ORD) optimal coder. Contours are approximated both by connected second-order spline segments, each defined by three consecutive control points, and by segments of the motioncompensated reference contours. Consecutive control points are then encoded predictively using angle and run temporal contexts. We utilize a novel criterion for selecting global object motion vectors, which further improves efficiency. Formulating this problem as Lagrangian minimization, we employ an iterative technique to remove dependency on a particular VLC and jointly arrive at the ORD optimal solution and its underlying <b>conditional</b> <b>parameter</b> distribution. 1. INTRODUCTION In the process of evaluating competing techniques for the MPEG- 4 standard, several binary coders were considered. These coders, however, lack optimality in their both intra and inter modes of ope [...] ...|$|E
40|$|We prove a no-go theorem for a {{class of}} hidden {{variables}} theories that satisfy parameter independence. Specifically, we show that, assuming two conditions, there are no non-trivial hidden variables models of the quantum predictions for product measurements on two systems in any maximally entangled state in a Hilbert space of dimension at least 3 x 3. The two conditions are parameter independence and a condition that we call <b>conditional</b> <b>parameter</b> independence. The result {{is analogous to the}} recent no-go theorems based on Leggett's inequalities and their generalisations. Comment: 13 pages. As compared to v 1, this version includes the corrections in proof. Published in L. Accardi, G. Adenier, C. Fuchs, G. Jaeger, A. Yu. Khrennikov, J. -Å. Larsson and S. Stenholm (eds), Foundations of Probability and Physics- 5, American Institute of Physics Conference Proceedings, Vol. 1101 (New York: AIP, 2009), pp. 233 - 24...|$|E
40|$|Value at risk (VaR) is {{a measure}} for senior {{management}} that summarises the financial risk a company faces into one single number. In this paper, we consider the use of fuzzy histograms for quantifying the value-at-risk of a portfolio. It is shown {{that the use of}} fuzzy histograms provides a good method of value-at-risk estimation for a portfolio of stocks. The <b>conditional</b> <b>parameters</b> of the model are obtained through minimisation of a test statistic for a VaR back testing method. Evolutionary optimisation is used for this purpose. It is found that statistical back testing always accepts fuzzy histogram models, while the popular GARCH models may be rejected...|$|R
40|$|The {{purpose of}} this study is to analyse the {{relationship}} between <b>conditional</b> <b>parameters</b> (leg strength, back strength, velocity 30 Mt, flexibility) by measuring some physical (height, body w eight) and physiological (systole, diastole, KAH) characteristics of male football players of Karakopru Belediyespor and Harran University. According to the results obtained from the measurements, mean age was 23, 46 ± 3, 50 /years); as a part of physical cha racteristics,mean height was 176, 20 ± 5, 10 (cm) and mean body weight was 70, 16 ± 5, 21 (kg). As a part of physiological characteristics, mean Systolic Blood Pressure was 123, 87 ± 14, 23 (mmhg), mean Diastolic Blood Pressure was 73, 60 ± 16, 42 (mmhg) and mean Resting Heart Rate was 64, 50 ± 10, 48 (beats/min). As a part of <b>conditional</b> <b>parameters,</b> mean leg strength was 101, 83 ± 40, 48 (kg), back strength was 75, 83 ± 19, 43 (kg), flexibility was 34, 16 ± 6, 65 (cm) and mean velocity in 30 Mt. was 4, 15 ± 0, 20 (sec). It was observed that there was a relationship between 30 meters velocity and leg strength parameters (r= -, 407). There was no relationship between 30 meters velocity and back strength parameters (r=, 429); and between 30 meters velocity and flexibility param eters (r=, 659). As a result, while the relationship between velocity and back strength values of the amateur football players was not significant (p> 0. 05); the relationship between velocity and leg strength values was found to be significant (p< 0. 05) ...|$|R
40|$|We {{estimate}} several GARCH- and Extreme Value Theory (EVT) -based {{models to}} forecast intraday Value-at-Risk (VaR) and Expected Shortfall (ES) for S&P 500 stock index futures returns for both {{long and short}} positions. Among the GARCH-based models we consider is the so-called Autoregressive Conditional Density (ARCD) model, which allows time-variation in higher-order conditional moments. ARCD model with time-varying <b>conditional</b> skewness <b>parameter</b> has the best in-sample fit among the GARCH-based models. The EVT-based model and the GARCH-based models which take conditional skewness and kurtosis (time-varying or otherwise) into account provide accurate VaR forecasts. ARCD model with time-varying <b>conditional</b> skewness <b>parameter</b> seems to provide the most accurate ES forecasts. Density estimation Higher-order conditional moments Intraday Value-at-Risk and Expected Shortfall...|$|R
40|$|In {{the absence}} of {{inflation}}-linked bonds or inflation swaps, no perfect hedging strategy exists for inflation-linked liabilities, and nominal bonds are often used as substitute hedging instruments. This paper provides a formal analysis {{of the problem of}} hedging inflation-linked liabilities with nominal bonds in the presence of real rate uncertainty as well as realized and expected inflation risks. While a long-only position in nominal bonds will always have a negative exposure to unexpected inflation, our analysis suggests that long-short nominal bond portfolio strategies can in principle be designed to achieve a zero exposure to changes in unexpected inflation (required to hedge inflation-linked liabilities), while having a target exposure to changes in real rate equal to that of liabilities. The practical implementation of such long-short replication strategies, however, is not a straightforward task in the presence of parameter uncertainty. We explore several non-exclusive solutions to the estimation risk problem, including the use of <b>conditional</b> <b>parameter</b> estimation methodologies as well as the introduction of robust restrictions on input parameters or portfolio weights. These approaches lead to substantial improvements in out-of-sample hedging performance...|$|E
40|$|This {{study focused}} on {{producing}} flash flood hazard susceptibility maps (FFHSM) using frequency ratio (FR) and statistical index (SI) models in the Xiqu Gully (XQG) of Beijing, China. First, a total of 85 flash flood hazard locations (n = 85) were surveyed {{in the field and}} plotted using geographic information system (GIS) software. Based on the flash flood hazard locations, a flood hazard inventory map was built. Seventy percent (n = 60) of the flooding hazard locations were randomly selected for building the models. The remaining 30 % (n = 25) of the flooded hazard locations were used for validation. Considering that the XQG used to be a coal mining area, coalmine caves and subsidence caused by coal mining exist in this catchment, as well as many ground fissures. Thus, this study took the subsidence risk level into consideration for FFHSM. The ten conditioning parameters were elevation, slope, curvature, land use, geology, soil texture, subsidence risk area, stream power index (SPI), topographic wetness index (TWI), and short-term heavy rain. This study also tested different classification schemes for the values for each <b>conditional</b> <b>parameter</b> and checked their impacts on the results. The accuracy of the FFHSM was validated using area under the curve (AUC) analysis. Classification accuracies were 86. 61 %, 83. 35 %, and 78. 52 % using frequency ratio (FR) -natural breaks, statistical index (SI) -natural breaks and FR-manual classification schemes, respectively. Associated prediction accuracies were 83. 69 %, 81. 22 %, and 74. 23 %, respectively. It was found that FR modeling using a natural breaks classification method was more appropriate for generating FFHSM for the Xiqu Gully...|$|E
40|$|Thesis (Master's) [...] University of Washington, 2013 Rarely {{observed}} covariate combinations, or "sparsity" is {{a phenomenon}} associated with research concerning the health risks of alternative-use (non-combusted tobacco products (AUPs)). Of particular concern is sparsity relating to AUP users who do not currently or formerly use other tobacco products. This thesis aims to identify reasons why sparsity is a concern, the effect that sparsity can have on statistical inference, and potential appropriate approaches {{in the presence of}} sparsity. Special attention will be paid to scenarios in which sparsity can lead to inference that results in estimates of the AUP effect that are in the opposite direction of the true effect (e. g. found to be harmful when truly beneficial) and to be in an opposite direction related to the cigarette effect (e. g. found to be less harmful than cigarettes when truly more harmful). The impact of sparsity will be assessed primarily by constructing examples from both case-control and cohort studies and investigating the results from common statistical modeling methods under sparse and non-sparse conditions. These examples will include hypothetical examples constructed to approximate real world study design as well as data from a published study of an AUP. These examples will focus on issues of sparsity in relation to interaction assumptions and model scale assumptions. <b>Conditional</b> <b>parameter</b> estimates can vary widely from the marginal estimates for that parameter. Data sets with few subjects who use AUPs without also using cigarettes have reduced power to detect interaction. When scale or interaction assumptions are violated estimation of incidence rate or parameter values can be biased. This bias can be such that conclusions from analysis of sparse data sets can be misleading. These issues can cause AUP use to be estimated as beneficial when it is in truth harmful, or as less harmful than cigarettes when in truth it is more harmful. These issues are of such severity that we, if {{it is not possible to}} oversample the sparse categories, recommend restricting analysis to subgroups in which sparsity is unlikely to be a concern...|$|E
50|$|Integrated <b>Conditional</b> Density: This <b>parameter</b> {{measures}} the density at various length scales and therefore describes the homogeneity of density throughout an animal group.|$|R
40|$|We derive the semiparametric {{efficiency}} {{bound in}} dynamic models of conditional quantiles under a sole strong mixing assumption. We also provide {{an expression of}} Stein’s (1956) least favorable parametric submodel. Our approach is as follows: First, we construct a fully parametric submodel of the semiparametric model defined by the conditional quantile restriction that contains the data generating process. We then compare the asymptotic covariance matrix of the MLE obtained in this submodel {{with those of the}} M-estimators for the <b>conditional</b> quantile <b>parameter</b> that are consistent and asymptotically normal. Finally, we show that the minimum asymptotic covariance matrix of this class of M-estimators equals the asymptotic covariance matrix of the parametric submodel MLE. Thus, (i) this parametric submodel is a least favorable one, and (ii) the expression of the semiparametric efficiency bound for the <b>conditional</b> quantile <b>parameter</b> follows. ...|$|R
40|$|AbstractWe present {{methods to}} handle error-in-variables models. Kernel-based {{likelihood}} score estimating equation methods are developed for estimating <b>conditional</b> density <b>parameters.</b> In particular, a semiparametric likelihood method is proposed for sufficiently using {{the information in}} the data. The asymptotic distribution theory is derived. Small sample simulations and a real data set are used to illustrate the proposed estimation methods...|$|R
40|$|Quantifying and characterizing {{groundwater}} recharge {{are critical for}} water resources management. Unfortunately, low recharge rates are difficult to resolve in dry environments, where groundwater is often most important. Motivated by such concerns, this thesis presents a new probabilistic approach for analyzing diffuse recharge in semiarid environments and demonstrates it for the Southern High Plains (SHP) in Texas. Diffuse recharge in semi-arid and arid regions {{is likely to be}} episodic, which could have important implications for groundwater. Our approach makes it possible to assess how episodic recharge can occur and to investigate the control mechanisms behind it. Of the common recharge analysis methods, numerical modeling is best suited for considering control mechanisms and is the only option for predicting future recharge. However, it is overly sensitive to model errors in dry environments. Natural chloride tracer measurements provide more robust indicators of low flux rates, yet traditional chloride-based estimation methods only produce recharge at coarse time scales that mask most control mechanisms. We present a data assimilation approach based on importance sampling that combines modeling and data-based estimation methods in a consistent probabilistic manner. Our estimates of historical recharge time series indicate that at the SHP data sites, deep percolation (potential recharge) is indeed highly episodic and shows significant interannual variability. Conditions that allow major percolation events are high intensity rains, moist antecedent soil conditions, and below-maximum root density. El Niño events can contribute to interannual variability of percolation by bringing wetter winters, which produce modest percolation events and provide wet antecedent conditions that trigger spring episodic recharge. (cont.) Our data assimilation approach also generates <b>conditional</b> <b>parameter</b> distributions, which are used to examine sensitivity of recharge to potential climate changes. A range of global circulation model predictions are considered, including wetter and drier futures. Relative changes in recharge are generally more pronounced than relative changes in rainfall, demonstrating high susceptibility to climate change impacts. The temporal distribution of rainfall changes is critical for recharge. Our results suggest that increased total precipitation or higher rain intensity during key months could make strong percolation peaks more common. by Gene-Hua Crystal Ng. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, Dept. of Civil and Environmental Engineering, 2009. This electronic version was submitted by the student author. The certified thesis is available in the Institute Archives and Special Collections. Includes bibliographical references (p. 153 - 161) ...|$|E
40|$|This {{dissertation}} {{is focused}} {{on the development of the}} optimal design and analysis for cluster randomized trials. Specifically, we tackle three common questions: whether or not to pair-match clusters, which causal parameter best captures the intervention effect, and how to select the adjustment set for the analysis. We begin by introducing a formal framework for causal inference in Chapter 1. Throughout, the Sustainable East Africa Research in Community Health (SEARCH) trial serves as the motivating example (NCT 01864603). SEARCH is an ongoing community randomized trial to evaluate the impact of immediate and streamlined antiretroviral therapy on HIV incidence in rural East Africa. In Chapter 2, we consider pair-matching, an intuitive design strategy to protect study validity and to potentially increase power in randomized trials. In a common design, candidate units are identified, and their baseline characteristics are used to create the best n/ 2 matched pairs. Within the resulting pairs, the intervention is randomized, and the outcomes are measured at the end of follow-up. We consider this design to be adaptive, because the construction of the matched pairs depends on the baseline covariates of all candidate units. As a consequence, the observed data cannot be considered as n/ 2 independent, identically distributed (i. i. d.) pairs of units, as common practice assumes. Instead, the observed data consist of n dependent units. Chapter 2 explores the consequences of adaptive pair-matching in randomized trials for estimation of the conditional average treatment effect (CATE) : the intervention effect, given the measured covariates of the n study units. We contrast the unadjusted estimator with TMLE and show substantial efficiency gains from matching and further gains with adjustment. In Chapter 3, we compare three causal parameters: the population, conditional and sample average treatment effects. Using a structural causal model, we explicitly define each parameter, discuss interpretation, and formally examine identifiability. To the best of our knowledge, Chapter 3 is the first to propose using TMLE for estimation and inference of the sample effect. In most settings, the sample parameter will be estimated more efficiently than the <b>conditional</b> <b>parameter,</b> which will, in turn, be estimated more efficiently than the population parameter. Finite sample simulations illustrate the potential gains in precision and power from selecting the sample effect as the target of inference. Finally in Chapter 4, we discuss adjustment for measured covariates during the analysis to reduce variance and increase power in randomized trials. To avoid misleading inference, the analysis plan must be pre-specified. However, it is often unclear a priori which baseline covariates (if any) should be included in the analysis. In the SEARCH trial, for example, there are 16 matched pairs of communities and many potential adjustment variables, including region, HIV prevalence, male circumcision coverage and measures of community-level viral load. In Chapter 4, we propose a rigorous procedure to data-adaptively select the adjustment set, which maximizes the efficiency of the analysis. Specifically, we use cross-validation to select from a pre-specified library the candidate TMLE that minimizes the estimated variance. For further gains in precision, we also propose a collaborative procedure for estimating the known exposure mechanism. Our small sample simulations demonstrate the promise of the methodology to maximize study power, while maintaining nominal confidence interval coverage. Our procedure is tailored to the scientific question (sample vs. population treatment effect) and study design (pair-matched or not) and alleviates many of the common concerns...|$|E
40|$|Abstract — We {{focus on}} the {{analysis}} of the performance of HTTP transaction over TCP connections as derived from a measured data set collected at the ingress/egress point of Politecnico di Torino. We argue that looking at global average performance figures may hide some interesting trends that researchers and operators may want to detect. Thus, we partition the original measured set by looking at conditional densities and averages; considered <b>conditional</b> <b>parameters</b> include, among others, TCP connections RTTs, length of data set, adopted loss recovery algorithm. We show that particular care must be taken when analyzing data, since the measured set whose size is reduced by the partitioning scheme could be not large enough to ensure significance. A simple method to evaluate the significance of the presented measures is used. Ingenuity is required to understand behaviors not in line with the classical intuition driven by TCP knowledge. I...|$|R
40|$|We present mlrMBO, a {{flexible}} and comprehensive R toolbox for model-based optimization (MBO), {{also known as}} Bayesian optimization, which addresses the problem of expensive black-box optimization by approximating the given objective function through a surrogate regression model. It is designed for both single- and multi-objective optimization with mixed continuous, categorical and <b>conditional</b> <b>parameters.</b> Additional features include multi-point batch proposal, parallelization, visualization, logging and error-handling. mlrMBO is implemented in a modular fashion, such that single components can be easily replaced or adapted by the user for specific use cases, e. g., any regression learner from the mlr toolbox for machine learning can be used, and infill criteria and infill optimizers are easily exchangeable. We empirically demonstrate that mlrMBO provides state-of-the-art performance by comparing it on different benchmark scenarios against {{a wide range of}} other optimizers, including DiceOptim, rBayesianOptimization, SPOT, SMAC, Spearmint, and Hyperopt. Comment: 23 pages, 5 figure...|$|R
40|$|Part 3 : Policy ManagementInternational audienceEnterprises {{nowadays}} are subscribing {{access to}} several Internet Service Providers (ISPs) for reliability, redundancy and better revenues underlying the service extension, while providing good Quality of Service (QoS). In this paper, a dynamic decision-making framework is presented for Session Initiation Protocol (SIP) based voice/video call routing in multihomed network. The decision engine takes multiple criteria into account while computing the routing decision (attributes from {{context of the}} request, platform’s latest <b>conditional</b> <b>parameters,</b> business objectives of the company, etc.). Two Multi-Criteria Decision Making (MCDM) methods, namely Grey Relational Analysis (GRA) and an extended version of Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) are used for decision calculation in outsourcing and provisioning enforcement modes respectively. The proposed solution gives higher throughput and lower call dropping probability while fulfilling the desired goals, {{taking into account the}} multiple attributes for choosing the best alternative...|$|R
40|$|Abstract—Companies {{nowadays}} are subscribing {{links to}} sev-eral Internet Service Providers (ISPs) for reliability, redundancy and better revenues underlying the service extension, while providing good Quality of Service (QoS). A dynamic decision-making framework is presented for SOCKS based data services over a multihomed platform that is primarily architectured for multimedia services. The decision engine takes multiple criteria (attributes from {{context of the}} request, platform’s latest <b>conditional</b> <b>parameters,</b> business objectives of the company, etc.) into account while computing the routing decision. Two Multi-Criteria Decision Making (MCDM) methods, namely Analytical Hierarchy Process (AHP) and Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) are used for weight cal-culation and decision-making respectively. The system supports outsourcing and provisioning decision enforcement modes. The proposed solution gives higher throughput and lower connection dropping probability with an add-on susceptible delay while fulfilling the desired goals, {{taking into account the}} multiple attributes for choosing the best alternative...|$|R
40|$|International audienceCompanies {{nowadays}} {{are approaching}} towards an all IP paradigm by subscribing to different access technology links from several service providers for reliability, redundancy and availability while providing good Quality of Service (QoS). A framework for rule-based converged hybrid network manage- ment in multihoming environment is presented. Diversity and dimensionality of the platform's service, control, transport and access planes along with multiple objectives over the platform has {{led us to}} use Multi Criteria Decision Making (MCDM) technique. A simple use-case involving the dynamic decision-based routing at private-public network border is presented to validate the proposed solution. The decision engine takes multiple criteria into account while computing the routing decisions (attributes from context of the request, platforms latest <b>conditional</b> <b>parameters,</b> business objectives of the company etc.). The system supports outsourcing and provisioning decision computation and enforcement modes. The system gives higher throughput and lower call dropping probability with an add-on susceptible delay offering good QoS...|$|R
5000|$|The {{sampling}} distribution is {{the distribution of}} the observed data <b>conditional</b> on its <b>parameters,</b> i.e[...] This is also termed the likelihood, especially when viewed {{as a function of the}} parameter(s), sometimes written [...]|$|R
40|$|We present {{methods to}} handle error-in-variables models. Kernel-based {{likelihood}} score estimating equation methods are developed for estimating <b>conditional</b> density <b>parameters.</b> In particular, a semiparametric likelihood method is proposed for sufficiently using {{the information in}} the data. The asymptotic distribution theory is derived. Small sample simulations and a real data set are used to illustrate the proposed estimation methods. Conditional density estimation Empirical likelihood Kernel estimation Measurement error Surrogate variables...|$|R
40|$|Magnesite {{deposits}} Dúbrava and Miková {{are located}} in Carboniferous formations between Brádno and Ochtiná (Dúbrava massif). Carboniferous magnesites are, according to the lithostratigraphic division of Early Paleozoic complexes of Gemericum by Bajaník et al. (1983), situated in the Dobiná Group, more precisely in {{the upper part of}} the Ochtiná Formation, in the environment of black schists with intercalation of metabasalts and their pyroclastics. In the lower parts of the formation are small-pebble conglomerates and polymict sandstones. Carbonatic bodies of the Dúbrava massif has the directional length, 4 500 m, course NE-SW, inclination 55 - 60 o to SE and maximal thickness 600 m. A calculation in 1967 indicated above 500 millions kt of reserves whish after a modification of <b>conditional</b> <b>parameters</b> was reduced to its three fifths. Reserves excluded during the second calculation had a higher content of Fe 2 O 3 causing the lowering of fireproof products quality. In the text the structural and stability conditions in the area of the Dúbrava deposit and the Miková deposit of the Dúbrava massif are analysed...|$|R
40|$|Because the {{application}} of surface fitting algorithms exerts a considerable fuzzy influence on the mathematical features of kinetic energy distribution, their relation mechanism in different external <b>conditional</b> <b>parameters</b> must be quantitatively analyzed. Through determining the kinetic energy value of each selected representative position coordinate point by calculating kinetic energy parameters, several typical algorithms of complicated surface fitting are applied for constructing microkinetic energy distribution surface models in the objective turbulence runner with those obtained kinetic energy values. On the base of calculating the newly proposed mathematical features, we construct fuzzy evaluation data sequence and present a new three-dimensional fuzzy quantitative evaluation method; then the value change tendencies of kinetic energy distribution surface features can be clearly quantified, and the fuzzy performance mechanism discipline between the performance results of surface fitting algorithms, the spatial features of turbulence kinetic energy distribution surface, and their respective environmental parameter conditions can be quantitatively analyzed in detail, which results in the acquirement of final conclusions concerning the inherent turbulence kinetic energy distribution performance mechanism and its mathematical relation. A further turbulence energy quantitative study can be ensured...|$|R
40|$|Estimation of {{marginal}} {{or partial}} effects of covariates x on various <b>conditional</b> <b>parameters</b> or functionals {{is often the}} main target of applied microeconometric analysis. In the specific context of probit models such estimation is straightforward in univariate models, and Greene, 1996, 1998, has extended these results to cover the case of quadrant probability marginal effects in bivariate probit models. The {{purpose of this paper}} is to extend these results to the general multivariate probit context for arbitrary orthant probabilities and to demonstrate the applicability of such extensions in contexts of interest in health economics applications. The baseline results are extended to models that condition on subvectors of y, to count data structures that derive from the probability structure of y, to multivariate ordered probit data structures, and to multinomial probit models whose marginal effects turn out to be a special case of those of the multivariate probit model. Simulations reveal that analytical formulae versus fully numerical derivatives result in a reduction in computational time as well as an increase in accuracy. ...|$|R
40|$|Dysfunctions of the {{autonomic}} {{nervous system}} in critically ill patients with Acute Brain Injury (ABI) lead to changes in Heart Rate Variability (HRV) which appear to be particularly marked in patients subsequently declared in Brain Death (BD). HRV series are non-stationary, exhibit long memory in the mean and time-varying conditional variance (volatility), characteristics that are well modeled by AutoRegressive Fractionally Integrated Moving Average (ARFIMA) models with Generalized AutoRegressive Conditional Heteroscedastic (GARCH) errors. The long memory is estimated by the parameter d of the ARFIMA-GARCH model, whilst the time-varying <b>conditional</b> variance <b>parameters,</b> u and v characterize, respectively, the short-range and the persistence in the conditional variance. In this work, the ARFIMA-GARCH approach is applied to HRV series of 15 pediatric patients with ABI admitted in a pediatric intensive care unit, 5 of which has BD confirmed and 9 patients survived. The long memory and time-varying <b>conditional</b> variance <b>parameters</b> estimated by ARFIMA-GARCH modeling significantly differ between groups and seem able to contribute to characterize disease severity in children with ABI...|$|R
30|$|In other words, {{we require}} that probabilities of {{different}} values of <b>parameters</b> <b>conditional</b> {{on the same}} set of measurements are different. In this paper, we assume that the parameters of all considered systems are observable.|$|R
40|$|License, which permits {{unrestricted}} use, distribution, {{and reproduction}} in any medium, provided the original work is properly cited. Because {{the application of}} surface fitting algorithms exerts a considerable fuzzy influence on the mathematical features of kinetic energy distribution, their relation mechanism in different external <b>conditional</b> <b>parameters</b> must be quantitatively analyzed. Through determining the kinetic energy value of each selected representative position coordinate point by calculating kinetic energy parameters, several typical algorithms of complicated surface fitting are applied for constructing microkinetic energy distribution surfacemodels in the objective turbulence runner with those obtained kinetic energy values. On the base of calculating the newly proposedmathematical features, we construct fuzzy evaluation data sequence and present a new three-dimensional fuzzy quantitative evaluation method; then the value change tendencies of kinetic energy distribution surface features can be clearly quantified, and the fuzzy performance mechanism discipline between the performance results of surface fitting algorithms, the spatial features of turbulence kinetic energy distribution surface, and their respective environmental parameter conditions can be quantitatively analyzed in detail, which results in the acquirement of final conclusions concerning the inherent turbulence kinetic energy distribution performance mechanism and its mathematical relation. A further turbulence energy quantitative study can b...|$|R
40|$|Detection {{and removal}} of rain streaks from videos has {{recently}} {{become a great}} and challenging topic of research. This paper discusses a new technique {{for the removal of}} rain from videos using the temporal-spatial statistical properties. For this the temporal statistical properties of the pixels affected by rain are made use of, and then an efficient and easy algorithm is implemented which takes care of the effective removal of rain from videos. This technique works very well for videos with still and moving backgrounds involving moving objects with a fixed camera position. For the videos which involve the motion of the camera, the technique works well for a small rate of change of background in the camera frames. Our algorithm does not use variable and <b>conditional</b> <b>parameters</b> like the shape, size, velocity, and spatio-temporal physical model of raindrops, and camera’s parameters like the aperture, focal length, and exposure time. The test results quantitatively and qualitatively illustrate that the performance of our algorithm is quite efficient in comparison to the previously existing algorithms which are state of the art techniques used for the purpose of removing rain from videos...|$|R
40|$|We study {{asymptotic}} {{properties of}} some (essentially <b>conditional</b> least squares) <b>parameter</b> estimators for the subcritical Heston model based on discrete time observations derived from conditional least squares estimators of some modified parameters. Comment: 22 pages, {{mistakes in the}} proof of Theorem 3. 2 are correcte...|$|R
40|$|Approved {{for public}} release; {{distribution}} unlimited. Most self-propelled vessels moving on, or under, the ocean surface, contain rotating machinery that radiate finite bandwidth signals into the water. Empirical {{evidence suggests that}} the signal bandwidth estimated with a far field receiver is often greater than expected. This thesis investigates the use of an acoustic propagation model to predict the received bandwidth of sinusoidal signals when both the source and the receiver are in motion. The bandwidth parameter is calculated from the multi-frequency transmission loss (TL) predicted with a re-written version of K. Smith's Monterey-Miami Parabolic Equation (MMPE) model, including both receiver and source motion. The results for various propagation environments allow exploration of the characteristics of received bandwidth, predicted from sources on the surface or at depth. The dependency of aggregate bandwidth upon <b>conditional</b> <b>parameters</b> such as range, depth, and normalized pressure are evaluated. In addition to modeling results, this thesis documents a new implementation of the MMPE model, for narrowband signals using only the MATLAB programming language. A MATLAB version has the inherent advantages of increased flexibility and portability. A MATLAB implementation of a range dependent ray trace function based upon a Runge-Kutta integration of the eikonal equations is also presented. Naval Postgraduate School author (civilian) ...|$|R
