5|10000|Public
40|$|This study {{analyses}} {{the factors}} affecting investment decisions concerning end-user support (EUS), which {{is even more}} complex than that of <b>centralised</b> <b>data</b> <b>processing</b> (DP). The alternatives of proactive and reactive end-user support (EUS) in a modern LAN environment are compared by analysing the failure risk of one specific component (HUB) of the LAN. The additional preventive actions and management costs are considered as well. Both proactive and reactive EUS are analysed in terms of investment and risk management. Arguments for and against enhanced support capability and a fault tolerant system structure are sought for optimising risk/spending levels. The reliability theory with its demand patterns demonstrated by electronic components {{has been used in}} the spirit of inventory-production theory in order to quantify the risk both with deterministic and random failure intensity parameters. Exact Bayesian tests have been developed in order to evaluate the two vendor MTBF figures, only th [...] ...|$|E
40|$|Recent {{research}} in sensor networks {{has made it}} possible to deploy networks of sensors with significant local processing. These sensor networks are revolutionising informa-tion collection and processing in many different environments. Often the amount of local data produced by these devices, and their sheer number, makes <b>centralised</b> <b>data</b> <b>processing</b> infeasible. Smart camera networks represent a particular challenge in this regard, partly because of the amount of data produced by each camera, but also because many high level vision algorithms require data from more than one camera. Many distributed algorithms exist that work locally to produce results from a collection of nodes, but as this number grows the algorithm's performance is quickly crippled by the resulting exponential increase in communication overhead. This thesis examines the limits this puts on peer-to-peer cooperation between nodes, and demon-strates how for large networks these can only be circumvented by locally formed or-ganisations of nodes. A local group forming protocol is described that provides a method for nodes to create a bottom-up organisation based purely on local condi...|$|E
40|$|In {{this paper}} we present an {{experience}} of migrating a legacy academic information system to a new system which has an entirely different characteristic. To be successful, project managers in such projects will need a comprehensive view of all projects aspects: in particular stakeholder participation and commitment {{with regards to the}} software development. In our work, the legacy system was a <b>centralised</b> <b>data</b> <b>processing</b> system having distributed and redundant data representation. The new system supports distributed transactions, and has a single centralised database and supports a superset of responsibilities (data processing and monitoring). While the new application has been completely redesigned and rewritten, the huge volume of legacy data had to be restructured, integrated and conserved. This was particularly important, since academic data has a long period of validity. The new system has to handle many concurrent users and has strong security requirements. Additionally, a short implementation phase with easy maintainability was demanded resulting in a short learning curve for the end-users. We will share process and methodology that has been undertaken, and conclude that top level management commitment, business process redefinition, application development & data migration strategy as well as user involvement are the key success factors of new system implementation...|$|E
40|$|Conventional Wireless Sensor Networks (WSNs) usually adopt a <b>centralised</b> {{approach}} to <b>data</b> <b>processing</b> and interpretation {{primarily due to}} the limited computation and energy resources available on sensor nodes. These constraints limits the potential of intelligent techniques to data analy- sis and such activities on the centralised host. In contrast, Intelligent WSNs (iWSNs) will be significantly more powerful thus enabling the harnessing of intelligent techniques for diverse purposes. One such purpose is the practical realisation of smart environments, and facilitating mobility and interaction with the inhabitants of such environments. As a step in this direction, this paper presents the design of an iWSN sensor node platform that enables the hosting of lightweight Artificial Intelligence (AI) frameworks whilst enabling the ubiquitous energy constraints be quantified, mitigated and managed...|$|R
40|$|The witness seminar ”Högre dataubildningar i Sverige i ett historiskt perspektiv” [Higher Education in the Computers Sciences in Sweden from a Historic Perspective] {{was held}} at Tekniska museet [The National Museum of Science and Technology] in Stockholm on 24 January 2008 and was led by Ingemar Dahlstrand. Different aspects of the {{development}} of higher education within the computer area were discussed and debated. The witness seminar focused on the expansion of the subject area that grew from Numerical Analysis, Administrative <b>Data</b> <b>Processing</b> and the area that in the end became Computer Science. The experiences from the different universities were compared and debated. The development of the subject area of computers took various paths at the universities. The experiences of working within a newly established subject area were discussed. These included, among other aspects, the relation to the industry. Also the experiences of having to rely on a <b>centralised</b> system with <b>Data</b> <b>Processing</b> Centres in order to use computers were mentioned. QC 20120507 </p...|$|R
40|$|With an {{increasing}} interest in Electric Vehicles (EVs), {{it is essential}} to understand howEV charging could impact demand on the Electricity Grid. Existing approaches used to achieve this make use of a <b>centralised</b> <b>data</b> collection mechanism - which often is agnostic of demand variation in a given geographical area. We present an in-transit <b>data</b> <b>processing</b> architecture that is more efficient and can aggregate a variety of different types of data. A model using Reference nets has been developed and evaluated. Our focus in this paper is primarily to introduce requirements for such an architecture...|$|R
40|$|Recent {{research}} in sensor networks {{has made it}} possible to deploy networks of sensors with significant local processing. These sensor networks are revolutionising information collection and processing in many different environments. Often the amount of local data produced by these devices, and their sheer number, makes <b>centralised</b> <b>data</b> <b>processing</b> infeasible. Smart camera networks represent a particular challenge in this regard, partly because of the amount of data produced by each camera, but also because many high level vision algorithms require data from more than one camera. Many distributed algorithms exist that work locally to produce results from a collection of nodes, but as this number grows the algorithm's performance is quickly crippled by the resulting exponential increase in communication overhead. This thesis examines the limits this puts on peer-to-peer cooperation between nodes, and demonstrates how for large networks these can only be circumvented by locally formed organisations of nodes. A local group forming protocol is described that provides a method for nodes to create a bottom-up organisation based purely on local conditions. This allows the formation of a dynamic information network of cooperating nodes, in which a distributed algorithm can organise the communications of its nodes using purely local knowledge to maintain its global network performance. (cont.) Building on recent work using SIFT feature detection, this protocol is demonstrated in a network of smart cameras. Local groups with shared views are established, which allow each camera to locally determine their relative position with others in the network. The result partitions the network into groups of cameras with known visual relationships, which can then be used for further analysis. by Jacky Mallett. Thesis (Ph. D.) [...] Massachusetts Institute of Technology, School of Architecture and Planning, Program in Media Arts and Sciences, 2006. Includes bibliographical references (p. 103 - 111) ...|$|E
40|$|Background: In Germany, {{government}} departments and nongovernmental organisations {{are active in}} efforts to promote and support breastfeeding, {{in order to increase}} (exclusive) breastfeeding duration. However, it is difficult to compare and interpret data on breastfeeding behaviour and changes over time because methods and definitions used to conduct studies on breastfeeding differ. This also delayed implementation of effective means of breastfeeding support. Objective: In view of this, the primary aim of this dissertation was to gain insight into the current situation on breastfeeding (initiation, rates, and duration) in Berlin and to analyse the impact of individual and hospital factors on breastfeeding behaviour and duration. In addition, the practicability and acceptance of the methods used to collect data on breastfeeding in hospitals, birth centres and within the child health screening programme in paediatric practices was assessed. For this purpose, three studies were undertaken in Berlin hospitals, birth centres, and paediatric practices from 2004 to 2006. Methods: A longitudinal cohort study with mother-child-pairs from two maternity units was conducted using questionnaires to collect quantitative and qualitative data on breastfeeding behaviour from the first days after birth until 6 months of age and to identify factors associated with breastfeeding at 4 and 6 months. Furthermore, quantitative data on breastfeeding rates and intensity were collected from mother-child-pairs who participated in the child health screening programme (U 3 to U 6) in 116 paediatric practices in Berlin. Finally, a three-month observational study was conducted in 19 maternity units and 4 birth centres, using a short questionnaire to collect quantitative data on the timing of first suckling and breastfeeding from mother–child pairs on the day of discharge. Methods were assessed with respect to practicability and acceptance for monitoring purposes. Results: In total, about 5, 000 mother–child pairs were included in the three studies. Of these, 807 women were interviewed to determine factors associated with breastfeeding duration. Data indicate a breastfeeding rate of 96 % at discharge. Although supplementary feeding was common within the first days after birth (55 %), the rate of exclusive breastfeeding increased up to and within the first week after hospital discharge (65 – 73 %). Data also show that infants born in birth centres were significantly more likely than in hospitals to be put to their mothers’ breast within the first hour after birth (P < 0. 05). Also, the prevalence of total (P < 0. 05) or exclusive (P < 0. 01) breastfeeding at discharge was higher in birth centres than in all other hospital categories. Hospitals’ breastfeeding policies (i. e. following the ‘Ten Steps to Successful Breastfeeding’) were not associated with a higher prevalence of early first suckling and any breastfeeding at discharge, but with exclusivity of breastfeeding (P < 0, 001). Two months after birth, 73. 1 % of infants were still breastfed, 49. 27 % of them exclusively. After 4 months, breastfeeding rates dropped to 64. 1 % (44. 2 % exclusively) and after 6 months to 57 % (16. 7 % exclusively). Breastfeeding duration of 4 months was significantly associated with maternal education interdependent with age, breastfeeding experience, partner’s attitude towards breastfeeding, and timing of first suckling after birth. Factors associated with breastfeeding up to 6 months were again maternal education mutually dependent with age, family status, timing of first suckling, and children’s birth weight. The main influencing factor on breastfeeding duration was, however, the mothers’ intention to breastfeed. Conclusions: Breastfeeding initiation rates are satisfactorily high in Berlin. Similar to results of previous studies, rates decreased rapidly within two months after birth, although declines were more gradual than {{at the end of the}} 1990 s. Also, (exclusive) breastfeeding rates at 4 and 6 months were higher in comparison with results of previous studies. However, it would be necessary to conduct a representative survey to confirm these results for Germany. Breastfeeding problems and obstacles identified in Berlin as well as factors associated with breastfeeding patterns are comparable with those found in other studies. With regard to practicability and acceptance of the methodology, it can be concluded that a short questionnaire was well accepted and suitable for the collection of reliable data on breastfeeding initiation in maternity units and birth centres. This method is thus deemed feasible and useful to be applied in a systematic nationwide monitoring on breastfeeding initiation. The child health screening programme U 3 to U 6 was, however, only suitable to a limited extent for collecting data on breastfeeding for monitoring purposes. Within this programme, reliable data could only be generated if the following requirements were met: Firstly, data collection would have to become an inherent part of the programme; secondly, regular participation of mother–child pairs in the programme would be essential, and thirdly, the accompanying booklets (Gelbes Heft) would have to be used for documentation. In any case, regular and <b>centralised</b> <b>data</b> <b>processing</b> and analysis is essential...|$|E
5000|$|A {{distributed}} ledger (also called shared ledger) {{is a consensus}} of replicated, shared, and synchronized digital data geographically spread across multiple sites, countries, or institutions. [...] There is no central administrator or <b>centralised</b> <b>data</b> storage.|$|R
30|$|There are {{a diverse}} set of methods for {{collecting}} monitoring state from cloud deployments. Many tools still rely upon fully <b>centralised</b> <b>data</b> collection, {{while others have}} extended this design {{through the use of}} trees and other forms of overlay.|$|R
5000|$|To build, provide, maintain, and {{optimise}} an (electronic) databank with <b>centralised</b> <b>data</b> on HLA (human leucocyte antigen) phenotypes {{and other}} relevant data of volunteer stem cell donors and cryopreserved cord blood products and make these data {{accessible to the}} physicians, search coordinators, and other parties worldwide who search for a potential match for their patient; ...|$|R
50|$|Palo is {{a memory}} {{resident}} multidimensional (online analytical processing (OLAP) or multidimensional online analytical processing (MOLAP)) database server and typically {{used as a}} business intelligence tool for controlling and budgeting purposes with spreadsheet software acting as the user interface. Beyond the multidimensional data concept, Palo enables multiple users to share one <b>centralised</b> <b>data</b> storage (single version of the truth).|$|R
40|$|<b>Data</b> <b>Processing</b> {{discusses}} the principles, practices, and associated tools in <b>data</b> <b>processing.</b> The book {{is comprised of}} 17 chapters that are organized into three parts. The first part covers the characteristics, systems, and methods of <b>data</b> <b>processing.</b> Part 2 deals with the <b>data</b> <b>processing</b> practice; this part {{discusses the}} data input, output, and storage. The last part discusses topics related to systems and software in <b>data</b> <b>processing,</b> which include checks and controls, computer language and programs, and program elements and structures. The text will be useful to practitioners of computer-re...|$|R
40|$|This paper {{proposes a}} {{conceptual}} matrix model with algorithms for biological <b>data</b> <b>processing.</b> The required elements for constructing a matrix model are discussed. The representative matrix-based methods and algorithms which have potentials in biological <b>data</b> <b>processing</b> are presented / proposed. Some application {{cases of the}} model in biological <b>data</b> <b>processing</b> are studied, which show the applicability of this model in various kinds of biological <b>data</b> <b>processing.</b> This conceptual model established a framework within which biological <b>data</b> <b>processing</b> and mining could be conducted. The model is also heuristic to other applications. <br /...|$|R
30|$|Quality <b>data</b> <b>processing</b> defines {{configuration}} {{and processing}} parameters, which are utilized {{in the evaluation}} process of quality attributes. Examples of configuration parameters include location of binary or configuration files, and environmental execution parameters (e.g. number of CPU cores, size of memory). Processing parameters refer to input data, which should be provided to <b>data</b> <b>processing</b> executables (e.g. Spark streaming). Supported quality <b>data</b> <b>processing</b> defines processing, which can be performed with a specific <b>data</b> <b>processing</b> tool. Especially, it can be specified what kind of quality attributes for a data source can be evaluated with a specific <b>data</b> <b>processing</b> tool.|$|R
40|$|Abstract. UnifiedViews is an Extract-Transform-Load (ETL) frame-work {{that allows}} users – publishers, consumers, or analysts – to define, execute, monitor, debug, schedule, and share RDF <b>data</b> <b>processing</b> tasks. The <b>data</b> <b>processing</b> tasks may use custom plugins created by users. UnifiedViews differs from other ETL {{frameworks}} by natively supporting RDF data and ontologies. The practical demonstration of UnifiedViews {{at the conference}} will (1) clearly demonstrate how UnifiedViews helps RDF/Linked Data users with RDF <b>data</b> <b>processing</b> (2) and show the real instance of UnifiedViews with tens of <b>data</b> <b>processing</b> tasks and DPUs motivated by real <b>data</b> <b>processing</b> use cases. ...|$|R
40|$|Significant {{numbers of}} {{physicians}} are using <b>data</b> <b>processing</b> services {{and a large}} number of firms are offering an increasing variety of services. This paper quantifies user dissatisfaction with office practice <b>data</b> <b>processing</b> systems and analyzes factors affecting dissatisfaction in large group practices. Based on this analysis, a proposal is made for a more structured approach to obtaining <b>data</b> <b>processing</b> services in order to lower the risks and increase satisfaction with <b>data</b> <b>processing...</b>|$|R
40|$|DE 102007026480 A 1 UPAB: 20081222 NOVELTY - The method {{involves}} attaching mobile <b>data</b> <b>processing</b> {{units to}} collection containers contained with products to be commissioned. The <b>data</b> <b>processing</b> units command over micro-controllers, local memory, sensor interfaces and wireless communication devices. The mobile <b>data</b> <b>processing</b> unit is addressed on the <b>data</b> <b>processing</b> {{system in a}} commissioning controlling system to identify the current collection containers. The numbers of products, which can be inferred, are indicated. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for the execution of a method for the commissioning of goods. USE - Method for the commissioning of goods. ADVANTAGE - The method involves attaching mobile <b>data</b> <b>processing</b> units to collection containers contained with products to be commissioned, where the mobile <b>data</b> <b>processing</b> unit is addressed on the <b>data</b> <b>processing</b> system in a commissioning controlling system to identify the current collection containers, and hence ensures to simplify the commissioning work by retrofitting the existing stock option and shelving systems...|$|R
5000|$|Pure {{communications}} and pure <b>data</b> <b>processing</b> {{have very different}} characteristics that led to different policy results. The markets that the technology existed on assisted the FCC make its policy decisions. [...] "The pure <b>data</b> <b>processing</b> market was viewed as an innovative, competitive market with low barriers to entry and little chance of monopolization." [...] The FCC established that no additional regulation or safeguards where required for the pure <b>data</b> <b>processing</b> market. The pure communications market {{on the other hand}} was being managed by an incumbent monopoly. The FCC had four concerns about the incumbent telephone companies which were: [...] "the sale of <b>data</b> <b>processing</b> services by carriers should not hurt the provision of common carrier services, the costs of such <b>data</b> <b>processing</b> services should not be passed on to telephone rate payers, revenues derived from common carrier services should not be used to cross subsidize <b>data</b> <b>processing</b> services, and the furnishing of such <b>data</b> <b>processing</b> services by carriers should not hurt the competitive computer market." ...|$|R
50|$|The IEA <b>Data</b> <b>Processing</b> and Research Center (DPC) is the <b>data</b> <b>processing</b> and {{research}} department of IEA, located in Hamburg, Germany.|$|R
40|$|Moore's law {{was first}} {{postulated}} in 1968, and it loosely {{says that the}} cost of making calculations on a computer falls by 50 % each year. Securities markets are, in essence, a form of <b>data</b> <b>processing.</b> Consequently, Moore’s law has driven important changes in those markets over the past forty years. Faster <b>data</b> <b>processing</b> was essential for major changes in securities trading. Increased turnover of portfolios was a result of faster <b>data</b> <b>processing.</b> Consequently, the criticism of that turnover may be misplaced. The effectiveness of regulatory changes, such as the lowering of brokerage commissions and the reduction in bid ask spreads, depended on reduced <b>data</b> <b>processing</b> costs, that is on Moore's Law. Deregulation of brokerage commissions could not have reduced rates by as much as it did if we had not had decreasing costs of <b>data</b> <b>processing.</b> The reduction in bid ask spreads which followed decimalization of securities quotes depended on improved <b>data</b> <b>processing.</b> Continued reductions in <b>data</b> <b>processing</b> costs will require a new regulatory approach. Regulators should consider the improvements in <b>data</b> <b>processing</b> and <b>data</b> transmission when they establish capital requirements and haircuts. ...|$|R
40|$|<b>Data</b> <b>processing</b> complexity, partitionability, {{locality}} and provenance play {{a crucial}} role in the effectiveness of distributed <b>data</b> <b>processing.</b> Dynamics in <b>data</b> <b>processing</b> necessitates effective modeling which allows the understanding and reasoning of the fluidity of <b>data</b> <b>processing.</b> Through virtualization, resources have become scattered, heterogeneous, and dynamic in performance and networking. In this paper, we propose a new distributed <b>data</b> <b>processing</b> model based on automata where <b>data</b> <b>processing</b> is modeled as state transformations. This approach falls within a category of declarative concurrent paradigms which are fundamentally different than imperative approaches in that communication and function order are not explicitly modeled. This allows an abstraction of concurrency and thus suited for distributed systems. Automata give us a way to formally describe <b>data</b> <b>processing</b> independent from underlying processes while also providing routing information to route data based on its current state in a P 2 P fashion around networks of distributed processing nodes. Through an implementation, named Pumpkin, of the model we capture the automata schema and routing table into a <b>data</b> <b>processing</b> protocol and show how globally distributed resources can be brought together in a collaborative way to form a <b>processing</b> plane where <b>data</b> objects are self-routable on the plane...|$|R
40|$|This paper {{reviews the}} {{historical}} development of <b>data</b> <b>processing,</b> discerning three approximate decades of distinct evolutionary cycles. Each cycle {{is seen as}} forking, two contrasting styles of <b>data</b> <b>processing</b> forming and separating during the cycle. On this basis, a classification of the major general areas of <b>data</b> <b>processing</b> is suggested. For each decade, characteristic aspects are discussed and both the lines of development are described. Finally, some observations regarding the present decade and its requirements are given, and some predictions relating to the next decade and its prerequisites are made. DESCRIPTORS: Classification of <b>data</b> <b>processing.</b> Evolution of <b>data</b> <b>processing.</b> Philosophical implications. Computing milieu. Terminology. CR CATEGORIES: 1. 2, 1. 3, 2. ...|$|R
40|$|A {{system for}} {{assessing}} vestibulo-ocular function includes a motion sensor system adapted to be coupled to a user's head; a <b>data</b> <b>processing</b> system configured {{to communicate with}} the motion sensor system to receive the head-motion signals; a visual display system configured {{to communicate with the}} <b>data</b> <b>processing</b> system to receive image signals from the <b>data</b> <b>processing</b> system; and a gain control device arranged to be operated by the user and to communicate gain adjustment signals to the <b>data</b> <b>processing</b> system...|$|R
5000|$|Mivar-based {{technology}} of <b>data</b> <b>processing</b> {{is a method}} of creating logical inference system or automated algorithm construction from modules, services or procedures {{on the basis of}} active trained mivar network of rules with the linear computational complexity. Mivar-based {{technology of}} <b>data</b> <b>processing</b> is designed for <b>data</b> <b>processing</b> including logical inference, computational procedures and services.|$|R
5000|$|Roger Lee Sisson (June 24, 1926 [...] - [...] January 22, 1992) was {{an early}} <b>data</b> <b>processing</b> pioneer. Sisson worked on Project Whirlwind while a {{graduate}} student at MIT, co-founded the first consulting firm devoted to electronic <b>data</b> <b>processing,</b> and published a number of the earliest books and periodicals on computers and <b>data</b> <b>processing.</b>|$|R
40|$|Fourier {{transform}} spectrometry {{is a type}} {{of novel}} information obtaining technology, which integrated the functions of imaging and spectra, but the data that the instrument acquired is the interference data of the target, which is an intermediate data and couldn&# 39;t be used directly, so <b>data</b> <b>processing</b> must be adopted for the successful application of the interferometric data. In the present paper, <b>data</b> <b>processing</b> techniques are divided into two classes: general-purpose and special-type. First, the advance in universal interferometric <b>data</b> <b>processing</b> technique is introduced, then the special-type interferometric data extracting method and <b>data</b> <b>processing</b> technique is illustrated according to the classification of Fourier transform spectroscopy. Finally, the trends of interferogram <b>data</b> <b>processing</b> technique are discussed...|$|R
50|$|The term <b>Data</b> <b>Processing</b> (DP) {{has also}} been used {{previously}} {{to refer to a}} department within an organization responsible for the operation of <b>data</b> <b>processing</b> applications.|$|R
40|$|A {{field survey}} was {{conducted}} to study the factors {{associated with the use}} of <b>data</b> <b>processing</b> charge-back information in organizations. The aim of this research was to identify the organizational and budgetary characteristics associated with how the output of a chargeback system is used by user-managers to control their <b>data</b> <b>processing</b> costs. It was found that involvement in budget preparation, accountability for meeting the <b>data</b> <b>processing</b> budget, and cost variability of the charges were the- most important factors to consider when designing <b>data</b> <b>processing</b> chargeback systems...|$|R
40|$|Scientific {{workflow}} {{systems have}} become a necessary tool for many applications, enabling the composition and execution of complex analysis. CO(2) flux data observed by eddy covariance technique is large in quantity and the procedure of flux data is complex, scientific workflow technique plays {{a very important role}} in the sharing, reusing and automatic calculation of flux <b>data</b> <b>processing</b> method. In this paper, we discuss the feasibility and validity of applying scientific workflow technique to flux <b>data</b> <b>processing</b> and make a tentative approach to construct a scientific workflow system for CO(2) flux <b>data</b> <b>processing</b> by taking Kepler scientific workflow system as the development platform. CO(2) flux data of Changbai Mountain in 2003 is used to verify the scientific workflow system. The results show that scientific workflow system for CO(2) flux <b>data</b> <b>processing</b> can solve many problems of too much multifarious calculation, inconsistent development platform and complicated procedure in flux <b>data</b> <b>processing.</b> This approach indicates that the scientific workflow system applied to CO(2) flux <b>data</b> <b>processing</b> can provide an automatic calculation platform for flux <b>data</b> <b>processing</b> and prompt the communication and sharing of international flux <b>data</b> <b>processing</b> method, which make it easier for scientists to focus on their research and not computation management...|$|R
40|$|In this {{research}} aims {{to change the}} personnel staffing <b>data</b> <b>processing,</b> which is stillperformed in the conventional / manual, {{with the help of}} computer hardware for fastdata processing, the company makes computerized <b>data</b> <b>processing</b> personnel in orderto accelerate the process of doing <b>data</b> <b>processing</b> personnel and presenting reports -reports / information to the right, quickly and accurately...|$|R
40|$|In {{carrying}} out real work practices and preparing this paper, the authors obtain abroader knowledge about <b>data</b> <b>processing</b> {{system on the}} computerized system and cancompare with the <b>data</b> <b>processing</b> system manually. Given the <b>data</b> <b>processing</b> systemwith a computerized warehouse section can then be expected to facilitate thewarehouse stock control of goods entering derta out systematically and efficiently aspossible...|$|R
40|$|PREFACE Many {{books and}} {{articles}} generally recognize that management personnel other {{than those in the}} <b>data</b> <b>processing</b> installation need to be oriented towards <b>data</b> <b>processing</b> if effective utilization of these costly computers is to evolve. Present emphasis in the U. S. Marine Corps is on training of <b>data</b> <b>processing</b> personnel with limited orienta-tion or training of other managerial personnel in the capabilities and limitations of computers. The author carries an additional military occupational specialty as a <b>Data</b> <b>Processing</b> Officer and has been involved with Marine Corps <b>data</b> <b>processing</b> for a number of years. He found this lack of training of other officers a perplexing problem to him in the conduct of everyday tasks. It soon became evident to him that all officers dealing with him should have adequate training in <b>data</b> <b>processing.</b> This problem has been recognized by some Marine Corps officials, but to date a standard syste...|$|R
5000|$|IT audits {{are also}} known as [...] "automated <b>data</b> <b>processing</b> (ADP) audits" [...] and [...] "computer audits". They were {{formerly}} called [...] "electronic <b>data</b> <b>processing</b> (EDP) audits".|$|R
5000|$|Smagorinsky, J., 1965: Remarks on <b>data</b> <b>processing</b> in meteorology. In, Proceedings of the WMO/IUGG Symposium on Meteorological <b>Data</b> <b>Processing,</b> Brussels, Belgium, WMO Technical Note 73, pp. 1-2.|$|R
40|$|The {{tremendous}} impact of electronic <b>data</b> <b>processing</b> {{on the lives}} of the American people is being felt in every segment of our economy. Regardless of the area of activity, be it business, education, government, industry, or the service areas, electronic <b>data</b> <b>processing</b> is performing an increasingly· important function. The volume of paper work, the magnitude of government reports, and the demands of management for information, have created mammoth challenges for electronic <b>data</b> <b>processing.</b> These challenges are complex. However, electronic <b>data</b> <b>processing</b> equipment and personnel are making favorable gains on the problems posed by these challenges...|$|R
