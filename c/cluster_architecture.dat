223|568|Public
25|$|Computer {{clustering}} {{relies on}} a centralized management approach which makes the nodes available as orchestrated shared servers. It is distinct from other approaches such as peer to peer or grid computing which also use many nodes, but with a far more distributed nature. By the 21st century, the TOP500 organization's semiannual list of the 500 fastest supercomputers often includes many clusters, e.g. the world's fastest in 2011, the K computer with a distributed memory, <b>cluster</b> <b>architecture.</b>|$|E
25|$|When a {{large number}} of local semi-independent {{computing}} nodes are used (e.g. in a <b>cluster</b> <b>architecture)</b> the speed and flexibility of the interconnect becomes very important. Modern supercomputers have taken different approaches to address this issue, e.g. Tianhe-1 uses a proprietary high-speed network based on the Infiniband QDR, enhanced with FeiTeng-1000 CPUs. On the other hand, the Blue Gene/L system uses a three-dimensional torus interconnect with auxiliary networks for global communications. In this approach each node is connected to its six nearest neighbors. A similar torus was used by the Cray T3E.|$|E
25|$|The K {{computer}} is a water-cooled, homogeneous processor, distributed memory {{system with a}} <b>cluster</b> <b>architecture.</b> It uses more than 80,000 SPARC64 VIIIfx processors, each with eight cores, {{for a total of}} over 700,000 coresalmost twice as many as any other system. It comprises more than 800 cabinets, each with 96 computing nodes (each with 16GB of memory), and 6 I/O nodes. Although it is more powerful than the next five systems on the TOP500 list combined, at 824.56MFLOPS/W it has the lowest power to performance ratio of any current major supercomputer system. The follow up system for the K computer, called the PRIMEHPC FX10 uses the same six-dimensional torus interconnect, but still only one processor per node.|$|E
40|$|Recently, {{extensive}} research {{efforts have been}} devoted {{to the design of}} clustering algorithms to organize all the hosts in a mobile ad hoc network into a <b>clustering</b> <b>architecture.</b> However, due to the dynamic nature of the mobile hosts, their association with and dissociation from clusters disturb the stability of the network, making reconfiguration of cluster heads unavoidable. Re-computation of cluster heads and frequent information exchange among the participating hosts will suffer high computation overheads. Therefore, it is obvious that a more stable <b>clustering</b> <b>architecture</b> will directly lead to the performance improvement of the whole network. In this paper, we will propose an efficient clustering algorithm that can establish a stable <b>clustering</b> <b>architecture</b> by keeping a host with weak battery power from being elected as a cluster head. Computer simulations show that the <b>clustering</b> <b>architectures</b> generated by our clustering algorithm are more stable than those generated by other clustering algorithms...|$|R
40|$|<b>Clustered</b> <b>architecture</b> {{processors}} are preferred for {{embedded systems}} because centralized register file architectures scale poorly {{in terms of}} clock rate, chip area, and power consumption. Scheduling for <b>clustered</b> <b>architectures</b> involves spatial concerns (where to schedule) as well as temporal concerns (when to schedule). Various clustered VLIW configurations, connectivity types, and inter-cluster communication models present different performance trade-offs to a scheduler. The scheduler is responsible for resolving the conflicting requirements of exploiting the parallelism offered by the hardware and limiting the communication among clusters to achieve better performance. In this paper, we describe our experience with developing a pragmatic scheme and also a generic graph-matching-based framework for cluster scheduling based on a generic and realistic clustered machine model. The proposed scheme effectively utilizes the exact knowledge of available communication slots, functional units, and load on different clusters as well as future resource and communication requirements known only at schedule time. The proposed graph-matching-based framework for cluster scheduling resolves the phase-ordering and fixed-ordering problem associated with earlier schemes for scheduling <b>clustered</b> VLIW <b>architectures.</b> The experimental evaluation {{in the context of}} a state-of-art commercial <b>clustered</b> <b>architecture</b> (using real-world benchmark programs) reveals a significant performance improvement over the earlier proposals, which were mostly evaluated using compiled simulation of hypothetical <b>clustered</b> <b>architectures.</b> Our results clearly highlight the importance of considering the peculiarities of commercial <b>clustered</b> <b>architectures</b> and the hard-nosed performance measurement...|$|R
40|$|<b>Clustered</b> <b>architecture</b> {{processors}} are preferred for {{embedded systems}} because centralized register file architectures scale poorly {{in terms of}} clock rate, chip area, and power consumption. Although clustering helps by improving clock speed, reducing energy consumption of the logic, and making design simpler, it introduces extra overheads by way of inter-cluster communication. This communication happens over long global wires having high load capacitance which leads to delay in execution and significantly high energy consumption. Technological advancements permit design {{of a variety of}} <b>clustered</b> <b>architectures</b> by varying the degree of clustering and the type of interconnects. In this paper, we focus on exploring energy performance trade-offs in going from a unified VLIW architecture to different types of <b>clustered</b> VLIW <b>architectures.</b> We propose a new instruction scheduling algorithm that exploits scheduling slacks of instructions and communication slacks of data values together to achieve better energy-performance trade-offs for <b>clustered</b> <b>architectures.</b> Our instruction scheduling algorithm for <b>clustered</b> <b>architectures</b> with heterogeneous interconnect achieves 35 % and 40 % reduction in communication energy, whereas the overall energy-delay product improves by 4. 5 % and 6. 5 % respectively for 2 cluster and 4 cluster machines with marginal 1. 6 % and 1. 1 % increase in execution time. Our test bed uses the Trimaran compiler infrastructure...|$|R
5000|$|... #Caption: A {{high-level}} {{illustration of}} the CoreOS <b>cluster</b> <b>architecture</b> ...|$|E
50|$|Although {{a cluster}} may consist {{of just a}} few {{personal}} computers connected by a simple network, the <b>cluster</b> <b>architecture</b> may also be used to achieve very high levels of performance. The TOP500 organization's semiannual list of the 500 fastest supercomputers often includes many clusters, e.g. the world's fastest machine in 2011 was the K computer which has a distributed memory, <b>cluster</b> <b>architecture.</b>|$|E
50|$|Supports single {{instance}} configuration-less start or requires set {{of options}} {{that used to}} build correspondent network <b>cluster</b> <b>architecture.</b>|$|E
40|$|Abstract—Agent-Based Modeling and Simulation (ABMS) {{has emerged}} as a new and {{powerful}} technology for the analysis of natural and artificial complex systems. In this paper ABMS is exploited for the modeling and performance evaluation of Conventional, Clustered and Cooperative Content Distribution Network (CDN) <b>architectures.</b> <b>Clustered</b> and Cooperative <b>architectures</b> differ from Conventional architectures as surrogate servers can loosely (in the Cooperative architectures) or tightly (in the <b>Clustered</b> <b>architectures)</b> cooperate to provide the requested contents to users. The results obtained from the simulation phase show that the <b>Clustered</b> <b>architectures</b> allow for significant improvements of the main CDN performance indices (average user perceived latency, cache hit ratio, and CDN utility) with respect to Conventional and Cooperative architectures...|$|R
40|$|Recent works (1) {{show that}} delays {{introduced}} in the issue and bypass logic will become critical for wide issue superscalar processors. One of the proposed solutions is clustering the processor core. <b>Clustered</b> <b>architectures</b> benefit from a less complex partitioned processor core and thus, incur in less critical delays. In this paper, we propose a dynamic instruction steering logic for these <b>clustered</b> <b>architectures</b> that decides at decode time the cluster where each instruction is executed. The performance of <b>clustered</b> <b>architectures</b> depends on the intercluster communication overhead and the workload balance. We present a scheme that uses runtime information to optimize the trade-off between these figures. The evaluation shows that this scheme can achieve an average speed-up of 350 over a conventional 8 -way issue (4 int+ 4 fp) machine and that it outperforms other previous proposals, either static or dynamic...|$|R
30|$|The grid {{middleware}} that {{introduced the}} lowest overhead was OurGrid, followed by OAR/Cigri and Globus. The performance {{difference between the}} HPC <b>clusters</b> <b>architectures</b> is 1.5  %.|$|R
50|$|The G11 and G12, {{which is}} the codename for the {{extended}} wheelbase model, are the first passenger vehicles of BMW {{to be based on}} the modular BMW CLAR platform (<b>cluster</b> <b>architecture).</b>|$|E
50|$|The system {{maintains}} the <b>cluster</b> <b>architecture</b> with 245 PS702 nodes, each one with 16 cores in two 64-bit processors POWER7 (eight cores each) 3.0 GHz, 32 GB of RAM and 300 GB of local hard disk. Each core provides 18.38 Gflops.|$|E
50|$|One {{can view}} NUMA as a tightly coupled form of cluster computing. The {{addition}} of virtual memory paging to a <b>cluster</b> <b>architecture</b> can allow {{the implementation of}} NUMA entirely in software. However, the inter-node latency of software-based NUMA remains several orders of magnitude greater (slower) than that of hardware-based NUMA.|$|E
40|$|Abstract — <b>Clustered</b> <b>architectures</b> which {{intend to}} process data within a {{localized}} PE {{are one of the}} approaches to increase the performance under the difficulties of the wire delay problems. The performance of the <b>clustered</b> <b>architecture</b> depends on the implemented instruction steering scheme. Existing steering schemes insert inter-PE communications to achieve load balance among PEs. These insertions delay the executions of the dependent instructions and lead to the degradation of the performance. In this paper, we propose a novel instruction steering scheme, which gives priority to critical dependencies. The way to find out the critical dependencies is by observing the status of the source operands of an instruction. We evaluate the proposed scheme and compare it with the existing ones. The results show that the proposed scheme outperforms the existing schemes in terms of instruction per clock because of reductions of the critical inter-PE communications with superior load balance among the PEs. Index Terms — Instruction steering, <b>Clustered</b> <b>architecture,</b> Data dependence-based design, Instructionlevel parallelis...|$|R
40|$|Many {{embedded}} processors use clustering {{to scale}} up instruction level parallelism in a cost effective manner. In a <b>clustered</b> <b>architecture,</b> the registers and functional units are partitioned into smaller units and clusters communicate through register-to-register copy operations. Texas Instruments, for example, has a series of architectures for embedded processors which are <b>clustered.</b> Such an <b>architecture</b> places a heavier burden on the compiler, which must now assign instructions to clusters (spatial scheduling), assign instructions to cycles (temporal scheduling), and schedule copy operations to move data between clusters. We consider instruction scheduling of local blocks of code on <b>clustered</b> <b>architectures</b> to improve performance. Scheduling for space and time {{is known to be}} a hard problem. Previous work has proposed greedy approaches based on list scheduling to simultaneously perform spatial and temporal scheduling, and phased approaches based on first partitioning a block of code to do spatial assignment and then performing temporal scheduling. Greedy approaches risk making mistakes that are then costly to recover from and partitioning approaches suffer from the well-known phase ordering problem. In this paper, we present a constraint programming approach for scheduling instructions on <b>clustered</b> <b>architectures.</b> We employ a problem decomposition technique that solves spatial and temporal scheduling in an integrated manner. We analyze the effect of different hardware parameters—such as the number of clusters, issue-width and inter-cluster communication cost—on applicatio...|$|R
40|$|TACO (Topologies and Collections) is a template-based object {{platform}} that strongly supports distributed data-parallel programming {{by means of}} distributed object groups and collective operations. This paper introduces TACO'S basic concepts and provides a performance analysis for collective operations on various <b>cluster</b> <b>architectures</b> with several different networks...|$|R
50|$|Although SGI {{continued}} to market Itanium-based machines, its more recent machines {{were based on}} the Intel Xeon processor. The first Altix XE systems were relatively low-end machines, but by December 2006 the XE systems were more capable than the Itanium machines by some measures (e.g., power consumption in FLOPS/W, density in FLOPS/m3, cost/FLOPS). The XE1200 and XE1300 servers used a <b>cluster</b> <b>architecture.</b> This was a departure from the pure NUMA architectures of the earlier Itanium and MIPS servers.|$|E
50|$|Computer {{clustering}} {{relies on}} a centralized management approach which makes the nodes available as orchestrated shared servers. It is distinct from other approaches such as peer to peer or grid computing which also use many nodes, but with a far more distributed nature. By the 21st century, the TOP500 organization's semiannual list of the 500 fastest supercomputers often includes many clusters, e.g. the world's fastest in 2011, the K computer with a distributed memory, <b>cluster</b> <b>architecture.</b>|$|E
50|$|Since February 2010, IBM is {{offering}} a hardened appliance version (better: evolution) of SoFS, called Scale Out Network Attached Storage (SONAS). SONAS includes {{a subset of the}} described software stack plus a new management layer. Major differences are a strict hardware support matrix, integrated high-density disk storage, and a delivery model with a standard IBM product warranty. Technically, SONAS uses an internal Infiniband network to get low latency and high cluster throughput. The <b>cluster</b> <b>architecture</b> is derived from leading Top500 supercomputer designs.|$|E
40|$|ABSTRACT- Analysis of {{reliability}} and availability of web-based software system is proposed using fuzzy technique. The analysis is useful to predict reliability of a web-based software system {{on the basis of}} software failure and repair rates in application server software system, router software system and <b>clustered</b> <b>architecture</b> system. 1...|$|R
40|$|TACO (Topologies and Collections) is a {{template}} library that introduces the flavour of distributed data parallel processing {{by means of}} reusable topology classes and C++ templates. This paper introduces TACO's basic abstractions and provides a performance analysis for basic collective operations on various <b>cluster</b> <b>architectures</b> with several different networks...|$|R
40|$|Abstract — Increasing wire delays {{have become}} a serious problem for {{sophisticated}} VLSI designs. <b>Clustered</b> <b>architecture</b> offers a promising alternative to alleviate the problem. In the <b>clustered</b> <b>architecture,</b> the cache, register file and function units are all partitioned into clusters such that short CPU cycle time can be achieved. A key challenge is the arrangement of inter-cluster communication. In this paper, we present a novel algorithm for scheduling inter-cluster communication operations. Our algorithm achieves better register resource utilization than the previous methods. By judiciously putting the selected spilled variables into their corresponding consumer’s local cache, the costly crosscache transfer is minimized. Therefore, the distributed caches are used more efficiently and the register constraint can be satisfied without compromising the schedule performance. The experiments shows that our technique outperforms the existing clusteroriented schedulers. I...|$|R
50|$|When a {{large number}} of local semi-independent {{computing}} nodes are used (e.g. in a <b>cluster</b> <b>architecture)</b> the speed and flexibility of the interconnect becomes very important. Modern supercomputers have taken different approaches to address this issue, e.g. Tianhe-1 uses a proprietary high-speed network based on the Infiniband QDR, enhanced with FeiTeng-1000 CPUs. On the other hand, the Blue Gene/L system uses a three-dimensional torus interconnect with auxiliary networks for global communications. In this approach each node is connected to its six nearest neighbors. A similar torus was used by the Cray T3E.|$|E
50|$|The K {{computer}} is a water-cooled, homogeneous processor, distributed memory {{system with a}} <b>cluster</b> <b>architecture.</b> It uses more than 80,000 SPARC64 VIIIfx processors, each with eight cores, {{for a total of}} over 700,000 cores - almost twice as many as any other system. It comprises more than 800 cabinets, each with 96 computing nodes (each with 16 GB of memory), and 6 I/O nodes. Although it is more powerful than the next five systems on the TOP500 list combined, at 824.56 MFLOPS/W it has the lowest power to performance ratio of any current major supercomputer system. The follow up system for the K computer, called the PRIMEHPC FX10 uses the same six-dimensional torus interconnect, but still only one processor per node.|$|E
30|$|The {{proposed}} cluster {{algorithm is}} {{more suitable for}} the multi-level heterogeneous networks. For efficient comparison, the cluster level-k chosen in this architecture is 3. Compared to LEACH, SEP and DEEC, the proposed <b>cluster</b> <b>architecture</b> is remarkable in message delivery, stability and network lifetime. The proposed <b>cluster</b> <b>architecture</b> is well organized by establishing a single-hop communication within the cluster using link correlation along with the TDMA time slot. On the other hand, a multi-hop communication among cluster heads is well controlled by network coding. This <b>cluster</b> <b>architecture</b> works well for medium-sized WSN. Since the heterogeneous nodes are mostly chosen as cluster heads and by incorporating the energy consuming task on those nodes increase the energy efficiency of networks.|$|E
40|$|<b>Clustered</b> <b>architecture</b> {{processors}} are preferred for {{embedded systems}} because centralized register file architectures scale poorly {{in terms of}} clock rate, chip area, and power consumption. Scheduling for <b>clustered</b> <b>architectures</b> involves spatial concerns (where to schedule) as well as temporal concerns (when to schedule). Various clustered VLIW configurations, connectivity types, and inter-cluster communication models present different performance trade-offs to a scheduler. The scheduler is responsible for resolving the conflicting requirements of exploiting the parallelism offered by the hardware and limiting the communication among clusters to achieve better performance. Earlier proposals for cluster scheduling fall into two main categories, viz., phase-decoupled scheduling and phase-coupled scheduling and they focus on <b>clustered</b> <b>architectures</b> which provide inter-cluster communication by an explicit inter-cluster copy operation. However, modern commercial <b>clustered</b> <b>architectures</b> provide snooping capabilities (apart from the support for inter-cluster communication using an explicit MV operation) by allowing some of the functional units to read operands from the register file {{of some of the}} other clusters without any extra delay. The phase-decoupled approach of scheduling suffers from the well known phase-ordering problem which becomes severe for such a machine model (with snooping) because communication and resource constraints are tightly coupled and thus are exposed only during scheduling. Tight integration of communication and resource constraints further requires taking into account the resource and communication requirements of other instructions ready to be scheduled in the current cycle while binding an instruction, in order to carry out effective binding. However, earlier proposals on integrated scheduling consider instructions and clusters for binding using a fixed order and thus they show different widely varying performance characteristics in terms of execution time and code size. Other shortcomings of earlier integrated algorithms (that lead to suboptimal cluster scheduling decisions) are due to non-consideration of future communication (that may arise due to a binding) and functional unit binding. In this thesis, we propose a pragmatic scheme and also a generic graph matching based framework for cluster scheduling based on a generic and realistic clustered machine model. The proposed scheme effectively utilizes the exact knowledge of available communication slots, functional units, and load on different clusters as well as future resource and communication requirements known only at schedule time to attain significant performance improvement without code size penalty over earlier algorithms. The proposed graph matching based framework for cluster scheduling resolves the phase-ordering and fixed-ordering problem associated with scheduling on <b>clustered</b> VLIW <b>architectures.</b> The framework provides a mechanism to exploit the slack of instructions by dynamically varying the freedom available in scheduling an instruction and hence the cost of scheduling an instruction using different alternatives to reduce the inter-cluster communication. An experimental evaluation of the proposed framework and some of the earlier proposals is presented {{in the context of a}} state-of-art commercial <b>clustered</b> <b>architecture...</b>|$|R
40|$|Current <b>cluster</b> <b>architectures</b> {{provide the}} ideal {{environment}} to run federations of main-memory database systems (FMMDBs). In FMMDBs, data {{resides in the}} main memory of the federation servers. FMMDBs significantly improve performance by avoiding I/O during the execution of read operations. To maximize the performance of update transactions as well, some applications recur to deferred disk writes...|$|R
40|$|We {{consider}} parallelization {{strategies for}} the two most frequently used algorithms in numerical investigations of the Anderson model of localization, a paradigmatic model of disordered quantum systems. After {{a brief review of}} the physics of Anderson localization, we discuss the application of the Cullum-Willoughby implementation of the Lanczos diagonalization scheme and the transfer-matrix method to massively parallel <b>cluster</b> <b>architectures...</b>|$|R
40|$|Abstract:- During the {{competitive}} environment, how to sufficiently and efficiently utilize the computational resources under {{the consideration of}} cost had became an important action {{to be done by}} most practitioners. For the recent years, the PC <b>cluster</b> <b>architecture</b> had been applied into many applications, especial for the computational resources. In this study, we will introduce a case study in Taiwan owing the issue based on PC <b>cluster</b> <b>architecture</b> with MPI techniques. This case is an application of clustering technique to DNA analysis. Not only the hardware/software architecture of PC cluster had been constructed, but the clustering algorithm based on such <b>cluster</b> <b>architecture</b> was also proposed in this study. And, the rationality of the proposed architecture and algorithm can be demonstrated well in this study. Key-Words:- PC cluster, Self-organizing feature map neural network (SOMNN), computational resource, Message Passing Interface (MPI). 1...|$|E
40|$|Abstract:-. In this research, {{we propose}} a {{parallel}} processing algorithm {{that runs on}} <b>cluster</b> <b>architecture</b> suitable for prime number generation. The proposed approach was written using Message Passing Interface (MPI) and is meant to decrease computational cost and accelerate the prime number generation process. Several experimental results conducted using High Performance Linpack (HPL) benchmark are presented to demonstrate the viability of our work. The {{results suggest that the}} performance of our work is at par with other parallel algorithms. Key-Words:- Prime number generation, parallel processing, <b>cluster</b> <b>architecture,</b> MPI, primality test...|$|E
40|$|This paper {{describes}} {{the final results}} of {{an assessment of the}} scalability of an NT <b>Cluster</b> <b>architecture</b> for very large Telecommunications Call Detail Record (CDR) databases 1. The IBM DB 2 DBMS was used on top of four standard Windows NT multi-processor computers to investigate the data warehousing capabilities of such an off-the-shelf solution. First a description of the experiment case and experiment set up is given. Next the results of the experiments are described. The paper ends with a conclusion on the manageability, robustness and scalability of the NT <b>Cluster</b> <b>architecture</b> for very large CDR stores...|$|E
40|$|<b>Clustered</b> <b>architectures</b> which {{intend to}} process data within a {{localized}} PE {{are one of the}} approaches to in-crease the performance under the difficulties of the wire de-lay problems. The performance of <b>clustered</b> <b>architectures</b> depends on the amount of parallel execution of instructions and the amount of inter-PE communication to synchronize dependent instructions. In this paper, we propose an ar-rangement of PEs cooperating with the adjacent PEs by means of adding communication structures between the ad-jacent PEs in order to relax the inter-PE communication and workload imbalance in an effective manner. We evalu-ate the proposed configurations and compare them with the existing one so far considered. The results show that the proposed adjacent forwarding network configuration with the instruction steering scheme that concerns both the reg-ister fanout and available free register can achieve higher instructions per clock (IPC) with the small number of regis-ters per PE than the other configurations. 1...|$|R
40|$|The MPI AlltoAllv {{operation}} is generally {{based on a}} large number of point-to-point messages being sent by all the processes in the communication. This report aims to discover whether this process can be optimised for a <b>clustered</b> <b>architecture</b> by collating the data from all the processors within a node into a shared memory segment and then sending all the data for one node using a single message. This planned method has the potential to decrease overheads by greatly reducing the number of messages sent, and therefore diminishing latency, of the AlltoAllv call. We compare the performance of the planned AlltoAllv with that of the standard MPI AlltoAllv on a number of <b>clustered</b> <b>architectures,</b> and nd that for a standard benchmark the planned AlltoAllv can perform considerably better than the MPI version, but for a more realistic benchmark the performance benet is not as great. ...|$|R
40|$|International audienceThe {{technological}} evolution {{involves a}} higher number of physical defects in circuits after manufacturing. One of the future challenge {{is to find a}} way to use a maximum of defected manufactured circuits. In this paper, multiple techniques are proposed to avoid defects in the cluster local interconnect of a SRAM-based Mesh of Clusters FPGA. Using defect tolerance, area and timing metrics, two previous hardware redundancy strategies are evaluated on the Mesh of <b>Clusters</b> <b>architecture</b> : Fine Grain Redundancy (FGR) and Improved Fine Grain Redundancy (IFGR). We show that using these techniques on a cluster of a Mesh of <b>Clusters</b> <b>architecture</b> permits to tolerate 8 times more defects than on an industrial Mesh FPGA with a low area overhead (- 6 % for FGR and 22 % for IFGR) and a low increase of Critical Path Delay (CPD) (6 % for FGR and 2 % for IFGR). We also proposed three new redundancy strategies using spare resources : Distributed Feedbacks (DF) for crossbar down, Adapted Fine Grain Redundancy (AFGR) to avoid defective multiplexers and Upward Redundant Multiplexer (URM) for the crossbar up. Compared to the Mesh of <b>Clusters</b> <b>architecture</b> without defect tolerance techniques, the best trade off between defect tolerance (36. 4 %), area overhead (11. 56 %) and CPD (+ 7. 46 %) is obtained using AFGR. Using the other methods permits to considerably limit the area overhead (10. 4 % with URM) with a lesser number of defective elements bypassed (18 % max) ...|$|R
