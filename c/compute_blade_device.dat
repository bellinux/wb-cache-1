0|140|Public
50|$|The IP-83 and IP-85 <b>compute</b> <b>blades</b> {{supported}} Intel Xeon 5200 or 5400 Series processors, and the IP-95 <b>compute</b> <b>blade</b> supported Intel Xeon 5500 Series processors.|$|R
50|$|The {{diskless}} {{nature of}} the <b>Compute</b> <b>Blades</b> means personnel can quickly swap out a failed unit, and have reassign the failed <b>Compute</b> <b>Blade's</b> volumes to the replacement. This can facilitate increased uptime in a production environment.|$|R
5000|$|The <b>Compute</b> <b>Blades</b> are {{referred}} to as [...] "Compute Modules" [...] in Intel literature. The MFS5000SI <b>Compute</b> <b>Blade</b> uses up to two Intel Xeon 5100, 5200, 5300 or 5400 processors; and supports up to 32 GB of RAM, running at either 1066 MHz or 1333 MHz. The MFS5520VI <b>Compute</b> <b>Blade</b> uses up to two Intel Xeon 5500 or 5600 processors; and supports up to 192 GB of RAM running at 800 MHz, 1066 MHz or 1333 MHz (note that 1333 MHz is supported only with 8 GB or smaller DIMMs).|$|R
5000|$|Building on the modular {{expansion}} {{capabilities of}} the <b>compute</b> <b>blade,</b> the storage blade enabled customers to add up to eight 2.5" [...] SATA HDDs or four 3.5" [...] SATA HDDs to a separate, but physically connected, single-width blade add-on. This bolted on expansion took {{the place of the}} default blade cover and extended the blade unit to a two-width module. From a computational standpoint, the storage blade was no different from the <b>compute</b> <b>blade,</b> offering the same Intel processor options. Unlike the <b>compute</b> <b>blade,</b> PCIe expansion was not available in the storage blade, as the RAID card supporting the additional hard drives occupied this port.|$|R
5|$|June: An Itanium2 sets {{a record}} SPECfp2000 result of 2,801 in a Hitachi, Ltd. <b>Computing</b> <b>blade.</b>|$|R
50|$|Similar to {{the storage}} blade expansion, both {{workstation}} visualization and GPGPU blades were offered, {{taking advantage of}} the PCIe expansion port to extend the capabilities of the base <b>compute</b> <b>blade.</b> Equipped with either workstation Nvidia Quadro graphics cards or Nvidia Tesla scientific cards, the visualization configurations extended the integrated graphics capabilities of the <b>compute</b> <b>blade</b> and offered customers access to Nvidia's CUDA programming architecture to drastically speed-up critical scientific and engineering applications.|$|R
50|$|The Chassis Management Module is used {{to manage}} the Intel Modular Server Chassis' {{integrated}} SAN, the other two to four Service Modules, and the <b>Compute</b> <b>Blades.</b>|$|R
50|$|The Ethernet Switch Module is a managed Gigabit Ethernet {{switch that}} {{provides}} the installed <b>Compute</b> <b>Blades</b> with connectivity {{to each other and}} to external Ethernet networks.|$|R
5000|$|A CX1 {{supercomputer}} supports GPU and scalar <b>compute</b> <b>blades</b> (Nvidia Quadro and Intel Xeon, respectively), typically {{supported by}} enclosed network switches (Gigabit Ethernet, Infiniband) and service blades hosting RAID modules.|$|R
2500|$|... 1858–60 Invention of {{paraffin}} candles. US Patent No. 22,739 on {{a candle}} mold {{for the same}} and US Patent No. 30,180 on a rotating <b>blade</b> <b>device</b> for finishing the same.|$|R
5000|$|An XT5h {{supercomputer}} supports vector, FPGA, and scalar <b>compute</b> <b>blades</b> (X2, XR1, and XT4/XT5 blades, respectively), typically {{supported by}} service blades for network access and hosting a Lustre filesystem layered over several RAID modules.|$|R
5000|$|The {{most basic}} of the modular blade configurations, the single-width <b>compute</b> <b>blade</b> {{supports}} dual-socket Intel Xeon 5400, 5500, and 5600 series processors, {{up to eight}} dims of DDR3 SDRAM (PC3-8500), and two 2.5" [...] SATA HDDs. Furthermore, each <b>compute</b> <b>blade</b> supports {{the addition of a}} PCIe x16 card for graphics or further expansion. Originally offered with the Intel E5400 series processor, later CX1 configurations made either the low-power [...] "L5xxx" [...] series or the high-performance [...] "X5xxx" [...] series Intel processors available to customers. Depending on the blade model, both Gigabit Ethernet and DDR Infiniband interconnects were available - those blades not factory-equipped with Infiniband supported third-party additions through the PCIe expansion port.|$|R
50|$|Two {{types of}} node, {{processor}} and memory, were contained within a <b>blade.</b> <b>Compute</b> <b>blades</b> contain a processor node and consist of two PAC611 sockets for Itanium 2 and Itanium microprocessors, a Super-Hub (SHub) {{application-specific integrated circuit}} (ASIC) (chipset) and eight dual in-line memory module (DIMM) slots for memory. The number of microprocessor sockets in a <b>compute</b> <b>blade</b> is one or two. One-processor socket configurations provide more bandwidth as only one microprocessor socket is using the front side bus and local memory. Two-processor socket configurations do not support hyperthreading. Memory blades are used to expand the amount of memory without {{increasing the number of}} processors. They contain a SHub ASIC and 12 DIMM slots. Both <b>compute</b> and memory <b>blades</b> support 1, 2 4, 8, and 16 GB DIMMs. SGI support does not currently support any installations with 16GB DIMMs.|$|R
50|$|HECToR's initial configuration, {{known as}} Phase 1, {{featured}} 60 Cray XT4 cabinets containing 1416 <b>compute</b> <b>blades,</b> giving {{a total of}} 11,328 2.8 GHz AMD Opteron processor cores, connected to 576 terabytes of RAID backing storage, later increased to 934 TB. The peak performance of the system was 59 teraflops.|$|R
50|$|The Phase 3 upgrade {{took place}} in November and December 2011. It {{involved}} extending the XT6 system to 30 cabinets containing 704 <b>compute</b> <b>blades,</b> and upgrading the processors to 16-core, 2.3 GHz Interlagos Opterons, giving a total of 90,112 cores. The operating system was also upgraded to CLE 4.0.|$|R
50|$|The Altix ICE 8200LX blade {{enclosure}} featured two 4x DDR IB {{switch blade}} and one high performing plane, while the Altix ICE 8200EX featured four 4x DDR IB switch blades, and two high performing planes. Both configurations supported either hypercube or fat tree topology, and 16 <b>compute</b> <b>blades</b> within an IRU.|$|R
50|$|Two {{types of}} <b>Compute</b> <b>Blade</b> can be used, in any combination, in the Intel Modular Server Enclosure; the MFS5000SI and the MFS5520VI. Both Compute Modules are dual-socket systems, which each have an {{integrated}} SAS HBA (for accessing volumes on the integrated SAN), an integrated Gigabit Ethernet port, and integrated graphics.|$|R
2500|$|The Computational Simulation Centre, International Fusion Energy Research Centre of the ITER Broader Approach/Japan Atomic Energy Agency {{operates}} a 1.52PFLOPS [...] supercomputer (currently operating at 442TFLOPS) in Rokkasho, Aomori. The system, called Helios (Roku-chan in Japanese), consists of 4,410 Group Bull bullx B510 <b>compute</b> <b>blades,</b> {{and is used}} for fusion simulation projects.|$|R
5000|$|The {{integrated}} SAN {{consists of}} the HDD module (which accommodates up to fourteen 2.5" [...] HDDs in the MFSYS25 chassis, and up to six 3.5" [...] HDDs in the MFSYS35 chassis) and the Storage Control Module(s). Each <b>Compute</b> <b>Blade</b> accesses volumes, which are assigned to it by connecting to the Storage Control Module(s) through its integrated SAS HBA.|$|R
50|$|While HPI Resources are {{abstract}} structures, typically, {{they are}} used to model the management capabilities of individual management controllers in the hardware platform. For example, in AdvancedTCA (ATCA) platforms, each <b>computing</b> <b>blade</b> usually includes an IPMI Management Controller (IPMC) responsible for hardware management tasks related to that blade. An HPI interface for an ATCA platform will normally include a Resource for each IPMC.|$|R
50|$|The Cray XT5 is {{an updated}} version of the Cray XT4 supercomputer, {{launched}} on November 6, 2007. It includes a faster version of the XT4's SeaStar2 interconnect router called SeaStar2+, and can be configured either with XT4 <b>compute</b> <b>blades,</b> which have four dual-core AMD Opteron processor sockets, or XT5 blades, with eight sockets supporting dual or quad-core Opterons. The XT5 uses a 3-dimensional torus network topology.|$|R
5000|$|The Intel Modular Server System is a {{blade system}} {{manufactured}} by Intel {{using their own}} motherboards and processors. The Intel Modular Server System consists of an Intel Modular Server Chassis, up to six diskless <b>Compute</b> <b>Blades,</b> an integrated storage area network (SAN), and three to five Service Modules. The system was formally announced in January 2008. The server is aimed at small to medium businesses with [...] "50 to 300 employees".|$|R
5000|$|The Modular Server Chassis {{comes in}} two versions; the MFSYS25 and MFSYS35. The key {{difference}} between these two versions is that the MFSYS25's integrated hard disk drive (HDD) bay accommodates fourteen 2.5" [...] HDDs, while the MFSYS35's integrated HDD bay accommodates six 3.5" [...] HDDs. Both versions have two Main Fan Modules, six <b>Compute</b> <b>Blade</b> bays, five Service Module slots, and up to four power supply units in an N+1 configuration.|$|R
50|$|The Altix ICE blade {{platform}} is an Intel Xeon-based system featuring diskless <b>compute</b> <b>blades</b> and a Hierarchical Management Framework (HMF) for scalability, performance, and resiliency. While the earlier Itanium-based Altix systems ran a single-system image (SSI) Linux kernel on 1024 processors or more using a standard SuSE Linux Enterprise Server (SLES) distribution, the Altix ICE's clustering capabilities uses standard SLES or Red Hat Enterprise Linux distributions and scales to over 51,200 cores on NASA's Pleiades supercomputer.|$|R
40|$|Adaptive Optics Real-Time Control {{systems for}} next {{generation}} ground-based telescopes demand significantly higher processing power, memory bandwidth and I/O capacity on the hardware platform {{than those for}} existing control systems. We present a FPGA based high-performance computing platform that is developed at Dominion Radio Astrophysical Observatory and is very suitable for the applications of Adaptive Optics Real-Time Control systems. With maximum of 16 <b>computing</b> <b>blades,</b> 110 TeraMAC/s processing power, 1. 8 Terabyte/s memory bandwidth and 19. 5 Terabit/s I/O capacity, this ATCA architecture platform has enough capacity to perform pixel processing, tomographic wave-front reconstruction and deformable mirror fitting for first and second generation AO systems on 30 +-meter class telescopes. As an example, we demonstrate that with only one <b>computing</b> <b>blade,</b> the platform can handle the real time tomography needs of NFIRAOS, the Thirty-Meter Telescope first light facility Multi-Conjugate Adaptive Optics system. The High-Performance FPGA platform is integrated with Board Software Development Kit to provide a complete and fully tested set of interfaces to access the hardware resources. Therefore the firmware development can be focused on unique, user-specific applications. 9 2012 SPIE. Peer reviewed: YesNRC publication: Ye...|$|R
5000|$|The Cray XT6 is {{an updated}} version of the Cray XT5 supercomputer, {{launched}} on 16 November 2009. [...] The dual- or quad-core AMD Opteron 2000-series processors of the XT5 are replaced in the XT6 with eight- or 12-core Opteron 6100 processors, giving up to 2,304 cores per cabinet. The XT6 includes the same SeaStar2+ interconnect router as the XT5, which is used to provide a 3-dimensional torus network topology between nodes. Each XT6 node has two processor sockets, one SeaStar2+ router and either 32 or 64 GB of DDR3 SDRAM memory. Four nodes form one X6 <b>compute</b> <b>blade.</b>|$|R
25|$|The {{windshield}} wiper is a <b>bladed</b> <b>device</b> used to wipe rain and dirt from a windshield. In 1903, Mary Anderson {{is credited with}} inventing the first operational {{windshield wiper}}. In Anderson's patent, she called her invention a window cleaning device for electric cars and other vehicles. Operated via a lever from inside a vehicle, her version of windshield wipers closely resembles the windshield wiper found on many early car models. Anderson had a model of her design manufactured. She then filed a patent (U.S. patent number 743,801) on June 18, 1903 that was issued to her by the U.S. Patent Office on November 10, 1903.|$|R
50|$|Stiletto has no superpowers but wields {{knives and}} shoots small <b>blades</b> from wrist <b>devices.</b>|$|R
50|$|A {{web site}} called Blade.org was {{available}} for the <b>blade</b> <b>computing</b> community through about 2009.|$|R
5000|$|HECToR: The 2010 system (Phase 2b, XT6) was {{the first}} {{production}} Cray XT6 24-core system in the world. It was contained in 20 cabinets and comprised a total of 464 <b>compute</b> <b>blades.</b> Each <b>blade</b> contained four <b>compute</b> nodes, each with two 12-core AMD Opteron 2.1 GHz Magny Cours processors. This amounted {{to a total of}} 44,544 cores. Each 12-core socket was coupled with a Cray SeaStar2 routing and communications chip. This was upgraded in late 2010 to the Cray Gemini interconnect. Each 12-core processor shared 16Gb of memory, giving a system total of 59.4 Tb. The theoretical peak performance of the phase 2b system was over 360 Tflops. HECToR was decommissioned in 2014.|$|R
5000|$|Gigantic saw <b>blades</b> {{and cutting}} <b>devices</b> (used in The Big Blow Out {{to get an}} energy antenna) ...|$|R
50|$|Neutron-velocity {{selectors}} {{are commonly}} used in neutron research facility to produce a monochromatic beam of neutrons. Due to physical limitations of materials and motors, limiting the maximum speed of rotation of the <b>blades,</b> these <b>devices</b> are only useful for relatively slow neutrons.|$|R
40|$|The {{final report}} on the {{projected}} application of larger-scale wind turbine on the northern German coast is summarized. The designs of the tower, machinery housing, rotor, and rotor blades are described accompanied various construction materials are examined. Rotor <b>blade</b> adjustment <b>devices</b> auxiliary and accessory equipment are examined...|$|R
40|$|The article {{presented}} a feasibility study of {{application of the}} developed constructions of untwisting devices in cyclone apparatus: <b>blade</b> untwisting <b>device</b> and the untwisting device with stream recirculation, allowing substantially to reduce pressure drop and promote efficiency of catching of dispersible particles. The calculations of economy of energy and money facilities are resulted at cleaning of gas in cyclones with the developed untwisting devices. Application in the most widespread cyclones CN- 11 and CN- 15 <b>blade</b> untwisting <b>device</b> power inputs on clearing 1, 000 m 3 gas decrease on the average on 0. 25 and 0. 15 kW · h, and using of the untwisting device with recirculation of a stream – on 0. 2 and 0. 11 kW · h accordingly. The term of recoupment of additional expenses on an untwisting device will make less than year...|$|R
40|$|Conference Name: 2012 International Conference on Materials and Manufacturing Research, ICMMR 2012. Conference Address: Hong kong. Time:December 19, 2012 - December 20, 2012. According to {{structural}} {{characteristics and}} measurement needs of tip clearance of turbine engine and other rotating <b>blade</b> <b>device,</b> a leaves lattice model with vector characteristics is established. Based on measurement {{principle of the}} capacitive displacement sensor, the tip clearance measurement model is established. The spatial filtering effect {{and the impact of}} the tip clearance measurement results caused by blade thickness, sensing zone size, blade rotation speed and signal sampling rate is analyzed. The research results show that there is a minimum requirement about sensing zone size, blade rotation speed and signal sampling rate under the condition of certain blade thickness. The conclusion provides an important theoretical basis for the design of the tip clearance measurement system. ? (2013) Trans Tech Publications, Switzerland...|$|R
40|$|A steady, {{three-dimensional}} viscous average passage {{computer code}} {{is used to}} analyze the flow through a compact radial turbine rotor. The code models the flow as spatially periodic from blade passage to blade passage. Results from the code using varying computational models are compared {{with each other and with}} experimental data. These results include blade surface velocities and pressures, exit vorticity and entropy contour plots, shroud pressures, and spanwise exit total temperature, total pressure, and swirl distributions. The three computational models used are inviscid, viscous with no blade clearance, and viscous with blade clearance. It is found that modeling viscous effects improves correlation with experimental data, while modeling hub and tip clearances further improves some comparisons. Experimental results such as a local maximum of exit swirl, reduced exit total pressures at the walls, and exit total temperature magnitudes are explained by interpretation of the flow physics and computed secondary flows. Trends in the <b>computed</b> <b>blade</b> loading diagrams are similarly explained...|$|R
