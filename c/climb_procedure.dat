3|29|Public
2500|$|Argentina has determined, in {{the light}} of {{information}} it has gathered, that the cause of the accident was: [...] "Failure to operate under IFR during a takeoff by night in weather conditions requiring IFR operation and failure to follow the <b>climb</b> <b>procedure</b> for this type of aircraft; a contributory cause was the lack of vigilance by the pilot-in-command during the operations." ...|$|E
5000|$|The Argentinian {{government}} {{issued the}} following statement:Argentina has determined, {{in the light}} of information it has gathered, that the cause of the accident was: [...] "Failure to operate under IFR during a takeoff by night in weather conditions requiring IFR operation and failure to follow the <b>climb</b> <b>procedure</b> for this type of aircraft; a contributory cause was the lack of vigilance by the pilot-in-command during the operations." ...|$|E
40|$|Normal 0 21 false false false ES X-NONE X-NONE The {{optimization}} {{of aircraft}} trajectories using {{the theory of}} singular optimal control is studied in this thesis. To describe the aircraft motion, a general nonlinear 3 -degree-of-freedom point-mass model is adopted, along with realistic aerodynamic and propulsion models. The controlled motion of an aircraft is modeled as a control system whose performance can be optimized according to some performance index. This control system exhibits different dynamics, constraints and performance indices depending on the flight phase considerd, {{which leads to a}} multiphase control system formulation. An indirect optimization method is applied, in which necessary conditions for optimality are explicitly involved into the problem resolution. The method proposed in this thesis exploits the singular character of the problem in order to provide analytical state-feedback control laws. With this approach, assuming a prescribed solution structure in terms of phase sequence and sequence of singular and bang arcs within each phase, the problem of finding the optimal control is transformed into the problem of finding the values of some unknowns such that the necessary conditions for optimality as well as the initial and final conditions are satisfied, that is, the problem of solving a nonlinear system of equations. Optimizing global trajectories implies not only addressing each flight phase, but also taking into account the interactions among them as well as looking for a global objective. Therefore, an optimal global trajectory cannot be obtained by simply piecing individually optimized phases together, not even when each phase is optimized with a performance index suitable for a global the optimal control and optimal path structure for a single-phase optimal trajectory also apply at each phase of an optimal multiphase trajectory. As a consequence, prior to applying this approach to the problem of multiphase trajectories of commercial transport aircraft providing minimum fuel consumption, this approach is applied to three auxiliary single-phase problems. First, the problem of fuel-optimal fixed-rating aircraft climb in the presence of altitude-dependent winds is analyzed. The climb is optimized to give minimum contribution to the global-trajectory fuel consumption. The optimal control is of the bang-singular type, and the optimal trajectories are formed by a singular are and two minimum-path-angle arcs joining the singular are with the given initial and final points. This analysis is used to assess the optimality of a standard <b>climb</b> <b>procedure</b> defined by segments with constant calibrated air speed and Mach number. Linear wind profiles defined by two parameters, the average wind and the wind shear, are considered. The effects of the wind profile and of the initial aircraft weight on the results are studied. Comparison with the optimal results shows that the performance of the optimized standard climb, in terms of global variables such as fuel     consumption, flight time and horizontal distance travelled, is very close to optimal. Second, minimum-fuel cruise at constant altitude with the constraint of a fixed arrival time is analyzed, including the effect of average horizontal winds. Again, the optimal control is of the bang-singular-bang type, and the optimal trajectories are formed by a singular arc and two minimum/maximum-thrust arcs joining the singular arc with the given initial and final points. The effects of average horizontal winds on the optimal results are analyzed, both qualitatively and quantitatively. The influence of the initial aircraft weight and the given cruise altitude is analyzed as well. Two applications are studied: first. The cost of meeting the given arrival time under mismodeled winds, and second, the cost of flight delays imposed on a nominal optimal path. The optimal results are used to assess the optimality of cruising at constant speed; the results show that the standard constant-Mach cruise is very close to optimal. Third, unpowered descents of commercial transport aircraft are optimized in the presence of altitude-dependent winds, with the objective of maximizing range. The optimal problem and an optimized constant-calibrated-airspeed procedure are analyzed. The optimal control is of the bang-singular-bang type, and the optimal trajectories are formed by a singular arc and two maximum-path-angle arcs joining the singular arc with the given initial and final points. Linear wind profiles defined by two parameters, the average wind and the wind shear, are considered. The effects of both the average, wind and the wind shear on the optimal results, as well as the effects of the aircraft weight, are analyzed. The wind shear is shown to have a clear effect on the maximum range. The comparison between the two sets of results shows that the optimized constant-calibrated-airspeed descent is very close to optimal. Once the auxiliary single-phase problems are solved, the problem of global trajectories of commercial transport aircraft providing minimum fuel consumption is analyzed. The global trajectories are considered to be composed of three types of phases: climb, cruise, and unpowered descent. The optimal control in every phase is of the bang-singular-bang type, and the optimal climb, cruise and descent trajectories are formed by a singular arc and two minimum/maximum-control arcs joining the singular arc with the given initial and final points. The optimal trajectories and controls, the minimum fuel consumption and some interesting global results are computed for an aircraft performing a climb-cruise-climb-cruise-descent trajectory. Linear wind profiles defined by two parameters, the average wind and the wind shear, are considered. The influence of the aircraft weight and the wind profile on the results is analyzed. <w:LsdException Locked="false" Prio...|$|E
40|$|Abstract. This {{contribution}} {{addresses the}} consideration of time windows in the optimization of multi-commodity network flows. For each node, one interval is specified in which the visitation is allowed. Applications in freight flow consolidation let this problem become interesting. An optimization model is proposed and a construction heuristic is presented. For improving the generated solutions, a genetic algorithm framework including several hill <b>climbing</b> <b>procedures</b> for local optimization, is configured. ...|$|R
50|$|The M20K was {{marketed as}} the Mooney 231. This models Continental TSI0-360-GB engine {{required}} specific pilot training and modified takeoff and <b>climb</b> <b>procedures</b> {{to operate at}} acceptable engine temperatures in hot weather, so by 1986, it was replaced with an intercooled TSIO-360-MB engine, reducing the temperature problems and achieving {{a top speed of}} 252 mi/h km/h in level flight (at FL 280). This subvariant of the M20K was marketed as the Mooney 252.|$|R
40|$|In {{spite of}} {{numerous}} advantages of biometrics-based personal authentication systems over traditional security systems based on token or knowledge, they {{are vulnerable to}} attacks that can decrease their security considerably. In this paper, we analyze these attacks {{in the realm of}} a fingerprint biometric system. We propose an attack system that uses a hill <b>climbing</b> <b>procedure</b> to synthesize the target minutia templates and evaluate its feasibility with extensive experimental results conducted on a large fingerprint database. Several measures that can be utilized to decrease the probability of such attacks and their ramifications are also presented...|$|R
40|$|We {{present a}} new {{approach}} to balancing the work-load in a multicomputer when the problem is de-composed into subproblems mapped to the processors. It is based on a hybrid genetic algo-rithm. A number of design choices for genetic algo-rithms are combined in order to ameliorate the problem of premature convergence that is often en-countered in the implementation of classical genet-ic algorithms. The algorithm is hybridized by including a hill <b>climbing</b> <b>procedure</b> which signifi-cantly improves the efficiency of the evolution. Moreover, it makes use of problem specific infor-mation to evade some computational costs and to reinforce favorable aspects of the genetic search a...|$|R
40|$|This paper {{projects}} Improved Baboon Algorithm (IBA) {{for solving}} the Reactive Power dispatch problem. The key feature in {{this problem is}} reduction of real power loss and to keep voltage profiles within limits. This algorithm is inspired from the tree <b>climbing</b> <b>procedures</b> of Baboons, where the Baboons look for the highest tree by climbing up from their positions. The simulation results expose amended performance of the IBA in solving an optimal reactive power dispatch problem. In order to evaluate up {{the performance of the}} proposed algorithm, it has been tested on Standard IEEE 30 bus system and compared to other stated algorithms. Simulation results show that IBA is better than other algorithms in reducing the real power loss and voltage profiles also within the limits...|$|R
40|$|Neural {{networks}} have {{recently had a}} lot of success for many tasks. However, neural network architectures that perform well are still typically designed manually by experts in a cumbersome trial-and-error process. We propose a new method to automatically search for well-performing CNN architectures based on a simple hill <b>climbing</b> <b>procedure</b> whose operators apply network morphisms, followed by short optimization runs by cosine annealing. Surprisingly, this simple method yields competitive results, despite only requiring resources in the same order of magnitude as training a single network. E. g., on CIFAR- 10, our method designs and trains networks with an error rate below 6 % in only 12 hours on a single GPU; training for one day reduces this error further, to almost 5 %. Comment: Under review as a conference paper at ICLR 201...|$|R
40|$|Abstract. The Denclue {{algorithm}} {{employs a}} cluster model based on kernel density estimation. A cluster {{is defined by}} a local maximum of the estimated density function. Data points are assigned to clusters by hill climbing, i. e. points going to the same local maximum are put into the same cluster. A disadvantage of Denclue 1. 0 is, that the used hill climbing may make unnecessary small steps in the beginning and never converges exactly to the maximum, it just comes close. We introduce a new hill <b>climbing</b> <b>procedure</b> for Gaussian kernels, which adjusts the step size automatically at no extra costs. We prove that the procedure converges exactly towards a local maximum by reducing it to a special case of the expectation maximization algorithm. We show experimentally that the new procedure needs much less iterations and can be accelerated by sampling based methods with sacrificing {{only a small amount}} of accuracy. ...|$|R
40|$|An {{important}} {{extension of}} constraint technology involves problems that undergo changes that may invalidate the current solution. Previous work on dynamic problems sought methods for efficiently finding new solutions. We {{take a more}} proactive approach, exploring methods for finding solutions more likely to remain valid after changes that temporarily alter the set of valid assignments (stable solutions). To this end, we examine strategies for tracking changes in a problem and incorporating this information to guide search to solutions that {{are more likely to}} be stable. In this work search is carried out with a min-conflicts hill <b>climbing</b> <b>procedure,</b> and information about change is used to bias value selection, either by distorting the objective function or by imposing further criteria on selection. We study methods that track either value losses or constraint additions, and incorporate information about relative frequency of change into search. Our experiments show that the [...] ...|$|R
40|$|The space {{elevator}} {{offers an}} alternate and very efficient method for space travel. It {{will have two}} main components. The first component is the tether (or the ribbon), which extends from the Earth to an equatorial satellite {{at an altitude of}} about 100, 000 kilometres, and is fixed to a base {{on the surface of the}} Earth at its lower end. The second component is the climber, which scales the ribbon, transporting payloads to space. An important issue for effective operation of the space elevator will be to understand its dynamics. This thesis attempts to develop a realistic and yet simple planar model for this. Both rigid and elastic ribbon models are considered. Their response to ascending climbers and to aerodynamic loads is studied. Specific <b>climbing</b> <b>procedures</b> are devised based on these results. The effect of the space elevator's motion on the orbit of a launched satellite is also examined...|$|R
40|$|The Denclue {{algorithm}} {{employs a}} cluster model based on kernel density estimation. A cluster {{is defined by}} a local maximum of the estimated density function. Data points are assigned to clusters by hill climbing, i. e. points going to the same local maximum are put into the same cluster. A disadvantage of Denclue 1. 0 is, that the used hill climbing may make unnecessary small steps in the beginning and never converges exactly to the maximum, it just comes close. We introduce a new hill <b>climbing</b> <b>procedure</b> for Gaussian kernels, which adjusts the step size automatically at no extra costs. We prove that the procedure converges exactly towards a local maximum by reducing it to a special case of the expectation maximization algorithm. We show experimentally that the new procedure needs much less iterations and can be accelerated by sampling based methods with sacrificing {{only a small amount}} of accuracy...|$|R
40|$|The exact {{locations}} of these points {{are determined by}} a subpixelaccurate matching procedure seeking for the maximum complex correlation coefficient. From the successfully matched points mean translations of the second scene {{with respect to the}} first scene are computed. With these translation values a coherence image is derived for a comparatively small region (e. g. 200 x 200 pixels) {{in the center of the}} region of interest. After the second scene is resampled to the geometry of the first scene using a bicubic-spline interpolation, the interferogram is derived in the original resolution of the scene. From this interferogram the interferogram of a ²flat earth² is subtracted using the approach described by Hartl & Xia [1993]. Finally, the correlation, i. e. the coherence, is computed for a predefined window size. As the evaluated region is small, the coherence computation can be repeated very rapidly. Therefore, with a hill <b>climbing</b> <b>procedure</b> to maximize th...|$|R
50|$|The {{geometric}} {{median of}} a set of points in the Euclidean plane is the point (not necessarily in the given set) that minimizes the sum of distances to the given points; the solution for three points was first given by Evangelista Torricelli, after being challenged with it by Pierre de Fermat in the 17th century. An algorithm for the more general problem with an arbitrarily large number of points, published by Weiszfeld in 1937, solves this problem numerically using a hill <b>climbing</b> <b>procedure</b> that repeatedly finds a point improving the sum of distances until no more improvements can be made. Each step of this algorithm assigns weights to the points, inversely proportional to the distances to the current solution, and then finds the weighted average of the points, which is the point that minimizes the sum of the squares of the weighted distances. The algorithm has been frequently rediscovered, and although other methods for finding the geometric mean are known, Weiszfeld's algorithm is still frequently used due to its simplicity and rapid convergence.|$|R
40|$|The one-sided {{bipartite graph}} drawing {{problem has been}} {{extensively}} studied in the graph drawing literature, with numerous papers appearing over the years showing novel algorithms and heuristics for minimizing associated edge crossings. Although stochastic methods have been highly successful when applied to bipartite graph drawing, a large, comprehensive study to compare said methods has not been carried out for one-sided crossing minimization on traditional, unweighted graphs. Even more so, the variant problems of weighted and bottleneck bipartite crossing minimization have seldom been studied, with only one published algorithm, 3 -WOLF (designed for one-sided weighted crossing minimization), and one published heuristic, MEC (designed for multi-layer bottleneck crossing minimization) appearing in the literature. To date {{there has been no}} published work for handling one-sided bottleneck crossing minimization. This thesis helps to close some of these gaps in the graph drawing literature. A comparative study of stochastic methods for one-sided bipartite graph drawing is carried out on both unweighted and weighted bipartite graphs. Novel variants of two previously published genetic algorithms, a one-sided version of a hybrid simulated annealing algorithm, a stochastic hill <b>climbing</b> <b>procedure,</b> and a top performing approximation algorithm, 3 -WOLF, are compared against each other and the standard benchmark barycenter heuristic on weighted and unweighted bipartite graphs. Both unweighted and weighted versions of barycenter are considered. In addition, a novel bottleneck crossings based stochastic hill <b>climbing</b> <b>procedure</b> is compared against a one-sided variant of the MEC heuristic for one-sided bottleneck crossing minimization. The algorithms in this latter set of tests are conducted with and without barycenter preprocessing. 	Stochastic hill climbing was found to obtain the best results for all of the bipartite graph drawing problems, significantly outperforming the other methods in the weighted and bottleneck crossing contexts. The results of bottleneck crossing based stochastic hill climbing were slightly improved with barycenter preprocessing. These experimental findings are particularly notable in the weighted and bottleneck crossing cases, since 3 -WOLF {{is one of the best}} methods for weighted one-sided crossing minimization and MEC the best for multi-layer bottleneck crossing minimization. In regards to barycenter, although it didn’t perform well for weighted crossing minimization, it was interesting to find that its traditional unweighted variant worked better for weighted crossing minimization than its edge-weighted version. Computer ScienceMastersUniversity of New Mexico. Dept. of Computer ScienceHayes, ThomasKelley, PatrickLoring, TerryHayes, Thoma...|$|R
40|$|In {{recent years}} the {{constraint}} satisfaction paradigm has been extended to the {{case in which a}} problem can undergo changes that may invalidate the current solution. If such changes are recurrent, {{it may be possible to}} find solutions that are more likely to remain valid after such changes (stable solutions). In earlier work we have found that, (i) hill <b>climbing</b> <b>procedures</b> such as min-conflicts are peculiarly suitable for dealing with dynamic CSPs, in part because, (ii) these procedures can be readily combined with simple strategies for penalizing values that `go bad' through simple loss or changes that render them invalid as assignments. However, the situation we have studied also lends itself to a Bayesian approach for choosing values that are more likely to yield stable solutions. In comparing the two approaches, we find that the Bayesian methods are less efficient with respect to runtime, and, more surprisingly, that the solutions obtained are no more stable than those found with penalty procedures under comparable conditions. Relative efficiency depends on not preventing hill climbing from finding any solution because there are no suitable candidate values, and this is accomplished more effectively by penalty methods. Differences in solution stability depend not only on the way in which values are differentiated, but also on the manner in which values are selected for consideration during the course of search...|$|R
40|$|The widely {{accepted}} industry High Speed Civil Transport (HSCT) design goal for exterior noise {{is to achieve}} Federal Aviation Regulation (FAR) Part 36 Stage 3 noise limits currently required for new subsonic aircraft. The three phases of the concern are as follows: (1) airport noise abatement at communities close to the airport, (2) <b>climb</b> power opening-up <b>procedures,</b> and (3) the climb to cruise phase affecting communities far from the airport...|$|R
40|$|Binary {{descriptors}} {{of image}} patches provide processing speed advantages and require less storage than methods that encode the patch appearance with a vector of real numbers. We provide evidence that, despite its simplicity, a stochastic hill <b>climbing</b> bit selection <b>procedure</b> for descriptor construction defeats recently proposed alternatives {{on a standard}} discriminative power benchmark. The method is easy to implement and understand, has no free parameters that need fine tuning, and runs fast...|$|R
40|$|En route noise {{emissions}} {{on the ground}} can {{be affected by the}} detailed characteristics of intended noise-abatement <b>climb</b> profiles and <b>procedures</b> to an extent of 10 or more nautical miles {{from the start of the}} takeoff roll of a large or heavy air-carrier-type aircraft. Suggestions submitted to the noise abatement officials of the airports at Frankfurt, Federal Republic of Germany, and Zurick, Switzerland, and the aircarriers Lufthansa German Airlines and SWISSAIR are explained and discussed...|$|R
40|$|AbstractScenario {{discovery}} {{is a novel}} model-based approach to scenario development {{in the presence of}} deep uncertainty. Scenario discovery frequently relies on the Patient Rule Induction Method (PRIM). PRIM identifies regions in the model input space that are highly predictive of producing model outcomes that are of interest. To identify these, PRIM uses a lenient hill <b>climbing</b> optimization <b>procedure.</b> PRIM struggles when confronted with cases where the uncertain factors are a mix of data types, and can be used only for binary classifications. We compare two more lenient objective functions which both address the first problem, and an alternative objective function using Gini impurity which addresses the second problem. We assess the efficacy of the modification using previously published cases. Both modifications are effective. The more lenient objective functions produce better descriptions of the data, while the Gini impurity objective function allows PRIM to be used when handling multinomial classified data...|$|R
500|$|Since it was {{no longer}} {{possible}} to abort the takeoff at this point, the crew followed the standard operating procedure for an [...] "engine out" [...] <b>climb.</b> This <b>procedure</b> is to <b>climb</b> at the takeoff safety airspeed (V2) and attitude (angle), as directed by the flight director. The partial electrical power failure (produced by the separation of the left No. 1 engine) meant that neither the stall warning nor slat retraction indicator was operative. The crew was, therefore, unaware that the slats on the left wing were retracting. This retraction significantly raised the stall speed of the left wing. Thus flying at the takeoff safety airspeed caused the left wing to stall while the right wing was still producing lift, so the aircraft banked sharply and uncontrollably to the left. Later, in simulator recreations of the accident, it was determined that by climbing at a higher airspeed the crash could have been averted.|$|R
40|$|Includes bibliographical {{references}} (leaves 148 - 150). The {{problem of}} regrouping service sites into {{a smaller number}} of service centers, such that a number of criteria are satisfied, is a realistic problem. Each service center then serves several customer-sites (e. g. towns) in a region. The objectives of regrouping are usually to consolidate human resources, improve service quality, reduce the cost of services, and centralize company branches, in addition to other application-dependent objectives. This regrouping problem is intractable and needs to be automated. Our approach is based on a weighted graph problem formulation, and the solution has two phases. In the first phase, the graph is decomposed into the required number of sub-graphs (regions) using a tuned hybrid genetic algorithm (Tuned HGA). The second phase finds a suitable center within each region by applying a heuristic algorithm. Genetic algorithms are stochastic algorithms based on the mechanics of natural evolution. They are adapted in our work by using a problem-specific objective function for the fitness of individuals. The algorithm is hybridized by a hill <b>climbing</b> <b>procedure</b> in order to direct the search into profitable search sub-domains. The results of the HGA are tuned by using a problem-specific iterative improvement heuristic (IIH) that aim to remove anomalies and hence improve the final solution's quality. We also explore using a pre-processing step to reduce the graph vertex granularity for the purpose of further reducing the objective function value and improving the solution's quality. In the second phase, the heuristic algorithm favors higher-weight and well-centered vertices for selection to be centers within a region. We empirically explored the behavior of the Tuned HGA and the center selecting heuristic algorithm using a number of graphs, representing service sites with their inner-site distances. The empirical results show that: (a) The two-phase approach can be used for solving this problem, (b) Hybridization of the GA improves both its solution quality and evolution time, (c) The tuning IIH does improve the results by removing most anomalies, (d) Breaking up graph vertex granularity can be useful for graphs where vertex weights vary significantly, and (e) The center selection heuristic selects reasonable centers within regions. 1 bound copy: vii, 150 leaves; ill. (some col.); 30 cm. available at RNL...|$|R
40|$|Equipment and {{operational}} concepts affecting {{aircraft in the}} terminal area are reported. Curved approach applications and modified <b>climb</b> and descent <b>procedures</b> for minimum fuel consumption are considered. The curved approach study involves the application of MLS guidance to enable execution of the current visual approach to Washington National Airport under instrument flight conditions. The operational significance and the flight path control requirements involved {{in the application of}} curved approach paths to this situation are considered. Alternative flight path control regimes are considered to achieve minimum fuel consumption subject to constraints related to air traffic control requirements, flight crew and passenger reactions, and airframe and powerplant limitations...|$|R
40|$|The Expedite Departure Path (EDP) is a {{decision}} support tool being developed at NASA Ames Research Center aimed at providing Terminal Area Radar Approach Control (TRACON) Traffic Management Coordinators (TMCs) with pertinent departure traffic loading and scheduling information and radar controllers with advisories for tactical control of terminal area departure traffic. One of the proposed features of EDP is to provide departure controllers {{with the ability to}} perform unrestricted <b>climbs</b> where <b>procedures</b> typically restrict departures below incoming arrival traffic streams. The potential benefits of this feature include reductions in time-to-climb, fuel burn, and aircraft noise impact to the surrounding communities. This paper focuses on the issue of unrestricted climb in congested terminal areas and describes the modeling and simulation of such climbs. First, flight data of departures in TRACON airspace were analyzed to estimate the level of uncertainties in climb trajectory prediction. Second, the existing Trajectory Synthesizer (TS) module of the Center-TRACON Automation System (CTAS) was modified to generate trajectories that closely model actual aircraft climb profiles and terminal airspace procedures. Third, an algorithm was applied to predict conflicts between trajectories of departure and arrival aircraft and to determine if an unrestricted climb is advisable. Controller-in-theloop simulations were performed to validate the feasibility of the algorithm and evaluate human factors. Lastly, a future application of a conflict probability estimation method for EDP was examined...|$|R
500|$|At about 2:55 pm, the four-engine Electra {{turboprop}} {{began its}} takeoff {{run to the}} west. At {{some point during the}} takeoff run or initial climb, the number three engine failed and caught fire. The crew continued the takeoff and <b>climb,</b> per standard <b>procedure,</b> using power from the remaining three engines. The pilot radioed the control tower declaring an emergency, and the control tower cleared the flight for an immediate landing. The number three engine was engulfed in flames as the crew retracted the flaps and maneuvered the plane into a left turn back to the runway. The plane entered a 30–45 degree bank, then rapidly lost altitude and crashed into hilly terrain about [...] west-southwest of the runway, above the village of San Jerónimo. [...] The fuel on board caught fire and all aboard perished except the copilot, 26 year old Juan Loo, who was found in the wreckage of the cockpit badly burned but alive. Two farm workers were killed on the ground.|$|R
40|$|Binary-Discrete State Manipulators (b-DSMs) are force {{regulated}} manipulators that undergo continuous motions {{despite being}} commanded through {{a finite number}} of states only. Designing a real-time control of such systems requires fast and efficient methods for solving their inverse static analysis (ISA), which is a challenging problem of this paper. In particular, an artificial intelligence method based on neuro-fuzzy method is proposed to investigate the on-line computation and the generalization error of ISA problem of a class of b-DSMs featuring two-state force actuators and six degree of freedom. The main advantages of a neuro-fuzzy system for b-DSMs are: it interprets IF-THEN rules from input-output relations (orientation, moment and binary state) and focuses on accuracy of the output network and offers efficient time consumption for on-line computation. The paper proposed two architectures which are based on the Neuro-Fuzzy Takagi-Sugeno (NFTS) inference scheme with Gaussian membership functions. They are NFTS and the Look-Up Table version of NFTS, which is called as NFLUT. Both structures are with multivariate input and multi-state outputs, such as orientations and moments as input networks and binary state of the b-DSMs as output networks. The learning procedure uses an accelerated LMA with optimal training parameters with at least half-million iterations with different 10 membership functions, employ 12 of the input-output correspondences from the known input-output dataset. For experimental database,the NF structure is tested using 1024 dataset. The optimized membership function (N) after two weeks searching time using Hill <b>Climbing</b> (HC) <b>procedure</b> is N = 17 for the 10 -binary Massive Parallel Robots (MPRs). Regarding model performances for the ISA solution, the NFLUT features better generalization ability compared to the NFTS model but requires a rather larger computational time during on-line testing phase...|$|R
40|$|Flexibility and {{automation}} in {{assembly lines}} {{can be achieved}} by the use of robots. The robotic assembly line balancing (RALB) problem is defined for robotic assembly line, where different robots may be assigned to the assembly tasks, and each robot needs different assembly times to perform a given task, because of its capabilities and specialization. The solution to the RALB problem includes an attempt for optimal assignment of robots to line stations and a balanced distribution of work between different stations. It aims at maximizing the production rate of the line. A genetic algorithm (GA) is used to find a solution to this problem. Two different procedures for adapting the GA to the RALB problem, by assigning robots with different capabilities to workstations are introduced: a recursive assignment procedure and a consecutive assignment procedure. The results of the GA are improved by a local optimization (hill <b>climbing)</b> work-piece exchange <b>procedure.</b> Tests conducted on a set of randomly generated problems, show that the Consecutive Assignment procedure achieves, in general, better solution quality (measured by average cycle time). Further tests are conducted to determine the best combination of parameters for the GA procedure. Comparison of the GA algorithm results with a truncated Branch and Bound algorithm for the RALB problem, demonstrates that the GA gives consistently better results...|$|R
40|$|Knowledge Management (KM) is a keen {{topic for}} an organization, in {{particular}} to those that {{have to deal with}} knowledge acquired from different sources, either from its own experiences or from that of others, to decide on the effective use of that knowledge to fulfill the goals of the organization. As representative examples of KM, one may have the object-oriented data bases, hypermedia or concept maps. On the other hand, techniques developed in Artificial Intelligence for knowledge representation and discovery may be of great use in KM; in particular, it seems natural to explore the potential of the organization past data to deal with management decisions of the present. One way is to use Time Series Forecasting (TSF), where forecasts are based on pattern recognition of past observations ordered in time. Traditional TSF methods, such as the Holt-Winters and the Box-Jenkins ones, are based on particular characteristics of the Time Series (TS), such as trend or seasonal effects. These methods work with well behaved TS, but present some drawbacks on TS with noise or some unknown nonlinear relations among the TS data. An alternative approach is the use of Artificial Neural Networks (ANNs), which present two main advantages: ANNs can extrapolate patterns from past data, even in TS with noise, and may adapt their behavior as new data comes in. A problem with the use of this approach is the search time for the best ANN architecture, which involves a large searching space, demanding a huge computational effort. Other aspect of concern is that of TS data filtering. Not all lags of the TS have the same influence over the forecast. Feeding the ANN with a big time window will slow the ANN forecasting efficiency. To solve these pitfalls, one may use random search, hill <b>climbing</b> or genetic <b>procedures.</b> The last ones are known to work well on problems of combinatorial nature, obtaining good solutions where other methods seem to fail. This paper presents an integrated approach for TSF: a set of rules will create the training cases, based on some lags of the TS; these rules and the ANN parameters will be encoded on the genetic chromosomes; finally, each ANN will be trained, leading to competition...|$|R
40|$|To {{achieve a}} {{sustainable}} future for air transport, the International Civil Aviation Organization has proposed goals for reductions in community noise impact, local air quality and climate impacting emissions. The goals {{are intended to}} be achieved through advances in engine design, aircraft design and through improvements in aircraft operational procedures. This thesis focuses on operational procedures, and considers how trajectory generation methods {{can be used to}} support ight and airspace planners in the planning and delivery of environmentally e cient ight operations. The problem of planning environmentally e cient trajectories is treated as an optimal control problem that is solved through the application of a direct method of trajectory optimisation combined with a stochastic Non Linear Programming (NLP) solver. Solving the problem in this manner allows decision makers to explore the relationships between how aircraft are operated and the consequent environmental impacts of the ights. In particular, this thesis describes a multi-objective optimisation methodology intended to support the planning of environmentally e cient <b>climb</b> and descent <b>procedures.</b> The method combines environmental, trajectory and NLP methods to generate Pareto fronts between several competing objectives. It is shown how Pareto front information can then be used to allow decision makers to make informed decisions about potential tradeo s between di erent environmental goals. The method is demonstrated through its application to a number of real world, many objective procedure optimisation studies. The method is shown to support in depth analysis of the case study problems and was used to identify best balance procedure characteristics and procedures in an objective, data driven approach not achievable through existing methods. Driven by operator speci c goals to reduce CO 2 emissions, work in this thesis also looks at trajectory based ight planning of CO 2 e cient trajectories. The results are used to better understand the impacts of ATM constraints and recommended procedures on both the energy management and fuel e ciency of ights. Further to this, it is shown how trajectory optimisation methods {{can be applied to the}} analysis of conventional assumptions on fuel e cient aircraft operations. While the work within is intended to be directly relevant to the current air tra c management system, both consideration and discussion is given over to the evolution and continued relevance of the work to the Single European Sky trajectory based concept of operation...|$|R

