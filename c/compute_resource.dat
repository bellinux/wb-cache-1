67|8649|Public
5000|$|Increase the <b>compute</b> <b>resource</b> (number of CPU cores, {{amount of}} ram) for the {{instance}} {{on which the}} application is running. This {{is also known as}} scaling up.|$|E
50|$|Sentient Technologies is an American {{artificial}} intelligence (AI) based technology {{company based in}} San Francisco. Sentient was founded in 2007 and has generated over $143 million in funding since its inception. As of 2016, Sentient is the world's most well-funded AI company. Focused on e-commerce and online content as well as trading, it runs the largest <b>compute</b> <b>resource</b> dedicated to distributed {{artificial intelligence}}.|$|E
50|$|A healthy {{level of}} {{skepticism}} must be employed when using computer software to <b>compute</b> <b>resource</b> volumetrics. The algorithms or {{methods used to}} create the volumetric models have limitations that may be acceptable for one type of deposit while being completely inappropriate for another. For example, a sand and gravel deposit requires an approach that is completely different from the methods used to evaluate a phosphate reserve. The {{best way to avoid}} misuse is to always compare “slices” through the models with borehole logs that show the original data. These cross-sections are used {{to make sure that the}} model “honors” the data. Just as importantly, cross-sections should be evaluated to make sure that the modeling conforms to the expected geology.|$|E
40|$|Mobile crowdsourcing, as an {{emerging}} service paradigm, enables the <b>computing</b> <b>resource</b> requestor (CRR) to outsource computation tasks to each <b>computing</b> <b>resource</b> provider (CRP). Considering {{the importance of}} pricing as an essential incentive to coordinate the real-time interaction among the CRR and CRPs, in this paper, we propose an optimal real-time pricing strategy for <b>computing</b> <b>resource</b> management in mobile crowdsourcing. Firstly, we analytically model the CRR and CRPs behaviors in form of carefully selected utility and cost functions, based on concepts from microeconomics. Secondly, we propose a distributed algorithm through the exchange of control messages, which contain the information of <b>computing</b> <b>resource</b> demand/supply and real-time prices. We show that there exist real-time prices that can align individual optimality with systematic optimality. Finally, we also {{take account of the}} interaction among CRPs and formulate the <b>computing</b> <b>resource</b> management as a game with Nash equilibrium achievable via best response. Simulation results demonstrate that the proposed distributed algorithm can potentially benefit both the CRR and CRPs. The coordinator in mobile crowdsourcing can thus use the optimal real-time pricing strategy to manage <b>computing</b> <b>resources</b> towards the benefit of the overall system...|$|R
40|$|In this research, {{we suggest}} {{appropriate}} information technology (IT) governance structures {{to manage the}} cloud <b>computing</b> <b>resources.</b> The interest in acquiring IT resources a utility is gaining momentum. Cloud <b>computing</b> <b>resources</b> present organizations with opportunities to manage their IT expenditure on an ongoing basis, and are providing organizations access to modern IT resources to innovate and manage their continuity. However, cloud <b>computing</b> <b>resources</b> are no silver bullet. Organizations would need to have appropriate governance structures and policies in place to ensure its effective management and fit into existing business processes to leverage the promised opportunities. Using a mixed method design, we identified four possible governance structures for managing the cloud <b>computing</b> <b>resources.</b> These structures are a chief cloud officer, a cloud management committee, a cloud service facilitation centre, and a cloud relationship centre. These governance structures ensure appropriate direction of cloud <b>computing</b> <b>resources</b> from its acquisition {{to fit into the}} organizations business processes...|$|R
40|$|CompuP 2 P is an {{architecture}} for sharing of <b>computing</b> <b>resources</b> in peer-to-peer (P 2 P) networks. It provide resources, such as processing power, memory storage etc., to user applications that might require them. CompuP 2 P creates dynamic markets for different amounts of <b>computing</b> <b>resources</b> without relying on any trusted centralized entity {{to monitor the}} activities of nodes in those markets. Moreover, the pricing of <b>computing</b> <b>resources</b> takes into account selfishness of network users and uses ideas from game theory and microeconomics...|$|R
30|$|We {{assume that}} the {{integration}} of a new provider freshly arrived at the federation, implies the creation of contacts {{with at least one}} BA from the BA organization. This link with only one BA will be enough for the new coming provider to be reached by all the BA organization. Thanks to BA interactions and request’s migration, all consumers will take benefits from this new provider services. For example, in Fig.  1 User 1 demands a request from Broker “A”, this request needs a <b>compute</b> <b>resource</b> (as seen in the cloud federation it is provided exclusively by a new coming compute cloud provider) but Broker “A” have no contact with the new coming compute cloud provider. Interactions between the BA organizations will permit Broker “A” to delegate User’s 1 <b>compute</b> <b>resource</b> request to neighbors of Broker “A” and so on until the delegated request will reach the suitable Broker in contact with the new coming compute cloud provider to allocate this <b>compute</b> <b>resource</b> to User 1. In this case, it is Broker “C” who is in position to satisfy User’s 1 <b>compute</b> <b>resource</b> request.|$|E
30|$|Over the years, {{a lot of}} {{state-of-the-art}} {{research work}} on the radio access network control problem has been conducted. In [9], K. Sundaresan et al. proposed a scalable, light-weight scheme for realizing the full potential of C-RAN systems. For small cells, this scheme determined configurations that maximized the traffic demand while simultaneously optimizing the <b>compute</b> <b>resource</b> usage in the baseband processing unit pool. Briefly, the developed scheme in [9] adopted a two-step approach: (i) the first-step determined the optimal combination of configurations; it needed to support the traffic demand from a set of small cells, and (ii) the second step consolidated the configurations to further reduce the <b>compute</b> <b>resource</b> usage [9].|$|E
40|$|In {{this article}} we present a first time {{evaluation}} of a metamorphosic <b>compute</b> <b>resource</b> (MCR) in a real world scenario. During day-time the MCR compute nodes are ordinary office computers, used in a student lab. Thanks to our enhancements, the computers become part of an OpenMosix based <b>compute</b> <b>resource</b> during the night. The single most common reason for partial loss of computing power was students and/or personnel switching {{off some of the}} machines in the evening by mistake: something easily solved by e. g. implementing `wake on LAN'. We also found that less than 1 per mill of all disturbances was non-trivial in nature which clearly indicates the strength of the architecture...|$|E
25|$|<b>Computing</b> <b>resources</b> are not {{administered}} centrally.|$|R
30|$|BBU scaling: BBUs are {{dynamically}} scaled {{according to}} the network requirements. For example, {{when there is an}} increase in network traffic, a virtual BBU can be scaled up to utilize more <b>computing</b> <b>resources.</b> In addition, in case of future network extensions, more virtual BBUs can be instantiated. A novel resource optimization algorithm which takes into account thermal and <b>computing</b> <b>resource</b> models was developed in [28]. Optimization is achieved by allocating the maximum load to BBU under thermal constraints. The optimization problem is solved using Lagrange multiplier with Kuhn-Tucker condition. In [29], Zhang et al. aimed at minimizing the total amount of <b>computing</b> <b>resources</b> needed, while balancing the allocated <b>computing</b> <b>resources</b> among BBUs. The optimization is formulated as a bin-packing problem and solved using a heuristic genetic algorithm.|$|R
5000|$|... allows around-the-clock high {{utilization}} of expensive <b>computing</b> <b>resources</b> ...|$|R
40|$|Storage Resource Managers (SRMs) are {{middleware}} components whose {{function is}} to provide dynamic space allocation and file management on shared storage components on the Grid. They complement <b>Compute</b> <b>Resource</b> Managers in providing storage reservation and dynamic information on storage availability for data movement, and fo...|$|E
30|$|In {{order to}} simplify the presentation, we only develop the fuzzy logic {{approach}} for the CPU processor load as core <b>compute</b> <b>resource,</b> linked to performance at the service level. However, specific characteristics that would distinguish CPU, storage and network do not play any role and, therefore, selecting just one here for simplification does not restrict the solution.|$|E
30|$|Virtual Resource: is an {{abstraction}} added onto compute, storage and network resources. It enables slicing {{of these resources}} into smaller chunks that can be scaled vertically or horizontally. Typically virtualisation is used in a data centre to slice data centre <b>compute</b> <b>resource</b> into Virtual machines, and potentially to present several logical processors by mapping these onto a single physical processor. Network cards and storage are also virtualised and presented as individual devices to VMs.|$|E
40|$|The {{production}} of 2 D and 3 D animated films demands high end systems with massive <b>computing</b> <b>resource.</b> Handling massive <b>computing</b> <b>resources</b> requires high investment in infrastructure and maintenance, which is amajor hindrance for the animation industry / animators {{who want to}} produce few minutes of film. The cloud services promises to deliver on-demand and scalable <b>computing</b> <b>resources</b> to the animation industry. In this paper, it is proposed to investigate the cloud services in animation. The research is designed in two case studies to analyze the scenarios in both traditional and through the cloud services...|$|R
5000|$|Distributed <b>computing</b> <b>resources</b> (Estonian Scientific <b>Computing</b> Infrastructure, Estonian Grid) ...|$|R
5000|$|... avoids idling the <b>compute</b> <b>resources</b> without minute-by-minute human {{supervision}} ...|$|R
40|$|Abstract Distributed and {{cooperative}} applications {{are becoming more}} common with the widespread adoption of network-centric computing models. One common design style for such systems is that of event generation and notification. This paper presents ECho, an event delivery middleware system that uses dynamic code generation to move application-level processing to remote locations in heterogeneous distributed systems. The resulting relocation of computation has potential to significantly reduce network and <b>compute</b> <b>resource</b> requirements for many applications...|$|E
40|$|Computational {{researchers}} typically work by accessing a <b>compute</b> <b>resource</b> {{miles from}} their desk. Similarly, their simulation output or other data {{is stored in}} remote terabyte data servers. VisBench is a component-based system for visualizing and analyzing this remote data. A time-varying CFD simulation of heat exchange over a louvered fin provides sample data to demonstrate a workbench-oriented version of VisBench. An analysis technique (POD) for spatiotemporal data is described and applied to the CFD data. ...|$|E
40|$|Distributed and {{cooperative}} applications {{are becoming more}} common with the widespread adoption of network-centric computing models. One common design style for such systems is that of event generation and noti cation. This paper presents ECho, an event delivery middleware system that uses dynamic code generation to move application-level processing to remote locations in heterogeneous distributed systems. The resulting relocation of computation has potential to signi cantly reduce network and <b>compute</b> <b>resource</b> requirements for many applications. ...|$|E
5000|$|... #Subtitle level 3: UNICORE: {{easy access}} to <b>computing</b> <b>resources</b> ...|$|R
50|$|DreamHost's DreamCompute is {{a public}} cloud {{computing}} service that provides scalable <b>compute</b> <b>resources</b> for developers and entrepreneurs. DreamCompute users select the amount of <b>compute</b> <b>resources</b> and storage resources needed and define their own virtual networks. DreamCompute is powered by OpenStack and Ceph and is designed for scalability, resiliency, and security.|$|R
50|$|Device nodes are {{physical}} <b>computing</b> <b>resources</b> with processing {{memory and}} services to execute software, such as typical computers or mobile phones. An execution environment node (EEN) is a software <b>computing</b> <b>resource</b> that runs within an outer node and which itself provides a service to host and execute other executable software elements.|$|R
40|$|Cloud-based {{radio access}} {{networks}} (C-RAN) {{have been proposed}} as a cost-efficient way of deploying small cells. Unlike conven-tional RANs, a C-RAN decouples the baseband processing unit (BBU) from the remote radio head (RRH), allowing for centralized operation of BBUs and scalable deployment of light-weight RRHs as small cells. In this work, we argue that the intelligent configu-ration of the front-haul network between the BBUs and RRHs, is essential in delivering the performance and energy benefits to the RAN and the BBU pool, respectively. We then propose FluidNet- a scalable, light-weight framework for realizing the full potential of C-RAN. FluidNet deploys a log-ically re-configurable front-haul to apply appropriate transmission strategies {{in different parts of}} the network and hence cater effec-tively to both heterogeneous user profiles and dynamic traffic load patterns. FluidNet ’s algorithms determine configurations that max-imize the traffic demand satisfied on the RAN, while simultane-ously optimizing the <b>compute</b> <b>resource</b> usage in the BBU pool. We prototype FluidNet on a 6 BBU, 6 RRH WiMAX C-RAN testbed. Prototype evaluations and large-scale simulations reveal that Flu-idNet ’s ability to re-configure its front-haul and tailor transmis-sion strategies provides a 50 % improvement in satisfying traffic demands, while reducing the <b>compute</b> <b>resource</b> usage in the BBU pool by 50 % compared to baseline transmission schemes...|$|E
40|$|Adhoc {{parrallel}} processing {{is one of}} {{the killer}} application of the cloud. most of the companies integrated into frame work for parallel data processing making essy to the customer. the processing frame work currently have been used static. homogeneous cluster setsup particularly nature of the cloud. the allocated <b>compute</b> <b>resource</b> big parts of the submitted job. increase processing time and cost. in this paper discuse about the oppurtunities and challenges for efficient parallel data processing in the cloud. nephele algorithm is used to the our project. nephele is a processing frame work. it is used to dynamic allocation. reduce the time and cos...|$|E
40|$|This paper {{develops}} a multi-objective optimisation model to <b>compute</b> <b>resource</b> allocation,shelter assignment and routing options to evacuate late evacuees from affected areas to shelters. Three bushfire scenarios are analysed to incorporate constraints of restricted time-window and potential road disruptions. Capacity {{and number of}} rescue vehicles and shelters are other constraints that are identical in all scenarios. The proposed mathematical model is solved by ?-constraint approach. Objective functions are simultaneously optimised to maximise {{the total number of}} evacuees and assigned rescue vehicles and shelters. We argue that this model provides a scenario-based decision-making platform to aid minimise resource utilisation and maximise coverage of late evacuees...|$|E
40|$|<b>Computing</b> <b>resources</b> {{supporting}} our {{activities are}} moving over from PC and workstations to the Clouds across Internet. Cloud is collective <b>computing</b> <b>resources,</b> which consist from computers, network, data storage, and data themselves. Cloud serves {{a large number}} of users and clients and it is pushing the demand for large-scale dat...|$|R
5000|$|Volunteer {{computing}} {{is a type}} of distributed computing,"an {{arrangement in}} which people, so-called volunteers, provide <b>computing</b> <b>resources</b> to projects, which use the resources to do distributed computing and/or storage". Thus, computer owners or users donate their <b>computing</b> <b>resources</b> (such as processing power and storage) to one or more [...] "projects".|$|R
30|$|CLUSTER: {{a set of}} <b>COMPUTE</b> <b>resources</b> being {{subject of}} auto-scaling.|$|R
40|$|Computational fluid {{dynamics}} simulations of relevance to jet-engine design, for instance, are extremely computationally demanding {{and the use}} of large-scale distributed computing will allow the solution of problems that cannot be tackled using current resources. It is often appropriate to leave the large datasets generated by CFD codes local to the <b>compute</b> <b>resource</b> in use at the time. This naturally leads to a distributed database of results that will need to be federated as a coherent resource for the engineering community. We describe the use of Globus and Condor within Cambridge for sharing compute resources, progress on defining XML standards for the annotation of CFD datasets and a distributed database framework for them...|$|E
40|$|Volatile {{computing}} environments such as desktop grids {{differs from}} traditional {{systems in the}} high volatility of compute nodes in both reachability and availability of <b>compute</b> <b>resource.</b> As a result, different fault tolerant techniques are required to ensure efficient execution of parallel jobs. This technical report summarizes failure and availability patterns of distributed computing systems; and propose simple models for characterizing the impact of parameters on the efficiency of checkpoint with restart and replication schemes. Our analysis shows that {{when the number of}} processors and/or the failure rate are high, it is indeed beneficial to use replication as it renders large speedup in comparison to checkpoint with restart with the optimal parameter settings. ...|$|E
40|$|Rendering {{volumetric}} data, as a compute/communication {{intensive and}} highly parallel application, represents {{the characteristics of}} future workloads for desktop computers. Interactively rendering volumetric data has been a challenging problem due to its high computational and communication requirements. With the consistent trend toward high resolution data, it has remained a difficult problem despite the continuous increase in processing power, because of the increasing performance gap between computation and communication. On the other hand, the new multi-core architecture trend in computational units in PC, which can be characterized by parallelism and heterogeneity, provides both opportunities and challenges. While the new on-chip parallel architectures offer opportunities for extremely high performance, widespread use of those parallel processors requires extensive changes in previous algorithms {{to take advantage of}} the new architectures. In this dissertation, we develop new methods and techniques to support interactive rendering of large volumetric data. In particular, we present a novel method to layout data on disk for efficiently performing an out-of-core axis-aligned slicing of large multidimensional scalar fields. We also present a new method to efficiently build an out-of-core indexing structure for n-dimensional volumetric data. Then, we describe a streaming model for efficiently implementing volume ray casting on a heterogeneous <b>compute</b> <b>resource</b> environment. We describe how we implement the model on SONY/TOSHIBA/IBM Cell Broadband Engine and on NVIDIA CUDA architecture. Our results show that our out-of-core techniques significantly reduce the communication bandwidth requirements and that our streaming model very effectively makes use of the strengths of those heterogeneous parallel <b>compute</b> <b>resource</b> environment for volume rendering. In all cases, we achieve scalability and load balancing, while hiding memory latency...|$|E
30|$|To {{summarize}} it, Listing 2 {{contains a}} list of resource types {{that need to be}} provisioned (two <b>compute</b> <b>resources</b> and one storage). For the <b>compute</b> <b>resources,</b> information related to the vendor, sla, vm image, deployment information, security information are specified. Storage resource has only information related to the vendor and sla defined.|$|R
40|$|This {{viewgraph}} presentation {{provides information}} on NASA's geographically dispersed <b>computing</b> <b>resources,</b> and the various methods by which the disparate technologies are integrated within a nationwide computational grid. Many large-scale science and engineering projects are accomplished through the interaction of people, heterogeneous <b>computing</b> <b>resources,</b> information systems and instruments at different locations. The overall goal is to facilitate the routine interactions of these resources to reduce the time spent in design cycles, particularly for NASA's mission critical projects. The IPG (Information Power Grid) seeks to implement NASA's diverse <b>computing</b> <b>resources</b> in a fashion {{similar to the way}} in which electric power is made available...|$|R
5000|$|... #Caption: The carrier cloud synchronizes {{delivery}} of network and <b>compute</b> <b>resources</b> ...|$|R
