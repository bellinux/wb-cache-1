287|395|Public
5000|$|TCAM <b>Content</b> <b>addressable</b> <b>memory</b> (hardware {{acceleration}} of route-search) ...|$|E
50|$|A true set-associative cache tests all the {{possible}} ways simultaneously, using something like a <b>content</b> <b>addressable</b> <b>memory.</b> A pseudo-associative cache tests each possible way one at a time. A hash-rehash cache and a column-associative cache are examples of a pseudo-associative cache.|$|E
50|$|FIBs {{may also}} be {{implemented}} with fast hardware lookup mechanisms, such as ternary <b>content</b> <b>addressable</b> <b>memory</b> (TCAM). TCAM, however, is quite expensive, and tends to be used more in edge routers with relatively small numbers of routes than in routers that must carry full Internet routing tables, with supplementary internal routes.|$|E
40|$|We {{present an}} {{implementation}} of NAT (Network Address Translation) for the NetFPGA platform capable of line-rate Gigabit Ethernet. Our implementation features RAM and CAM (Random Access and <b>Content</b> <b>Addressable)</b> <b>memories</b> for a fast and efficient NAT table. Several simulation and regression tests are included. 1...|$|R
40|$|Abstract—Ternary <b>content</b> <b>addressable</b> <b>memories</b> (TCAMs) are {{attractive}} for {{applications such as}} packet forwarding and classification in network routers. However, the high cost and power consumption are limiting their popularity and versatility. In this paper, we present a comparative study of the design techniques for low-power TCAMs. I...|$|R
40|$|Associative or <b>content</b> <b>addressable</b> <b>memories</b> can be {{used for}} many {{computing}} applications. This paper dis-cusses fault modeling for the <b>content</b> <b>addressable</b> mem-ory (CAM) chips. Detailed examination of a single CAM cell is presented. A functional fault model for a CAM architecture executing exact match derived from the single cell model is presented. Eficient testing strategy can be derived using the proposed fault model. ...|$|R
5000|$|Content-addressable: Each {{individually}} accessible unit {{of information}} is selected based {{on the basis of}} (part of) the contents stored there. Content-addressable storage can be implemented using software (computer program) or hardware (computer device), with hardware being faster but more expensive option. Hardware <b>content</b> <b>addressable</b> <b>memory</b> is often used in a computer's CPU cache.|$|E
50|$|Various {{forms of}} fast RAM and, eventually, basic <b>content</b> <b>addressable</b> <b>memory</b> (CAM) {{were used to}} speed lookup. CAM, while useful in layer 2 {{switches}} that needed to look up {{a relatively small number}} of fixed-length MAC addresses, had limited utility with IP addresses having variable-length routing prefixes (see Classless Inter-Domain Routing). Ternary CAM (CAM), while expensive, lends itself to variable-length prefix lookups.|$|E
50|$|Dr. Dudley Allen Buck (1927-1959) was an {{electrical}} engineer and inventor of components for high-speed computing devices in the 1950s. He {{is best known for}} invention of the cryotron, a superconductive computer component that is operated in liquid helium at a temperature near absolute zero. Other inventions were ferroelectric memory, <b>content</b> <b>addressable</b> <b>memory,</b> non-destructive sensing of magnetic fields, and writing printed circuits with a beam of electrons.|$|E
40|$|International audienceThis paper {{presents}} an innovative approach to detect soft errors in Ternary <b>Content</b> <b>Addressable</b> <b>Memories</b> (TCAMs) {{based on the}} use of Bloom Filters. The proposed approach is described in detail and its performance results are presented. The advantages of the proposed method are that no modifications to the TCAM device are required, the checking is done on-line and the approach has low power and area overheads...|$|R
40|$|Architectural {{innovations}} are {{reducing the}} dynamic power in ternary <b>content</b> <b>addressable</b> <b>memories</b> (TCAMs). Thus, the static power {{is becoming a}} significant portion of the total TCAM power. This paper presents two novel ternary storage cells that exploit the unique properties of TCAMs for reducing the cell leakage. Simulation results of the proposed cells show up to 41 % leakage reduction over the conventional TCAM cell. I...|$|R
40|$|This paper {{presents}} an innovative approach to detect soft errors in Ternary <b>Content</b> <b>Addressable</b> <b>Memories</b> (TCAMs) {{based on the}} use of Bloom Filters. The proposed approach is described in detail and its performance results are presented. The advantages of the proposed method are that no modifications to the TCAM device are required, the checking is done on-line and the approach has low power and area overheads...|$|R
50|$|STARAN {{might be}} the first commercially {{available}} computer designed around an associative memory. The STARAN computer was designed and built by Goodyear Aerospace Corporation. It is a Content Addressable Parallel Processor (CAPP), a type of parallel processor which uses <b>content</b> <b>addressable</b> <b>memory.</b> STARAN is a single instruction, multiple data array processor with a 4x256 1-bit processing element (PE) computer. The STARAN machines became available in 1972.|$|E
50|$|Dudley Buck invented {{recognition}} unit memory. Also called <b>content</b> <b>addressable</b> <b>memory,</b> it is {{a technique}} of storing and retrieving data {{in which there is}} no need to know the location of that data. Not only is there no need to query an index for the location of data, the inquiry for data is broadcast to all memory elements simultaneously; thus data retrieval time is independent of the size of the database.|$|E
5000|$|... eSilicon {{specializes in}} a high-performance, {{high-bandwidth}} IP + 2.5D solution that targets networking, high-performance computing, artificial intelligence (AI) and 5G wireless infrastructure applications by offering specialized memories with 2.5GHz worst-case operation {{with more than}} a billion searches per second along with the 2.5D integration of 1024 Gbytes/sec data rate high-bandwidth memory (HBM2). Memory and I/O products in this category include ternary <b>content</b> <b>addressable</b> <b>memory</b> (TCAMs), fast cache, multi-port and asynchronous register files and HBM2 PHY.|$|E
40|$|The {{storage and}} {{retrieval}} of information in networks of biological neurons can be modeled by certain types of <b>content</b> <b>addressable</b> <b>memories</b> (CAMs). We demonstrate numerically {{that the amount of}} information that can be stored in such CAMs is substantially increased by an unlearning algorithm. Mechanisms for the increase in capacity are identified and illustrated in terms of an energy function that describes the convergence properties of the network...|$|R
40|$|Hardware and {{simulation}} results for a 370 -MHz memory built-in self-test state machine are presented. Dynamic differential cascode voltage switch logic, unique clocking techniques, and logic pipelining {{were used to}} achieve the 370 -MHz performance. Testing of multiple SRAMs and <b>content</b> <b>addressable</b> <b>memories</b> is accomplished with deterministic patterns generated by the state machine. Inclusion of a programmable pattern implemented via scan initialization provides test-pattern flexibility. Failing addresses are stored for redundancy implementation...|$|R
40|$|<b>Content</b> <b>addressable</b> <b>memories</b> (CAMs) {{are gaining}} {{popularity}} with computer networks. Testing costs of CAMs are extremely high owing to their unique configuration. In this paper, we {{carried out a}} transistor-level fault analysis and devise a search path test algorithm. The proposed algorithm is of the order (nl / log 2 n) compared to the brute-force algorithm of complexity (nl). For the analyzed CAM, the search path test complexity is reduced by 30 x. 1...|$|R
5000|$|Grenander’s Pattern Theory {{treatment}} of Bayesian inference in {{seems to be}} skewed towards on image reconstruction (e.g. <b>content</b> <b>addressable</b> <b>memory).</b> That is given image I-deformed, find I. However, Mumford’s interpretation of Pattern Theory is broader and he defines PT to include many more well-known statistical methods. Mumford’s criteria for inclusion of a topic as Pattern Theory are those methods [...] "characterized by common techniques and motivations", such as the HMM, EM algorithm, dynamic programming circle of ideas. Topics in this section will reflect Mumford's {{treatment of}} Pattern Theory. His principle of statistical Pattern Theory are the following: ...|$|E
50|$|Associativist {{representations}} {{are most}} often described with connectionist systems. In connectionist systems, representations are distributed across all the nodes and connection weights {{of the system and}} thus are said to be sub symbolic. It is worth noting that a connectionist system is capable of implementing a symbolic system. There are several important aspects of neural nets that suggest that distributed parallel processing provides a better basis for cognitive functions than symbolic processing. Firstly, the inspiration for these systems came from the brain itself indicating biological relevance. Secondly, these systems are capable of storing <b>content</b> <b>addressable</b> <b>memory,</b> which is far more efficient than memory searches in symbolic systems. Thirdly, neural nets are resilient to damage while even minor damage can disable a symbolic system. Lastly, soft constraints and generalization when processing novel stimuli allow nets to behave more flexibly than symbolic systems.|$|E
50|$|In 1987 {{an article}} was {{published}} detailing {{the development of}} a character string search engine (SSE) for rapid text retrieval on a double-metal 1.6-μm n-well CMOS solid-state circuit with 217,600 transistors lain out on a 8.62x12.76-mm die area. The SSE accommodated a novel string-search architecture which combines a 512-stage finite-state automaton (FSA) logic with a <b>content</b> <b>addressable</b> <b>memory</b> (CAM) to achieve an approximate string comparison of 80 million strings per second. The CAM cell consisted of four conventional static RAM (SRAM) cells and a read/write circuit. Concurrent comparison of 64 stored strings with variable length was achieved in 50 ns for an input text stream of 10 million characters/s, permitting performance despite the presence of single character errors in the form of character codes. Furthermore, the chip allowed nonanchor string search and variable-length `don't care' (VLDC) string search.|$|E
40|$|Symbolic {{trajectory}} evaluation (Seger and Bryant, 1995) or STE {{in short}} {{has been successfully}} used in verification of large industrial sized circuit designs. The data abstraction in STE via Xs is often insufficient to bring about any reasonable reduction {{in the size of}} the verification problem. For circuits with large number of state holding elements like random access <b>memories</b> (RAM), <b>content</b> <b>addressable</b> <b>memories</b> (CAM) and caches, these reduction techniques are not adequate...|$|R
40|$|Abstract. <b>Content</b> <b>Addressable</b> <b>Memories</b> or CAMs {{are popular}} {{parallel}} matching circuits. They provide the capability, in hardware, to search {{a table of}} data for a matching entry. This functionality is a high performance alternative to popular software-based searching schemes. CAMs are typically found in embedded circuitry where fast matching is essential. This paper presentsanovel approach to CAM implementation using FPGAs and run-time recon guration. This approach produces CAM circuits that are smaller, faster and more exible than traditional approaches. ...|$|R
40|$|We propose {{algorithms}} for {{distributing the}} classifier rules to two TCAMs (ternary <b>content</b> <b>addressable</b> <b>memories)</b> and for incrementally updating the TCAMs. The performance of our scheme is compared against the prevalent scheme of storing classifier rules {{in a single}} TCAM in priority order. Our scheme results in an improvement in average lookup speed by up to 48 % and our experiments demonstrate an improvement in update performance by up to 2. 8 times {{in terms of the}} number of TCAM writes...|$|R
50|$|Grossberg {{was then}} hired as an {{assistant}} professor of applied mathematics at MIT on the strength of his PhD thesis and strong recommendations from Kac and Rota. At MIT, Grossberg was kindly received by Norman Levinson, at that time the most famous MIT mathematician and an Institute Professor, and his wife Zipporah, or Fagi, who treated him like a scientific son. Levinson and Rota, who returned to MIT when Grossberg arrived there, each submitted some of Grossberg’s early articles in 1967-1971 on the foundational concepts and equations of neural networks, global <b>content</b> <b>addressable</b> <b>memory</b> theorems, and constructions of specialized networks for spatial and spatio-temporal pattern learning, for publication in prestigious scientific and mathematical journals, notably the Proceedings of the National Academy of Sciences. In 1969, Grossberg was promoted to associate professor after publishing a stream of conceptual and mathematical results about many aspects of neural networks.|$|E
5000|$|Training a Hopfield net {{involves}} {{lowering the}} energy of states that the net should [...] "remember". This allows the net {{to serve as a}} <b>content</b> <b>addressable</b> <b>memory</b> system, that is to say, the network will converge to a [...] "remembered" [...] state if it is given only part of the state. The net can be used to recover from a distorted input to the trained state that is most similar to that input. This is called associative memory because it recovers memories on the basis of similarity. For example, if we train a Hopfield net with five units so that the state (1, -1, 1, -1, 1) is an energy minimum, and we give the network the state (1, -1, -1, -1, 1) it will converge to (1, -1, 1, -1, 1). Thus, the network is properly trained when {{the energy of}} states which the network should remember are local minima.|$|E
50|$|After taking 90 credits of {{graduate}} mathematics and reading extensively in multiple fields, Grossberg therefore left Stanford in 1964 with an MS {{in mathematics and}} transferred to The Rockefeller Institute for Medical Research (now The Rockefeller University) in Manhattan, which {{had a number of}} famous neuroscientists on its faculty as well as mathematicians and physicists who might be interested in behavioral and neural modeling, notably the famous probability theorist and statistical physicist, Mark Kac. In his first year at Rockefeller, Grossberg wrote a 440-page student monograph called The Theory of Embedding Fields with Applications to Psychology and Neurophysiology that summarized his discoveries over the past decade. The monograph was distributed by Rockefeller to 125 of the leading labs in psychology and neuroscience at that time. Grossberg received a PhD in mathematics from Rockefeller in 1967 for a thesis that proved the first global <b>content</b> <b>addressable</b> <b>memory</b> theorems about the neural learning models that he had discovered at Dartmouth. His PhD thesis advisor was Gian-Carlo Rota, whose unusual breadth as a mathematician and philosopher enabled him to provide personal and political support for Grossberg’s unusual research interests.|$|E
40|$|<b>Content</b> <b>addressable</b> <b>memories</b> (CAMs) {{are gaining}} {{popularity}} with computer networks. Testing costs of CAMs are extremely high owing to their unique configuration. In this thesis, a fault analysis {{is carried out}} on an industrial ternary CAM (TCAM) design, and search path test algorithms are designed. The proposed algorithms are able to test the TCAM array, multiple-match resolver (MMR), and match address encoder (MAE). The tests represent a 6 x decrease in test complexity compared to existing algorithms, while dramatically improving fault coverage...|$|R
40|$|<b>Content</b> <b>addressable</b> <b>memories</b> (CAMs) {{are very}} {{attractive}} for high-speed table lookups in modern network systems. This paper presents a low-power dual match line (ML) ternary CAM (TCAM) {{to address the}} power consumption issue of CAMs. The highly capacitive ML {{is divided into two}} segments to reduce the active capacitance and hence the power. We analyze possible cases of mismatches and demonstrate a significant reduction in power (up to 43 %) for a small penalty in search speed (4 %). 1...|$|R
40|$|The {{microprocessor}} device MONICA {{is used in}} the TASSO experiment at PETRA. Its task is {{to reconstruct}} events in the cylindrical driftchamber on-line. Used as an event filter MONICA provides a 2 prong trigger without any further requirements. The speed of the processor (event reconstruction times must be in the order of 1 ms) is achieved by a 4 * 4 bit slice processor in ECL technology, <b>content</b> <b>addressable</b> <b>memories</b> and table look-up. The track finding efficiency is 80 %. (6 refs) ...|$|R
50|$|Building on his 1964 Rockefeller PhD thesis, in the 1960s and 1970s, Grossberg {{generalized}} the Additive and Shunting {{models to}} a class of dynamical systems that included these models as well as non-neural biological models, and proved <b>content</b> <b>addressable</b> <b>memory</b> theorems for this more general class of models. As part of this analysis, he introduced a Liapunov functional method to help classify the limiting and oscillatory dynamics of competitive systems by keeping track of which population is winning through time. This Liapunov method led him and Michael Cohen to discover in 1981 and publish in 1982 and 1983 a Liapunov function {{that they used to}} prove that global limits exist in a class of dynamical systems with symmetric interaction coefficients that includes the Additive and Shunting models. John Hopfield published this Liapunov function for the Additive model in 1984. Some scientists started to call Hopfield’s contribution the Hopfield model. In an attempt to correct this historical error, other scientists called the more general model and Liapunov function the Cohen-Grossberg model. Still other scientists call it the Cohen-Grossberg-Hopfield model. In 1987, Bart Kosko adapted the Cohen-Grossberg model and Liapunov function, which proved global convergence of STM, to define an Adaptive Bidirectional Associative Memory that combines STM and LTM and which also globally converges to a limit.|$|E
40|$|A <b>Content</b> <b>Addressable</b> <b>Memory</b> (CAM) is {{hardware}} {{search engine}} in a memory unit that can perform a single clock cycle search operation. <b>Content</b> <b>Addressable</b> <b>Memory</b> is mostly {{used with the}} static RAM with the comparison unit to compare data in a single clock cycle leads to more power consumption. This paper proposes a <b>Content</b> <b>Addressable</b> <b>Memory</b> using a NAND cell structure with the comparison unit as parity bit and sensing technique as Match-line Sensing. This architecture reduces the power consumption in <b>Content</b> <b>Addressable</b> <b>Memory</b> based on combination and modification on power saving sensing technique...|$|E
40|$|Abstract: The <b>content</b> <b>addressable</b> <b>memory</b> is {{a memory}} unit that uses content {{matching}} instead of addresses. <b>content</b> <b>addressable</b> <b>memory</b> {{are used in}} different networking, telecommunications and storage applications because of their parallel, fast search capabilities. This paper presents a new method (called array method) for designing Reconfigurable <b>content</b> <b>addressable</b> <b>memory</b> (RCAM). The behavior of the new method was described using VHDL and implemented using FPGA technique. Then, {{the performance of the}} method was compared to other traditional <b>content</b> <b>addressable</b> <b>memory</b> design methods. The proposed RCAM is configured and used as the main part of different network security devices and unit...|$|E
40|$|Abstract—We propose {{algorithms}} for {{distributing the}} classifier rules to two TCAMs (ternary <b>content</b> <b>addressable</b> <b>memories)</b> and for incrementally updating the TCAMs. The performance of our scheme is compared against the prevalent scheme of storing classifier rules {{in a single}} TCAM in priority order. Our scheme results in an improvement in average lookup speed by up to 49 % and an improvement in update performance by up to 3. 72 times {{in terms of the}} number of TCAM writes. Index Terms—Packet classifiers, TCAM, updates. I...|$|R
40|$|A {{mixed order}} hyper network (MOHN) is a neural network in which weights can connect {{any number of}} neurons, rather than the usual two. MOHNs {{can be used as}} <b>content</b> <b>addressable</b> <b>memories</b> (CAMs) with higher {{capacity}} than standard Hopfield networks. They can also be used for regression learning of functions in ƒ : {− 1, 1 }n→R in which the turning points are equivalent to memories in a CAM. This paper presents a number of methods for learning an energy function from data that can act as either a CAM or a regression model and presents the advantages of using such an approach...|$|R
40|$|In {{this paper}} we shall present a fully {{synchronous}} digital {{implementation of the}} Address Event Representation (AER) communication scheme {{that has been used}} in the PERPLEXUS chip in order to permit the emulation of large-scale biologically inspired spiking neural networks models. By introducing specific commands in the AER protocol it is possible to distribute the AER bus among a large number of chips where the functionality of the spiking neurons is being emulated. A careful design of the AER encoder module using compact <b>Content</b> <b>Addressable</b> <b>Memories</b> (CAMs) allows for a feasible realization of large-scale models. Postprint (published version...|$|R
