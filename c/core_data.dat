1466|1841|Public
25|$|<b>Core</b> <b>Data</b> is {{the object}} {{persistence}} framework included with Foundation and Cocoa and found in Cocoa.h.|$|E
25|$|IBM Training {{includes}} {{a complete set}} of <b>core</b> <b>Data</b> Servers Training courses that apply to Informix. These courses delve into many essential Informix concepts, from fundamentals to advanced SQL topics.|$|E
25|$|Cocoa {{consists}} of the Foundation Kit, Application Kit, and <b>Core</b> <b>Data</b> frameworks, as included by the Cocoa.h header file, and the libraries and frameworks included by those, such as the C standard library and the Objective-C runtime.|$|E
5000|$|Privacy Analytics <b>CORE</b> for Structured <b>Data</b> <b>CORE</b> for {{structured}} <b>data</b> is {{a software}} tool {{based on the}} HIPAA Expert Determination Method that de-identifies data based on maximizing data granularity while still protecting subject privacy.|$|R
30|$|The {{data from}} Well#A (1046 <b>core</b> and log <b>data)</b> {{were chosen to}} provide the {{training}} patterns. This well was chosen because, it had the most complete set of <b>core</b> and log <b>data.</b> It was randomly divided into training data (70  %) and testing data (30  %). The data from Well#B (152 <b>core</b> and log <b>data),</b> Well#C (91 <b>core</b> and log <b>data)</b> and Well#D (40 <b>core</b> and log <b>data)</b> were {{used to test the}} model’s ability to predict porosity in the oilfield.|$|R
30|$|Usually {{there is}} a {{relatively}} clear correlation between core porosity and <b>core</b> permeability <b>data,</b> but this correlation is somewhat ambiguous due to scattered data resulting from differences in the depositional environment, clay content and diagenetic development. The variability in the <b>core</b> analysis <b>data</b> means that a perfect correlation between core porosity and <b>core</b> permeability <b>data</b> cannot be obtained. Despite these uncertainties, this computed porosity–permeability relation forms the basis of calculating a log-based permeability curve for each well using the log-derived porosity as input data.|$|R
25|$|Nye cited radiometric dating, ice <b>core</b> <b>data,</b> and {{the light}} from distant stars {{to argue that the}} Earth must be much older than young Earth creationists like Ham theorize. Ham argued that these various dating methods are untrustworthy because they often give varying ages for the same artifacts.|$|E
25|$|Over {{the past}} 800,000 years, ice <b>core</b> <b>data</b> shows that carbon dioxide has varied from values {{as low as}} 180 ppm to the pre-industrial level of 270 ppm. Paleoclimatologists {{consider}} variations in carbon dioxide concentration to be a fundamental factor influencing climate variations over this time scale.|$|E
25|$|With {{the arrival}} of Mac OS X 10.4, Apple {{extended}} this foundation further by introducing the <b>Core</b> <b>Data</b> framework, which standardizes change tracking and persistence in the model layer. In effect, the framework greatly simplifies {{the process of making}} changes to application data, undoing changes (if necessary), saving data to disk, and reading it back in.|$|E
30|$|Analyze the <b>core</b> samples <b>data</b> to {{determine}} such individual sedimentary facies in wells.|$|R
5000|$|The major {{elements}} of a <b>core</b> architecture <b>data</b> model are described as follows: ...|$|R
5000|$|... #Caption: Icinga modular {{architecture}} with <b>Core,</b> Icinga <b>Data</b> Out Database, Web as of v1.5 ...|$|R
25|$|Despite these {{different}} estimates, scientists {{agree that a}} supereruption of the scale at Toba must have led to very extensive ash-fall layers and injection of noxious gases into the atmosphere, with worldwide effects on weather and climate. In addition, the Greenland ice <b>core</b> <b>data</b> display an abrupt climate change around this time, {{but there is no}} consensus that the eruption directly generated the 1,000-year cold period seen in Greenland or triggered the last glaciation.|$|E
25|$|In Northwest Greenland, ice {{coverage}} {{attained a}} very early maximum in the last glacial period around 114,000. After this early maximum, the ice coverage was similar to today {{until the end of}} the last glacial period. Towards the end, glaciers readvanced once more before retreating to their present extent. According to ice <b>core</b> <b>data,</b> the Greenland climate was dry during the last glacial period, precipitation reaching perhaps only 20% of today's value.|$|E
25|$|FiveThirtyEight's {{predictions}} for each state primary, {{both for the}} Republican and the Democratic party nominations, were based on statistical analysis, not on the analyst's opinions. The <b>core</b> <b>data</b> employed were polls, which FiveThirtyEight aggregated for each state (while also considering national polls) using essentially the same method it had employed since 2008. In the 2016 primaries, the projections also took into account endorsements. The website also kept track of the accumulation of national party convention delegates. In a comparison of prediction success published by Bloomberg News after the primary season was completed, FiveThirtyEight's prediction success tied for {{the highest percentage of}} correct primary poll winners, at 92%; but it lagged behind PredictWise in predicting a larger set of primaries. Notably, even with FiveThirtyEights track record of correctly predicting elections that pollsters get wrong, it still missed Bernie Sanders's upset victory in the Michigan primary, for instance, regarded as the 'one of the biggest upsets in modern political history'.|$|E
30|$|The {{conventional}} <b>core</b> analysis <b>data</b> {{from the}} Børglum- 1 and Flyvbjerg- 1 wells are also plotted after applying a porosity cut-off (Fig.  11). The mid-line honours calculated averages of the <b>core</b> analysis <b>data</b> {{from the two}} wells. Data points plotting below the uncertainty envelope were, however, excluded prior to calculating averages, because these data points represent sandstones with large amounts of fine-grained material (shale, mudstone and siltstone) not contributing to reservoir performance. The width of the uncertainty band is transferred from our Stenlille study, i.e. factors of 2 and 0.5 define {{the high and low}} bounds. The uncertainty band does not address the spread in the actual <b>core</b> analysis <b>data,</b> but it points out an uncertainty range that is related to the average permeability for a typical reservoir, presuming that the reservoir sandstones are almost shale-free. Furthermore, the example demonstrates {{that it is difficult to}} narrow the uncertainty range on the basis of a limited set of conventional <b>core</b> analysis <b>data.</b>|$|R
30|$|Calibrate well-log <b>data</b> with <b>core</b> {{analysis}} <b>data</b> {{to interpret}} the sedimentary facies along the well.|$|R
30|$|Rheological {{properties}} of emulsions were calculated theoretically from the <b>core</b> flooding <b>data</b> of emulsion flow tests.|$|R
2500|$|During {{this last}} glacial period, {{there were several}} changes between glacier advance and retreat. The Last Glacial Maximum, the maximum extent of {{glaciation}} within the last glacial period, was approximately 22,000 years ago. While the general pattern of global cooling and glacier advance was similar, local differences {{in the development of}} glacier advance and retreat make it difficult to compare the details from continent to continent (see picture of ice <b>core</b> <b>data</b> below for differences). Approximately 13,000 years ago, the Late Glacial Maximum began. Around 11,700 years ago marked the [...] beginning of the Holocene geological epoch, which includes the Holocene glacial retreat.|$|E
2500|$|CO2 is {{produced}} by fossil fuel burning and other activities such as cement production and tropical deforestation. [...] Measurements of CO2 from the Mauna Loa observatory show that concentrations have increased from about 313 parts per million (ppm) in 1960 to about 389 ppm in 2010. It reached the 400 ppm milestone on May 9, 2013. [...] The current observed amount of CO2 exceeds the geological record maxima (~300 ppm) from ice <b>core</b> <b>data.</b> The effect of combustion-produced carbon dioxide on the global climate, a special case of the greenhouse effect first described in 1896 by Svante Arrhenius, has also been called the Callendar effect.|$|E
2500|$|The Temagami Mining and Milling Company {{operated}} a concentrating plant and worked two shafts, one open cut and one adit from 1905 to 1908. A former railway siding, known as Grey's siding, {{was used to}} ship ore away from the mine site. The shipped ore averaged [...] of gold per ton. Analysis of drill <b>core</b> <b>data</b> ranged from trace to [...] of gold per ton and from trace to [...] of silver per ton. Ore was hoisted by skip then dumped into a Blake jaw crusher at the concentrating plant. It was then dried, passed through roll crushers and elevated to trommels for sizing. The oversized ore passed down through a fine roll crusher. After the reduced ore fragments passed through roll crushers, it was then put through three Kriem air separators in the mill. The ore concentrates from these separators were drawn off and bagged for shipping. A [...] long and [...] wide arsenic vein was also mined during this period. All of the mining infrastructure was subsequently destroyed by a forest fire and operations were not renewed. There are no data available on how much the mine produced. Since the closure of Big Dan, the adit and shafts have been flooded. Remains of the former concentrating plant are also present.|$|E
5000|$|... pSeven MDO (Workflow construction, {{execution}} and post-processing capabilities - pSeven <b>Core</b> for <b>data</b> analysis and optimization) ...|$|R
30|$|Establishing {{an initial}} {{permeability}} model {{based on a}} conventional porosity–permeability plot using <b>core</b> analysis <b>data</b> from all <b>cored</b> study wells.|$|R
50|$|<b>Core</b> {{architecture}} <b>data</b> model (CADM) in {{enterprise architecture}} {{is a logical}} data model of information used to describe and build architectures.|$|R
5000|$|<b>Core</b> <b>data</b> {{integration}} {{is the use}} of data integration technology for a significant, centrally planned and managed IT initiative within a company. Examples of <b>core</b> <b>data</b> integration initiatives could include: ...|$|E
50|$|The Mac OS X <b>Core</b> <b>Data</b> API helps {{developers}} create data {{structures for}} their applications. <b>Core</b> <b>Data</b> provides undo, redo and save functions for developers without them having to write any code.|$|E
50|$|<b>Core</b> <b>data</b> {{integrations}} {{are often}} {{designed to be}} enterprise-wide integration solutions. They may be {{designed to provide a}} data abstraction layer, which in turn will be used by individual <b>core</b> <b>data</b> integration implementations, such as ETL servers or applications integrated through EAI.|$|E
40|$|The uses of soft {{computing}} techniques comprising artificial neural networks, {{fuzzy logic}} and genetic algorithms are emerging {{for the building}} of permeability interpretation models in well log data analysis. Regardless of which soft computing techniques are used, they rely {{on a set of}} <b>core</b> permeability <b>data</b> to give {{a better understanding of the}} formation. However uncertainties and errors with the <b>core</b> permeability <b>data</b> may undetermined the accuracy of permeability determination. This paper examines the problems that could possibly appear in the <b>core</b> permeability <b>data.</b> In most cases, data preprocessing and postprocessing are required to ensure that the permeability determination is successful. In this paper, soft computing techniques that are mainly based on fuzzy and neural networks approaches are used to assist the preprocessing and post processing stages thereby improving the overall accuracy...|$|R
30|$|Analyze the {{sedimentary}} facies of coring intervals in {{a single}} well by using the surface outcrop <b>data,</b> <b>coring</b> <b>data,</b> and lab test results. Use the analysis results to calibrate the SP, GR, etc., logs of the coring intervals and represent the sedimentary facies {{in the form of}} logs. Then, use these logs to study the sedimentary facies of non-coring intervals, thus accomplishing the sedimentary facies analysis of the target interval (single-well facies division), and build the sedimentary facies model.|$|R
30|$|Core {{description}} reports: <b>Core</b> analysis <b>data</b> of just well no. 3 {{are available}} {{which is used}} for generation permeability data in entire reservoir.|$|R
50|$|<b>Core</b> <b>Data</b> is {{an object}} graph and {{persistence}} framework provided by Apple in the macOS and iOS operating systems. It {{was introduced in}} Mac OS X 10.4 Tiger and iOS with iPhone SDK 3.0. It allows data organized by the relational entity-attribute model to be serialized into XML, binary, or SQLite stores. The data can be manipulated using higher level objects representing entities and their relationships. <b>Core</b> <b>Data</b> manages the serialized version, providing object lifecycle and object graph management, including persistence. <b>Core</b> <b>Data</b> interfaces directly with SQLite, insulating the developer from the underlying SQL.|$|E
5000|$|<b>Core</b> <b>Data</b> can {{serialize}} objects into XML, Binary, or SQLite for storage. With {{the release}} of Mac OS X 10.5 Leopard, developers can also create their own custom atomic store types. Each method carries advantages and disadvantages, such as being human readable (XML) or more memory efficient (SQLite). This portion of <b>Core</b> <b>Data</b> {{is similar to the}} original Enterprise Objects Framework (EOF) system, in that one can write fairly sophisticated queries. Unlike EOF, {{it is not possible to}} write your own SQL. Recently, <b>Core</b> <b>Data</b> store for ODBC has been made available in ODBC framework.|$|E
5000|$|For example: a {{developer}} might be writing {{a program to}} handle vCards. In order to manage these, the author intends to read the vCards into objects, and then store them in a single larger XML file. Using <b>Core</b> <b>Data</b> the developer would drag their schema from the data designer in Xcode into an interface builder window to create a GUI for their schema. They could then write standard Objective-C or Swift code to read vCard files and put the data into <b>Core</b> <b>Data</b> managed entities. From that point on the author's code manipulates these <b>Core</b> <b>Data</b> objects, rather than the underlying vCards. Connecting the [...] menu item to the appropriate method in the controller object will direct the controller to examine the object stack, determine which objects are dirty, and then re-write a <b>Core</b> <b>Data</b> document file with these changes.|$|E
5000|$|Levi 9 IT Services runs [...]NET {{development}} {{centers in}} Serbia and Ukraine and maintains <b>core</b> development, <b>data</b> management and business intelligence Microsoft competences.|$|R
40|$|Properly {{control of}} blow molding machine’s wall-thickness {{relates to the}} quality of plastic products,design on curves is the key {{technology}} of blow molding machine’s wall-thickness control. Adopted cubic spline curve as a fitting prototype,used advanced HMI as a design platform,by cooperation with HMI configuration function and macro code,an interface of curves was designed,a good performance of human-computer interaction and <b>core</b> <b>data’s</b> transmission in the whole machine were realized. By using this method,the integration of the system was increased and costs was reduced dramatically...|$|R
50|$|SNMPv1 {{specifies}} five <b>core</b> protocol <b>data</b> units (PDUs). Two other PDUs, GetBulkRequest and InformRequest {{were added}} in SNMPv2 and the Report PDU was added in SNMPv3.|$|R
