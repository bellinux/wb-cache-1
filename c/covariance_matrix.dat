10000|3259|Public
25|$|The <b>covariance</b> <b>matrix</b> {{defines a}} {{bijective}} transformation (encoding) for all solution vectors into a space, where the sampling takes place with identity <b>covariance</b> <b>matrix.</b> Because the update equations in the CMA-ES are invariant under linear coordinate system transformations, the CMA-ES can be re-written as an adaptive encoding procedure {{applied to a}} simple evolution strategy with identity <b>covariance</b> <b>matrix.</b>|$|E
25|$|Finally, the <b>covariance</b> <b>matrix</b> is updated, where {{again the}} {{respective}} evolution path is updated first.|$|E
25|$|An {{important}} {{advantage of}} the MBF {{is that it does}} not require finding the inverse of the <b>covariance</b> <b>matrix.</b>|$|E
40|$|Maximum {{likelihood}} estimation under constraints for estimation in the Wishart {{class of}} distributions, is considered. It provides a unified approach to estimation {{in a variety}} of problems concerning <b>covariance</b> <b>matrices.</b> Virtually all <b>covariance</b> structures can be translated to constraints on the covariances. This includes <b>covariance</b> <b>matrices</b> with given structure such as linearly patterned <b>covariance</b> <b>matrices,</b> <b>covariance</b> <b>matrices</b> with zeros, independent <b>covariance</b> <b>matrices</b> and structurally dependent <b>covariance</b> <b>matrices.</b> The methodology followed in this paper provides a useful and simple approach to directly obtain the exact maximum likelihood estimates. These maximum likelihood estimates are obtained via an estimation procedure for the exponential class using constraints. [URL]...|$|R
40|$|We {{consider}} {{the problem of}} constructing a fixed-size confidence region of the difference of two multinormal means when the <b>covariance</b> <b>matrices</b> have intraclass correlation structure. When the <b>covariance</b> <b>matrices</b> are known, we derive an optimal allocation. A two-stage procedure is given for the problem with unknown <b>covariance</b> <b>matrices...</b>|$|R
40|$|This {{paper is}} the third chapter {{of three of the}} author's {{undergraduate}} thesis. In this paper, we study the convergence of local bulk statistics for linearized <b>covariance</b> <b>matrices</b> under Dyson's Brownian motion. We consider deterministic initial data $V$ approximate the Dyson Brownian motion for linearized <b>covariance</b> <b>matrices</b> by the Wigner flow. Using universality results for the Wigner flow, we deduce universality for the linearized <b>covariance</b> <b>matrices.</b> We deduce bulk universality of averaged bulk correlation functions for both biregular bipartite graphs and honest <b>covariance</b> <b>matrices.</b> We also deduce a weak level repulsion estimate for the Dyson Brownian motion of linearized <b>covariance</b> <b>matrices.</b> Comment: 32 page...|$|R
25|$|Navarro {{proved that}} these bounds are sharp, that is, {{they are the}} best {{possible}} bounds for that regions when we just know the mean and the <b>covariance</b> <b>matrix</b> of X.|$|E
25|$|Although an {{explicit}} inverse {{is not necessary}} to estimate the vector of unknowns, it is unavoidable to estimate their precision, found in the diagonal of the posterior <b>covariance</b> <b>matrix</b> of the vector of unknowns.|$|E
25|$|The Ensemble Kalman Filter (EnKF) is a Monte Carlo {{implementation}} of the Bayesian update problem: given a probability density function (pdf) {{of the state of}} the modeled system (the prior, called often the forecast in geosciences) and the data likelihood, the Bayes theorem is used to obtain the pdf after the data likelihood has been taken into account (the posterior, often called the analysis). This is called a Bayesian update. The Bayesian update is combined with advancing the model in time, incorporating new data from time to time. The original Kalman Filter assumes that all pdfs are Gaussian (the Gaussian assumption) and provides algebraic formulas for the change of the mean and the <b>covariance</b> <b>matrix</b> by the Bayesian update, as well as a formula for advancing the <b>covariance</b> <b>matrix</b> in time provided the system is linear. However, maintaining the <b>covariance</b> <b>matrix</b> is not feasible computationally for high-dimensional systems. For this reason, EnKFs were developed. EnKFs represent the distribution of the system state using a collection of state vectors, called an ensemble, and replace the <b>covariance</b> <b>matrix</b> by the sample covariance computed from the ensemble. The ensemble is operated with as if it were a random sample, but the ensemble members are really not independent – the EnKF ties them together. One advantage of EnKFs is that advancing the pdf in time is achieved by simply advancing each member of the ensemble. For a survey of EnKF and related data assimilation techniques, see G. Evensen.|$|E
40|$|AbstractMultivariate {{isotonic}} regression theory plays a {{key role}} in the field of statistical inference under order restriction for vector valued parameters. Two cases of estimating multivariate normal means under order restricted set are considered. One case is that <b>covariance</b> <b>matrices</b> are known, the other one is that <b>covariance</b> <b>matrices</b> are unknown but are restricted by partial order. This paper shows that when <b>covariance</b> <b>matrices</b> are known, the estimator given by this paper always dominates unrestricted maximum likelihood estimator uniformly, and when <b>covariance</b> <b>matrices</b> are unknown, the plug-in estimator dominates unrestricted maximum likelihood estimator under the order restricted set of <b>covariance</b> <b>matrices.</b> The isotonic regression estimators in this paper are the generalizations of plug-in estimators in unitary case...|$|R
30|$|In {{checking}} {{the equality of}} the <b>covariance</b> <b>matrices</b> for the three groups using the new data (six variate data), Box M test was employed and the three <b>covariance</b> <b>matrices</b> of the sheep breeds {{were found to be}} unequal or {{at least one of the}} <b>covariance</b> <b>matrices</b> is not equal to the other. Hence, since the <b>covariance</b> <b>matrices</b> are not equal, the appropriate discriminant function to be derived for classification of the sheep breeds using the six variate data is the Quadratic Discriminant Function (QDF).|$|R
40|$|In this thesis, a Bayes linear {{methodology}} for the adjustment of <b>covariance</b> <b>matrices</b> is presented and discussed. A geometric framework for quantifying uncertainties about <b>covariance</b> <b>matrices</b> is set up, and an inner-product for spaces of random matrices {{is motivated and}} constructed. The inner-product on this space captures aspects of belief about the relationships between <b>covariance</b> <b>matrices</b> of interest, providing a structure rich enough to adjust beliefs about unknown matrices {{in the light of}} data such as sample <b>covariance</b> <b>matrices,</b> exploiting second-order exchangeability and related specifications to obtain representations allowing analysis. Adjustment is associated with orthogonal projection, and illustrated by examples for some common problems. The difficulties of adjusting the <b>covariance</b> <b>matrices</b> underlying exchangeable random vectors is tackled and discussed. Learning about the <b>covariance</b> <b>matrices</b> associated with multivariate time series dynamic linear models is shown to be amenable to a similar approach. Diagnostics for matrix adjustments are also discussed...|$|R
25|$|A {{special case}} of {{generalized}} least squares called weighted least squares occurs {{when all the}} off-diagonal entries of Ω (the correlation matrix of the residuals) are null; the variances of the observations (along the <b>covariance</b> <b>matrix</b> diagonal) may still be unequal (heteroscedasticity).|$|E
25|$|First, a maximum-likelihood principle, {{based on}} the idea to {{increase}} the probability of successful candidate solutions and search steps. The mean of the distribution is updated such that the likelihood of previously successful candidate solutions is maximized. The <b>covariance</b> <b>matrix</b> of the distribution is updated (incrementally) such that the likelihood of previously successful search steps is increased. Both updates can be interpreted as a natural gradient descent. Also, in consequence, the CMA conducts an iterated principal components analysis of successful search steps while retaining all principal axes. Estimation of distribution algorithms and the Cross-Entropy Method are based on very similar ideas, but estimate (non-incrementally) the <b>covariance</b> <b>matrix</b> by maximizing the likelihood of successful solution points instead of successful search steps.|$|E
25|$|The work of James and Stein {{has been}} {{extended}} {{to the case of}} a general measurement <b>covariance</b> <b>matrix,</b> i.e., where measurements may be statistically dependent and may have differing variances. A similar dominating estimator can be constructed, with a suitably generalized dominance condition. This can be used to construct a linear regression technique which outperforms the standard application of the LS estimator.|$|E
40|$|AbstractModern random matrix theory {{indicates}} {{that when the}} population size p is not negligible {{with respect to the}} sample size n, the sample <b>covariance</b> <b>matrices</b> demonstrate significant deviations from the population <b>covariance</b> <b>matrices.</b> In order to recover the characteristics of the population <b>covariance</b> <b>matrices</b> from the observed sample <b>covariance</b> <b>matrices,</b> several recent solutions are proposed when the order of the underlying population spectral distribution is known. In this paper, we deal with the underlying order selection problem and propose a solution based on the cross-validation principle. We prove the consistency of the proposed procedure...|$|R
40|$|Multivariate {{isotonic}} regression theory plays a {{key role}} in the field of statistical inference under order restriction for vector valued parameters. Two cases of estimating multivariate normal means under order restricted set are considered. One case is that <b>covariance</b> <b>matrices</b> are known, the other one is that <b>covariance</b> <b>matrices</b> are unknown but are restricted by partial order. This paper shows that when <b>covariance</b> <b>matrices</b> are known, the estimator given by this paper always dominates unrestricted maximum likelihood estimator uniformly, and when <b>covariance</b> <b>matrices</b> are unknown, the plug-in estimator dominates unrestricted maximum likelihood estimator under the order restricted set of <b>covariance</b> <b>matrices.</b> The isotonic regression estimators in this paper are the generalizations of plug-in estimators in unitary case. Multivariate normal mean Order restrict Graybill-Deal estimator Isotonic regression...|$|R
40|$|The Riemannian {{geometry}} of <b>covariance</b> <b>matrices</b> has been essential to several successful applications, in computer vision, biomedical signal and image processing, and radar data processing. For these applications, an important ongoing {{challenge is to}} develop Riemannian-geometric tools which are adapted to structured <b>covariance</b> <b>matrices.</b> The present paper proposes to meet this challenge by introducing {{a new class of}} probability distributions, Gaussian distributions of structured <b>covariance</b> <b>matrices.</b> These are Riemannian analogs of Gaussian distributions, which only sample from <b>covariance</b> <b>matrices</b> having a preassigned structure, such as complex, Toeplitz, or block-Toeplitz. The usefulness of these distributions stems from three features: (1) they are completely tractable, analytically or numerically, when dealing with large <b>covariance</b> <b>matrices,</b> (2) they provide a statistical foundation to the concept of structured Riemannian barycentre (i. e. Fréchet or geometric mean), (3) they lead to efficient statistical learning algorithms, which realise, among others, density estimation and classification of structured <b>covariance</b> <b>matrices.</b> The paper starts from the observation that several spaces of structured <b>covariance</b> <b>matrices,</b> considered from a geometric point of view, are Riemannian symmetric spaces. Accordingly, it develops an original theory of Gaussian distributions on Riemannian symmetric spaces, of their statistical inference, and of their relationship to the concept of Riemannian barycentre. Then, it uses this original theory to give a detailed description of Gaussian distributions of three kinds of structured <b>covariance</b> <b>matrices,</b> complex, Toeplitz, and block-Toeplitz. Finally, it describes algorithms for density estimation and classification of structured <b>covariance</b> <b>matrices,</b> based on Gaussian distribution mixture models. Comment: Final versio...|$|R
25|$|Maximum {{likelihood}} estimation can {{be performed}} when {{the distribution of the}} error terms is known to belong to a certain parametric family ƒθ of probability distributions. When fθ is a normal distribution with zero mean and variance θ, the resulting estimate is identical to the OLS estimate. GLS estimates are maximum likelihood estimates when ε follows a multivariate normal distribution with a known <b>covariance</b> <b>matrix.</b>|$|E
25|$|Adaptation of the <b>covariance</b> <b>matrix</b> {{amounts to}} {{learning}} a second order {{model of the}} underlying objective function similar to the approximation of the inverse Hessian matrix in the Quasi-Newton method in classical optimization. In contrast to most classical methods, fewer assumptions {{on the nature of}} the underlying objective function are made. Only the ranking between candidate solutions is exploited for learning the sample distribution and neither derivatives nor even the function values themselves are required by the method.|$|E
25|$|In image processing, {{processed}} {{images of}} faces {{can be seen}} as vectors whose components are the brightnesses of each pixel. The dimension of this vector space is the number of pixels. The eigenvectors of the <b>covariance</b> <b>matrix</b> associated with a large set of normalized pictures of faces are called eigenfaces; {{this is an example of}} principal component analysis. They are very useful for expressing any face image as a linear combination of some of them. In the facial recognition branch of biometrics, eigenfaces provide a means of applying data compression to faces for identification purposes. Research related to eigen vision systems determining hand gestures has also been made.|$|E
40|$|We {{introduce}} covariance reducing {{models for}} studying the sample <b>covariance</b> <b>matrices</b> of a random vector observed in different populations. The models are based on reducing the sample <b>covariance</b> <b>matrices</b> to an informational core that is sufficient to characterize the variance heterogeneity among the populations. They possess useful equivariance properties and provide a clear alternative to spectral models for <b>covariance</b> <b>matrices.</b> Copyright 2008, Oxford University Press. ...|$|R
40|$|In this paper, {{we study}} the local asymptotics of the {{eigenvalues}} and eigenvectors {{for a general}} class of sample <b>covariance</b> <b>matrices,</b> where the spectrum of the population <b>covariance</b> <b>matrices</b> can have {{a finite number of}} spikes and bulk components. Our paper is a unified framework combining the spiked model and <b>covariance</b> <b>matrices</b> without outliers. Examples and statistical applications are considered to illustrate our results...|$|R
40|$|Modern random matrix theory {{indicates}} {{that when the}} population size p is not negligible {{with respect to the}} sample size n, the sample <b>covariance</b> <b>matrices</b> demonstrate significant deviations from the population <b>covariance</b> <b>matrices.</b> In order to recover the characteristics of the population <b>covariance</b> <b>matrices</b> from the observed sample <b>covariance</b> <b>matrices,</b> several recent solutions are proposed when the order of the underlying population spectral distribution is known. In this paper, we deal with the underlying order selection problem and propose a solution based on the cross-validation principle. We prove the consistency of the proposed procedure. © 2011 Elsevier Inc. postprin...|$|R
25|$|Second, two {{paths of}} the time {{evolution}} of the distribution mean of the strategy are recorded, called search or evolution paths. These paths contain significant information about the correlation between consecutive steps. Specifically, if consecutive steps are taken in a similar direction, the evolution paths become long. The evolution paths are exploited in two ways. One path {{is used for the}} <b>covariance</b> <b>matrix</b> adaptation procedure in place of single successful search steps and facilitates a possibly much faster variance increase of favorable directions. The other path is used to conduct an additional step-size control. This step-size control aims to make consecutive movements of the distribution mean orthogonal in expectation. The step-size control effectively prevents premature convergence yet allowing fast convergence to an optimum.|$|E
25|$|Class {{prediction}} analysis: This approach, called supervised classification, {{establishes the}} basis for developing a predictive model into which future unknown test objects can be input in order to predict the most likely class membership of the test objects. Supervised analysis for class prediction involves use of techniques such as linear regression, k-nearest neighbor, learning vector quantization, decision tree analysis, random forests, naive Bayes, logistic regression, kernel regression, artificial neural networks, support vector machines, mixture of experts, and supervised neural gas. In addition, various metaheuristic methods are employed, such as genetic algorithms, <b>covariance</b> <b>matrix</b> self-adaptation, particle swarm optimization, and ant colony optimization. Input data for class prediction are usually based on filtered lists of genes which are predictive of class, determined using classical hypothesis tests (next section), Gini diversity index, or information gain (entropy).|$|E
25|$|The eigendecomposition of a {{symmetric}} positive semidefinite (PSD) matrix yields an orthogonal {{basis of}} eigenvectors, {{each of which}} has a nonnegative eigenvalue. The orthogonal decomposition of a PSD matrix is used in multivariate analysis, where the sample covariance matrices are PSD. This orthogonal decomposition is called principal components analysis (PCA) in statistics. PCA studies linear relations among variables. PCA is performed on the <b>covariance</b> <b>matrix</b> or the correlation matrix (in which each variable is scaled to have its sample variance equal to one). For the covariance or correlation matrix, the eigenvectors correspond to principal components and the eigenvalues to the variance explained by the principal components. Principal component analysis of the correlation matrix provides an orthonormal eigen-basis for the space of the observed data: In this basis, the largest eigenvalues correspond to the principal components that are associated with most of the covariability among a number of observed data.|$|E
30|$|The Riemannian {{geometry}} of Sym_+^d can be exploited when the extracted feature descriptors are <b>covariance</b> <b>matrices,</b> e.g., region <b>covariance</b> [15], since the SPD cone {{is exactly the}} set of non-singular <b>covariance</b> <b>matrices</b> [16].|$|R
40|$|ISBN 978 - 3 - 642 - 15994 - 7, SoftcoverInternational audienceIn brain-computer {{interfaces}} {{based on}} motor imagery, <b>covariance</b> <b>matrices</b> {{are widely used}} through spatial ﬁlters computation and other signal processing methods. <b>Covariance</b> <b>matrices</b> lie {{in the space of}} Symmetric Positives-Deﬁnite (SPD) matrices and therefore, fall within the Riemannian geometry domain. Using a diﬀerential geometry framework, we propose diﬀerent algorithms in order to classify <b>covariance</b> <b>matrices</b> in their native space...|$|R
40|$|The Fréchet {{distance}} between two multivariate normal distributions having means [mu]X, [mu]Y and <b>covariance</b> <b>matrices</b> [Sigma]X, [Sigma]Y {{is shown to}} be given by d 2 = [mu]X - [mu]Y 2 + tr([Sigma]X + [Sigma]Y - 2 ([Sigma]X[Sigma]Y) 1 / 2). The quantity d 0 given by d 02 = tr([Sigma]X + [Sigma]Y - 2 ([Sigma]X[Sigma]Y) 1 / 2) is a natural metric on the space of real <b>covariance</b> <b>matrices</b> of given order. Frechet distance multivariate normal distributions <b>covariance</b> <b>matrices...</b>|$|R
25|$|The {{choice of}} the kernel {{function}} K is not crucial {{to the accuracy of}} kernel density estimators, so we use the standard multivariate normal kernel throughout: , where H plays the role of the <b>covariance</b> <b>matrix.</b> On the other hand, the {{choice of the}} bandwidth matrix H {{is the single most important}} factor affecting its accuracy since it controls the amount and orientation of smoothing induced. That the bandwidth matrix also induces an orientation is a basic difference between multivariate kernel density estimation from its univariate analogue since orientation is not defined for 1D kernels. This leads to the choice of the parametrisation of this bandwidth matrix. The three main parametrisation classes (in increasing order of complexity) are S, the class of positive scalars times the identity matrix; D, diagonal matrices with positive entries on the main diagonal; and F, symmetric positive definite matrices. The S class kernels have the same amount of smoothing applied in all coordinate directions, D kernels allow different amounts of smoothing in each of the coordinates, and F kernels allow arbitrary amounts and orientation of the smoothing. Historically S and D kernels are the most widespread due to computational reasons, but research indicates that important gains in accuracy can be obtained using the more general F class kernels.|$|E
500|$|Consider a {{portfolio}} of [...] assets where the weight of asset [...] is [...] [...] The [...] form the allocation vector [...] [...] Let us further denote the <b>covariance</b> <b>matrix</b> of the assets by [...] [...] The volatility of the portfolio is defined as: ...|$|E
500|$|Statistics {{also makes}} use of {{matrices}} in many different forms. Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The <b>covariance</b> <b>matrix</b> encodes the mutual variance of several random variables. Another technique using matrices are linear least squares, a method that approximates a finite set of pairs (x1, y1), (x2, y2), ..., (x'N, y'N), by a linear function ...|$|E
40|$|Both {{predictive}} {{discriminant analysis}} (PDA) and descriptive discriminant analysis (DDA) require {{a decision to}} pool group <b>covariance</b> <b>matrices,</b> or alternatively, to retain separate group <b>covariance</b> <b>matrices</b> when the group <b>covariance</b> <b>matrices</b> are too dissimilar to pool. Pooling the group Lovariance matrices invol-;es th 3 so-called "linear " rule, generally preferred in predictive and descriptive analysis. Retaining separate group <b>covariance</b> <b>matrices</b> invokes the "quadratic " rule, resulting in a higher hit rate in PDA and a lower lambda in DDA. However, the quadratic rule is influenced by unique sampling error variance, making the generalizability of quadratic results suspect. (Contains 12 references.) (Author/SLD) Reproductions supplied by EDRS are the best {{that can be made}} from the original document...|$|R
40|$|This work is {{concerned}} with finite range bounds on the variance of individual eigenvalues of random <b>covariance</b> <b>matrices,</b> both in the bulk and {{at the edge of}} the spectrum. In a preceding paper, the author established analogous results for Wigner matrices and stated the results for <b>covariance</b> <b>matrices.</b> They are proved in the present paper. Relying on the LUE example, which needs to be investigated first, the main bounds are extended to complex <b>covariance</b> <b>matrices</b> by means of the Tao, Vu and Wang Four Moment Theorem and recent localization results by Pillai and Yin. The case of real <b>covariance</b> <b>matrices</b> is obtained from interlacing formulas. Comment: arXiv admin note: substantial text overlap with arXiv: 1203. 159...|$|R
40|$|Rao's (1973) {{test for}} {{additional}} information for discrimination between two multivariate normal populations is modified for the case of unequal <b>covariance</b> <b>matrices.</b> An application {{to the problem of}} discriminating between two geographic populations of tigers is provided. Discrimination, Rao's test Unequal <b>covariance</b> <b>matrices...</b>|$|R
