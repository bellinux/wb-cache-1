77|148|Public
5000|$|Version 4.0.7 of 30. August 2013 added {{a lot of}} enhancements, like a client/server feature with a local <b>cache</b> <b>storage</b> of the {{definition}} files of the used tokens, a completely new implementation of the MySQL support (including database tables creation and update), CHAP authentication (in addition to PAP authentication), QRcode generation for direct provisioning in Google Authenticator, fast creation of a user in a single command, ...|$|E
5000|$|The 3880 Model 13 has two caching storage {{directors}} {{that access}} subsystem storage; a larger portion of subsystem storage is the cache {{which is used}} to store active data for quick access; a smaller portion of the storage is the directory {{which is used to}} locate the data stored in the cache. The <b>cache</b> <b>storage</b> director attaches only one or two 3380 A-units each of which can in turn attach up to three 3380 B-units for a total of 16 devices. Because each 3380 DASD has two actuators the 3880 Model 13 can have up to 32 device addresses. [...] The Model 23 increased the cache size and somewhat improved performance but otherwise performed the same functions ...|$|E
50|$|The Mendocino Celerons also {{introduced}} new packaging. When the Mendocinos debuted {{they came in}} both a Slot 1 SEPP and Socket 370 PPGA package. The Slot 1 form had been designed to accommodate the off-chip cache of the Pentium II and had mounting problems with motherboards. Because all Celerons are a single-chip design, however, {{there was no reason}} to retain the slot packaging for L2 <b>cache</b> <b>storage,</b> and Intel discontinued the Slot 1 variant: beginning with the 466 MHz part, only the PPGA Socket 370 form was offered. (Third-party manufacturers made motherboard slot-to-socket adapters (nicknamed Slotkets) available for a few dollars, which allowed, for example, a Celeron 500 to be fitted to a Slot 1 motherboard.) One interesting note about the PPGA Socket 370 Mendocinos is they supported symmetric multiprocessing (SMP), and there was at least one motherboard released (the ABIT BP6) which took advantage of this fact.|$|E
50|$|Book II - Virtual Environment Architecture {{defines the}} storage model {{available}} to the application programmer, including timing, synchronization, <b>cache</b> management, <b>storage</b> features, byte ordering.|$|R
40|$|Abstract—Resources {{are often}} shared to improve {{resource}} utilization and reduce costs. However, not all resources exhibit good performance when shared among multiple applications. The work presented here focuses on effectively managing a shared <b>storage</b> <b>cache.</b> To provide differentiated services to applications exercising a <b>storage</b> <b>cache,</b> we propose a novel scheme that uses curve fitting to dynamically partition the <b>storage</b> <b>cache.</b> Our scheme quickly adapts to application execution, showing increasing accuracy over time. It satisfies application QoS {{if it is}} possible to do so, maximizes the individual hit rates of the applications utilizing the cache, and consequently increases the overall <b>storage</b> <b>cache</b> hit rate. Through extensive trace-driven simulation, we show that our <b>storage</b> <b>cache</b> partitioning strategy not only effectively insulates multiple applications from one another but also provides QoS guarantees to applications {{over a long period of}} execution time. Using our partitioning strategy, we were able to increase the individual <b>storage</b> <b>cache</b> hit rates of the applications by 67 % and 53 % over the no-partitioning and equal-partitioning schemes, respectively. Additionally, we improved the overall cache hit rates of the entire storage system by 11 % and 12. 9 % over the no-partitioning and equal-partitioning schemes, respectively, while meeting the QoS goals all the time. I...|$|R
30|$|While various user {{association}} schemes {{have been}} proposed for HetNets [2 – 7] and effectively enhanced the transmission performance of the RANs, the backhaul links may still cause challenges and difficulties in offering QoS guaranteed services to UEs. In particular, the demanding requirements of user services, such as multimedia streaming, web browsing applications, and socially interconnected networks, may cause network congestion and long transmission delay in backhaul links [8]. One promising approach for achieving backhaul offloading and reducing user download latency is to deploy <b>cache</b> <b>storages</b> at the mobile edge networks, e.g., the BSs or APs of the HetNets [9, 10].|$|R
3000|$|The applied user load, {{based on}} the number of users and the average file sizes per request, is uniformly {{increased}} to levels that are beyond the assigned <b>cache</b> <b>storage,</b> C [...]...|$|E
40|$|Peer-to-Peer (P 2 P) {{systems are}} {{generating}} {{a large portion}} of the total Internet traffic and imposing a heavy burden on Internet Services Providers (ISPs). Proxy caching for P 2 P traffic is an effective means of reducing network usage, thereby reducing operation costs for ISPs. Proxy <b>cache</b> <b>storage</b> design has a significant impact on ISPs. While there are several works on how to optimally design cache locations and capacity allocation to each location given a total capacity, few works tell ISPs what is the optimal total P 2 P <b>cache</b> <b>storage</b> capacity. In this paper, we propose an analysis method to the problem of optimally determining P 2 P cache size. An analysis methodology is proposed to determine the optimal cache size by considering the monetary costs of <b>cache</b> <b>storage</b> and bandwidth. Guided by our model, a close-form expression is developed to guide an ISP in the cache capacity design. Numerical evaluation results show that ISPs can achieve significant cost saving by deploying P 2 P cache and by allocating the cache capacity optimally...|$|E
40|$|The video {{content for}} on-demand {{services}} is generally stored and streamed in a compressed format. The compressed video is naturally with the VBR (variable-bit-rate) {{property and the}} stream traffic is highly burst. Subject to the QoS-guaranteed playback, the WAN bandwidth needs to allocate the video's peak bit rate {{if there is no}} client buffer for regulating the video delivery [7][8][9][10][11]. To reduce the requirement of WAN bandwidth, Video Staging [2], first proposed by Zhang et al., caches parts of a video content in the video proxy closed to clients. Therefore, the video can be streamed across the WAN with CBR (constant-bit-rate) services and its bandwidth requirement is significantly reduced. In this paper, we propose an Optimal Caching (OC) algorithm to handle the Video Staging problem. We also prove that the <b>cache</b> <b>storage</b> computed by our OC algorithm is minimal. Relatively, if the same cache size is given, OC requires less WAN bandwidth than [2] does to provide streaming services. By doing experiments on several benchmark videos [12], we show that the OC algorithm can reduce the <b>cache</b> <b>storage</b> requirement by over 30 % while comparing to [2]. With the same proxy <b>cache</b> <b>storage</b> of [2], we can reduce the WAN bandwidth requirement with more than 50 %. Additionally, the WAN bandwidth utilization can also be increased by over 30 %...|$|E
5000|$|With read caches, a {{data item}} {{must have been}} fetched from its {{residing}} location {{at least once in}} order for subsequent reads of the data item to realize a performance increase by virtue of being able to be fetched from the <b>cache's</b> (faster) intermediate <b>storage</b> rather than the data's residing location. With write caches, a performance increase of writing a data item may be realized upon the first write of the data item by virtue of the data item immediately being stored in the <b>cache's</b> intermediate <b>storage,</b> deferring the transfer of the data item to its residing storage at a later stage or else occurring as a background process. Contrary to strict buffering, a caching process must adhere to a (potentially distributed) cache coherency protocol in order to maintain consistency between the <b>cache's</b> intermediate <b>storage</b> and the location where the data resides. Buffering, on the other hand, ...|$|R
50|$|Dog Feed <b>Cache</b> and Sled <b>Storage,</b> {{comprising}} dog {{houses and}} kennels , built 1929-1930.|$|R
40|$|In this paper, {{we propose}} three novel cache models using Multiple-Valued Logic (MVL) {{paradigm}} {{to reduce the}} <b>cache</b> data <b>storage</b> area and <b>cache</b> energy consumption for embedded systems. Multiple-valued caches have significant potential for compact and powerefficient cache array design. The cache models differ from each other depending on whether they store tag and data in binary, radix-r or a mix of both. Our analytical study of cache silicon area shows that an embedded System-on-achip (SoC) equipped with a multiple-valued cache model can reduce the <b>cache</b> data <b>storage</b> area up to 6 % regardless of cache parameters. Also, our experiments on several embedded benchmarks demonstrate that dynamic cache energy consumption can be reduced up to 62 % in a multiple-valued instruction cache in an embedded SoC. 1...|$|R
40|$|We present HashCache, a {{configurable}} <b>cache</b> <b>storage</b> engine {{designed to}} meet the needs of <b>cache</b> <b>storage</b> in the developing world. With the advent of cheap commodity laptops geared for mass deployments, developing regions are poised to become major users of the Internet, and given the high cost of bandwidth in these parts of the world, they stand to gain significantly from network caching. However, current Web proxies are incapable of providing large storage capacities while using small resource footprints, a requirement for the integrated multi-purpose servers needed to effectively support developing-world deployments. Hash-Cache presents a radical departure from the conventional wisdom in network cache design, and uses 6 to 20 times less memory than current techniques while still providing comparable or better performance. As such, Hash-Cache can be deployed in configurations not attainable with current approaches, such as having multiple terabytes of external storage cache attached to low-powered machines. HashCache has been successfully deployed in two locations in Africa, and further deployments are in progress. ...|$|E
30|$|Some {{considerations}} {{are important to}} mention about memory hierarchy with unified second level. The advantage of this architecture is more flexible use of <b>cache</b> <b>storage.</b> However, since CPU accesses instructions and data in different pattern, caching these together may pollute the cache and degrade cache performance for instructions and/or data. Therefore, split caches for instructions and data are used, to best utilize the accessing pattern.|$|E
40|$|Content-Centric Networks (CCNs) {{promise to}} deliver con-tent {{in a better}} way than today’s Internet. In CCNs, effi-cient content {{delivery}} services can be achieved by using <b>cache</b> <b>storage</b> on routers. However, an effective caching scheme in CCNs is never established, because the communication method in CCNs differs from that in traditional networks. Therefore, we propose a selective caching scheme which only caches content likely to be used next. Furthermore, we show its effectiveness through simulations...|$|E
5000|$|In February 1990, {{the first}} {{computers}} from IBM {{to incorporate the}} POWER ISA were called the [...] "RISC System/6000" [...] or RS/6000. These RS/6000 computers {{were divided into two}} classes, workstations and servers, and hence introduced as the POWERstation and POWERserver. The RS/6000 CPU had 2 configurations, called the [...] "RIOS-1" [...] and [...] "RIOS.9" [...] (or more commonly the POWER1 CPU). A RIOS-1 configuration had a total of 10 discrete chips — an instruction cache chip, fixed-point chip, floating-point chip, 4 data L1 <b>cache</b> chips, <b>storage</b> control chip, input/output chips, and a clock chip. The lower cost RIOS.9 configuration had 8 discrete chips - an instruction cache chip, fixed-point chip, floating-point chip, 2 data <b>cache</b> chips, <b>storage</b> control chip, input/output chip, and a clock chip.|$|R
30|$|We {{began the}} {{discussion}} by highlighting {{the need for}} having capabilities for scalable solutions in storage cloud domains so that infrastructure-based responses can be achieved for maintaining performance within SLA thresholds {{in the event of}} such challenges as increases in user demand or, interruptions to the operating states of the service entities making up the cloud resource fabric. We went on to argue that for SLA-compliant services to be provided consistently over a wide range of load levels, an in-depth understanding of the performance trends associated with <b>storage</b> <b>cache</b> resource entities in the cloud infrastructure, is an important foundation on which to base QoS-ready solutions. It was further pointed out that from the performance characterisations of <b>storage</b> <b>cache</b> entities, <b>storage</b> resource management decisions on infrastructure sizing can be made, which are relevant to important stages of resource deployment such as initial roll-outs, short-term expansions to deal with overflow requests, and permanent upgrades.|$|R
5000|$|For example, {{it may be}} {{beneficial}} to cluster a record of an [...] "item" [...] in stock with all its respective [...] "order" [...] records. The decision of whether to cluster certain objects or not depends on the objects' utilization statistics, object sizes, <b>caches</b> sizes, <b>storage</b> types, etc.|$|R
40|$|In this paper, {{we present}} Whoops!, a {{clustered}} web cache prototype based on SciFS, a Distributed Shared Memory (DSM) that {{benefits from the}} high performances and the remote addressing capabilities of memory mapped networks like Scalable Coherent Interface (SCI). Whoops! uses the DSM for all web cache management and <b>cache</b> <b>storage.</b> Using a memory mapped network and a DSM programming model allow us to investigate new algorithm to distribute and handle requests. We presen...|$|E
40|$|A {{concurrent}} cache {{design is}} presented which allows cached {{data to be}} spread across a cluster of computers. The implementation separates persistent storage from <b>cache</b> <b>storage</b> and abstracts the cache behaviour so that the user can experiment with cache size and replacement policy to optimize performance for a given system, even if the production data store is not available. Using processes to implement cached objects allows for runtime configurability and adaptive use policies as well as parallelization to optimize resource access efficiency...|$|E
40|$|Peer-to-peer (P 2 P) {{networks}} have emerged {{over the past}} several years as new and effective ways for distributed resources to communicate and cooperate. &quot;Peer-to-peer computing is the sharing of computer resources and services by direct exchange between systems. These resources and services include the exchange of information, processing cycles, <b>cache</b> <b>storage,</b> and disk storage for files. &quot; P 2 P networking has the potential to greatly expand the usefulness of the network- be it for sharing music and video, privately contracting for services or for coordinatin...|$|E
40|$|Abstract—This paper {{presents}} our replacement algorithm named RED for <b>storage</b> <b>caches.</b> RED is exclusive. It {{can eliminate}} the duplications between a <b>storage</b> <b>cache</b> and its client cache. RED is high performance. A new criterion Resident Distance is proposed for making an efficient replacement decision instead of Recency and Frequency. Moreover, RED is non-intrusive to a storage client. It {{does not need}} to change client software and could be used in a real-life system. Previous work on the management of a <b>storage</b> <b>cache</b> can attain one or two of above benefits, but not all of them. We have evaluated the performance of RED by using simulations with both synthetic and real-life traces. The simulation results show that RED significantly outperforms LRU, ARC, MQ, and is better than DEMOTE, PROMOTE {{for a wide range of}} cache sizes...|$|R
40|$|Abstract—File {{layout of}} array data is a {{critical}} factor that effects the behavior of <b>storage</b> <b>caches,</b> and has so far taken not much attention {{in the context of}} hierarchical storage systems. The main contribution of this paper is a compiler-driven file layout optimization scheme for hierarchical <b>storage</b> <b>caches.</b> This approach, fully automated within an optimizing compiler, analyzes a multi-threaded application code and determines a file layout for each disk-resident array referenced by the code, such that the performance of the target <b>storage</b> <b>cache</b> hierarchy is maximized. We tested our approach using 16 I/O intensive application programs and compared its performance against two previously proposed approaches under different cache space management schemes. Our experimental results show that the proposed approach improves the execution time of these parallel applications by 23. 7 % on average. I...|$|R
5000|$|In February 1990, {{the first}} {{computers}} from IBM {{to incorporate the}} POWER instruction set were called the [...] "RISC System/6000" [...] or RS/6000. These RS/6000 computers {{were divided into two}} classes, workstations and servers, and hence introduced as the POWERstation and POWERserver. The RS/6000 CPU had 2 configurations, called the [...] "RIOS-1" [...] and [...] "RIOS.9" [...] (or more commonly the [...] "POWER1" [...] CPU). A RIOS-1 configuration had a total of 10 discrete chips - an instruction cache chip, fixed-point chip, floating-point chip, 4 data <b>cache</b> chips, <b>storage</b> control chip, input/output chips, and a clock chip. The lower cost RIOS.9 configuration had 8 discrete chips - an instruction cache chip, fixed-point chip, floating-point chip, 2 data <b>cache</b> chips, <b>storage</b> control chip, input/output chip, and a clock chip.|$|R
40|$|In-network caching {{is one of}} the {{fundamental}} operations of Information-centric networks (ICN). The default caching strategy taken by most of the current ICN proposals is caching along [...] default [...] path, which makes popular objects to be cached redundantly across the network, resulting in a low utilization of available cache space. On the other hand, efficient use of network-wide cache space requires possible cooperation among caching routers without the use of excessive signaling burden. While most of the cache optimization efforts strive to improve the latency and the overall traffic efficiency, we have taken a different path in this work and improved the storage efficiency of the cache space so that it is utilized to its most. In this work we discuss the ICN caching problem, and propose a novel distributed architecture to efficiently use the network-wide <b>cache</b> <b>storage</b> space based on distributed caching. The proposal achieves cache retention efficiency by means of controlled traffic redirection and selective caching. We utilize the ICN mechanisms and routing protocol messages for decision making, thus reducing the overall signaling need. Our proposal achieves almost 9 -fold increase in <b>cache</b> <b>storage</b> efficiency, and around 20 % increase in server load reduction when compared to the classic caching methods used in contemporary ICN proposals. Comment: Submitted and under review in Computer Networks Journal - Elsevie...|$|E
40|$|Caching at {{the network}} edge {{has emerged as a}} viable {{solution}} for alleviating the severe capacity crunch in modern content centric wireless networks by leveraging network load-balancing in the form of localized content storage and delivery. In this work, we consider a cache-aided network where the <b>cache</b> <b>storage</b> phase is assisted by a central server and users can demand multiple files at each transmission interval. To service these demands, we consider two delivery models - $(1) $ centralized content delivery where user demands at each transmission interval are serviced by the central server via multicast transmissions; and $(2) $ device-to-device (D 2 D) assisted distributed delivery where users multicast to each other in order to service file demands. For such cache-aided networks, we present new results on the fundamental <b>cache</b> <b>storage</b> vs. transmission rate tradeoff. Specifically, we develop a new technique for characterizing information theoretic lower bounds on the storage-rate tradeoff and show that the new lower bounds are strictly tighter than cut-set bounds from literature. Furthermore, using the new lower bounds, we establish the optimal storage-rate tradeoff to within a constant multiplicative gap. We show that, for multiple demands per user, achievable schemes based on repetition of schemes for single demands are order-optimal under both delivery models. Comment: Extended version of a submission to IEEE Trans. on Communication...|$|E
30|$|The Cloud Structure with Edge Caching (CSEC) scheme {{presented}} an information-theoretic model of F-RANs [6]. This scheme aimed at providing a latency-centric {{understanding of the}} degrees of freedom in the F-RAN network by accounting for the available limited resources in terms of fronthaul capacity, <b>cache</b> <b>storage</b> sizes, as well as power and bandwidth on the wireless channel. In addition, a new performance measure was introduced; it captured the worst-case latency incurred over the fronthaul. Finally, the CSEC scheme characterized the trade-off between the fronthaul and caching resources of the system while revealing optimal caching-fronthaul transmission policies [6].|$|E
5000|$|Decreased {{performance}} of the PC: If the volunteer computing application runs while the computer is in use, it may impact {{performance of}} the PC. This is due to increased usage of the CPU, CPU <b>cache,</b> local <b>storage,</b> and network connection. If RAM is a limitation, increased disk cache misses and/or increased paging can result. Volunteer computing applications typically execute at a lower CPU scheduling priority, which helps to alleviate CPU contention.|$|R
40|$|Current {{big data}} cloud systems often use {{different}} data migration strategies from providers to customers. This {{often results in}} increased bandwidth usage and herewith a decrease of the performance. To enhance the performance often caching mechanisms are adopted. However, the implementations of these caching mechanisms are often dedicated solutions for specific applications and/or use case scenarios. The adoption of different caching implementations within the same system leads to different problems including increased maintenance overhead, decrease of reuse, reduced adaptability, and resource allocation problems. To overcome these problems, {{in this paper we}} propose the so-called OneService Framework, which provides a generic cache aggregator mechanism that can be used with different <b>cache</b> <b>storages</b> to fetch and distribute the data from various providers. The framework as such helps to increase reuse, support adaptability, resolve the resource allocation problems, and enhance the overall performance of the system. We discuss the overall design of the framework together with the basic implementation concerns. The framework is illustrated for analyzing the maintenance, reusability and cost of MSN backend services...|$|R
5000|$|Cache: cache {{represents}} {{a small amount}} of very fast memory. A <b>cache</b> is a <b>storage</b> for a specific type of object, such as semaphores, process s, file objects, etc. A cache is stored in one or more slabs.|$|R
40|$|ISPs (Internet Service Providers) {{to reduce}} the {{bandwidth}} requirement in the backbone WAN. By caching portions of a video in a video proxy closed to clients, the video playback quality can be dramatically improved {{and the problem of}} insufficient WAN bandwidth is eliminated. In the loss-less nehvork environment, the OC (Optimal Cache) algorithm uses minimum <b>cache</b> <b>storage</b> in the video proxy and reduces the maximum bandwidth required in the backbone WAN. However, data packets may be lost to affect video playback quality while streaming video data through the Internet. Consider an MPEG video in which an I-frame is referenced by all other frames (B-or P-frames) in the same GOP (Group of Picture). Losing packets belonging to an I-frame makes it difficult to decode all of subsequent frames retrieved from the same GOP. The major goal {{of this paper is to}} select maximum video data from high-priority frames (I-frames) caching in the video proxy in order to defeat decoding error caused by packet loss and improve error recovery while serving QoSguaranteed video playback. We propose a novel PSC (Priority Selected Cache) algorithm for solving this cache data selection problem. The PSC algorithm uses minimum <b>cache</b> <b>storage</b> in the video proxy and reduces maximum bandwidth requirement in the backbone WAN (as does the OC algorithm). Additionally, experiment results with several benchmark videos show that the PSC algorithm is 15 % better than the conventional OC algorithm a t caching I-frame data in a video proxy. I...|$|E
40|$|The {{number of}} {{functional}} errors escaping design verification and being released into final silicon is growing, {{due to the}} increasing complexity and shrinking production schedules of modern processor designs. Recent trends towards chip multiprocessors (CMPs) are exacerbating the problem because of their complex and sometimes nondeterministic memory subsystems, prone to subtle but devastating bugs. This deteriorating situation calls for highefficiency, high-coverage results in functional validation, results that are be achieved by leveraging the performance of post-silicon validation, that is, those verification tasks that are executed directly on prototype hardware. The orders-of-magnitude faster testing in post-silicon enables designers to achieve much higher coverage before customer release, {{but only if the}} limitations of this technology in diagnosis and internal node observability could be overcome. In this work, we unlock the full performance of postsilicon validation through Dacota, a new high-coverage solution for validating memory operation ordering in CMPs. When activated, Dacota reconfigures a portion of the <b>cache</b> <b>storage</b> to log memory accesses using a compact datacoloring scheme. Logs are periodically aggregated and checked by a distributed algorithm running in-situ on the CMP to verify correct memory operation ordering. When the design is ready for customer shipment, Dacota can be deactivated, releasing all <b>cache</b> <b>storage,</b> and only leaving a small silicon area footprint, less than 0. 01 % (three orders of magnitude smaller than previous solutions). We found experimentally that Dacota is effective in exposing memory subsystem bugs, and it delivers its high coverage capabilities at a 26 % performance slowdown (only during validation) for real-world applications. 1...|$|E
40|$|Nowadays, {{the idea}} of media {{contents}} streaming through the Internet has become a very important issue. On the other hand, full caching for media objects is not a practical solution and leads to consume the <b>cache</b> <b>storage</b> in keeping few media objects because of its limited capacity. Furthermore, repeated traffic which is being sent to clients wastes the network bandwidth. Thus, utilizing the bandwidth of the network is considered as an important objective for network administrators. Media objects have some characteristics {{that have to be}} considered when a caching algorithm is going to be performed. In this paper, recent approaches that have been proposed for media streams caching in peer-to-peer systems are reviewed...|$|E
50|$|Wiping, backing up, {{restoring}} {{and mounting}} various device partitions (e.g. system, boot, userdata, <b>cache</b> and internal <b>storage)</b> are also supported. TWRP also features file transfer by MTP, a basic file manager and a terminal emulator. It is fully themeable.|$|R
40|$|We {{introduce}} SLIM, {{a hybrid}} storage system that uses disks within data center racks to implement persistent <b>caches</b> for <b>storage</b> area networks (SANs). SLIM leverages compression and block overwrites to reduce traffic on data center oversubscribed network links {{and to improve}} performance for low-cost SANs. We evaluate SLIM using various microbenchmarks and industry standard benchmarks. Our results show that SLIM reduces network storage traffic by 40 %- 90 % and significantly increases application performance in bandwidth-constrained environments. 1...|$|R
40|$|The main {{contribution}} {{of this paper}} is a topologyaware storage caching scheme for parallel architectures. In a parallel system with multiple <b>storage</b> <b>caches,</b> these caches form a shared cache space, and effective management of this space is a critical issue. Of particular interest is data migration (i. e., moving data from one <b>storage</b> <b>cache</b> to another at runtime), which may help reduce the distance between a data block and its customers. As the data access and sharing patterns change during execution, we can migrate data in the shared cache space to reduce access latencies. The proposed storage caching approach, which is based on the two-dimensional post-office placement model, takes advantage of the variances across the access latencies of the different <b>storage</b> <b>caches</b> (from a given node’s perspective), by selecting the most appropriate location (cache) to place a data block shared by multiple nodes. This paper also presents experimental results from our implementation of this data migration-based scheme. The results reveal that the improvements brought by our proposed scheme in average hit latency, average miss rate, and average data access latency are 29. 1 %, 7. 0 % and 32. 7 %, respectively, over an alternative storage caching scheme. ...|$|R
