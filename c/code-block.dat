28|15|Public
50|$|<b>Code-block</b> {{reordering}}: <b>Code-block</b> reordering {{alters the}} order of the basic blocks in a program in order to reduce conditional branches and improve locality of reference.|$|E
50|$|As {{the product}} matured, it {{remained}} a DOS tool for many years, but added {{elements of the}} C programming language and Pascal programming language, as well as OOP, and the <b>code-block</b> data-type (hybridizing the concepts of dBase macros, or string-evaluation, and function pointers), to become far {{more powerful than the}} original. Nantucket's Aspen project later matured into the Windows native-code Visual Objects compiler.|$|E
50|$|Packets {{from all}} sub-bands are then {{collected}} in so-called layers.The way the packets are built {{up from the}} <b>code-block</b> coding passes, and thus which packets a layer will contain, is not defined by the JPEG 2000 standard, but in general a codec will try to build layers {{in such a way}} that the image quality will increase monotonically with each layer, and the image distortion will shrink from layer to layer. Thus, layers define the progression by image quality within the code stream.|$|E
30|$|Depending on the situation, a {{model can}} be better or worse, {{therefore}} the comparative study becomes more important. Taking this into consideration, comparison is done graphically between the concerned models (Models 1 and 2) by computing various measures of system effectiveness using Laplace transforms and software package <b>Code-Blocks</b> 13.12.|$|R
3000|$|The {{experimental}} {{setup is}} as follows. The test image Lena 512 × 512 initially coded at 8 bpp is considered. By analogy {{to the system}} proposed in the previous section, each bit-stream resulting from the compression of P = 4 <b>code-blocks</b> form the message b. The latter is scrambled then, coded by an 8 -state RSCC with rate [...]...|$|R
30|$|The {{considered}} {{coding scheme}} uses the JPEG 2000 encoder which compresses the source image at Ds {{bits per pixel}} (bpp). In JPEG 2000 coding, the 9 - 7 filters are used for wavelet transform {{and the number of}} resolution levels is equal to five. The wavelet domain is divided into rectangular regions called <b>code-blocks.</b> The JPEG 2000 encoding machine defines multiple quality layers that help image reconstruction at different rates (scalability). In the following experiment, we consider a single quality layer for simplicity and the scheme can be generalized to multiple quality layers. The values of Ds = 0.4 bpp and Ds = 1 bpp are considered. The bit-stream generated by the JPEG 2000 encoder is composed of headers describing the coding parameters followed by a sequence of packets containing the encoded data. We assume that the data contained in the headers are transmitted without error. The JPEG 2000 uses a context-based binary adaptive AC called the MQ-coder. The <b>code-blocks</b> are independently encoded by the AC [3].|$|R
40|$|Machine (TAM) TAM [Culler 93] has {{its roots}} in the {{dataflow}} model of execution, but can be understood independently of dataflow. A language called Threaded Machine Language, TL 0, was designed to permit programming using the TAM model. TAM recognizes three major storage resources [...] -codeblocks, frames, and structures [...] -and the existence of critical processor resources, such as registers. A program is represented by a collection of re-entrant code-blocks, corresponding roughly to individual functions or loop bodies in the high-level program text. A <b>code-block</b> comprises a collection of threads and inlets. Invoking a <b>code-block</b> involves allocating a frame [...] -much like a conventional call frame [...] - depositing argument values into locations within the frame, and enabling threads within the <b>code-block</b> for execution. Instructions may refer to registers and to slots in the current frame: the compiler statically determines the frame size for each <b>code-block</b> and is responsible for correctly using sl [...] ...|$|E
40|$|The JPEG 2000 {{standard}} adopts an error-resilient {{structure to}} organize its code-stream. Meanwhile, this robust structure also provides potential optimization space for packetization algorithms. In this paper, we study the optimal packetization issue for JPEG 2000 code-streams over wireless channels. We first discuss {{the best strategy}} for packetizing bits from an individual <b>code-block</b> into channel packets. Then, we extend it to bits {{from a group of}} code-blocks. Experimental results indicate that our scheme is outperforming the normal method by significant margins. © 2004 IEEE...|$|E
40|$|We {{present a}} scheme for {{compressed}} domain interactive rendering of large volume data sets over distributed environments. The scheme exploits the distortion scalability and multi-resolution properties offered by JPEG 2000 {{to provide a}} unified framework for interactive rendering over low bandwidth networks. The interactive client is provided breadth in terms of scalability in resolution, position and progressive improvement by quality. The server exploits the spatial locality offered by the DWT and packet indexing information to transmit, {{in so far as}} possible, compressed volume data relevant to the clients query. Once the client identifies its volume of interest (VOI), the volume is refined progressively within the VOI. Contextual background information can also be made available having quality fading away from the VOI. The scheme is ideally suited for client-server setups with low bandwidth constraints, with the server maintaining the compressed volume data, to be browsed by a client with low processing power and/or memory. Rendering can be performed at a stage when the client feels that the desired quality threshold has been attained. We investigate the effects of <b>code-block</b> size on compression ratio, PSNR, decoding times and data transmission to arrive at an optimal <b>code-block</b> size for typical VOI decoding scenarios...|$|E
40|$|Abstract:- In this paper, novel {{selective}} encryption image schemes {{based on}} JPEG 2000 are proposed. The first one encrypts only the <b>code-blocks</b> corresponding to some sensitive precincts. In {{order to improve}} the security level we introduce the permutation of codeblocks contributing in the selected precincts. The idea of combining permutation and selective encryption is used {{in order to minimize}} the amount of processed data encryption while ensuring the best possible degradation through the permutation. Many of proposals format compliant encryption schemes for JPEG 2000 that have been made encrypt packet body data, but leave header packet in plaintext. The second approach, combines <b>code-blocks</b> data encryption to a cyclic permutation of all packets headers in the bitstream. Actually, in the JPEG 2000 codestream, packet header information is specific to the visual content, and it is {{can be used as a}} fingerprint of the codestream. Symmetric encryption AES with CFB mode is used to encrypt in the two schemes. The proposed schemes don’t introduce superfluous JPEG 2000 markers in the protected codestream, i. e, its format is compliant to JPEG 2000 codestream one. It keeps file format and compression ratio unchanged and doesn’t degrade the original error robustness. The proposed scheme works with any standard ciphers and introduces negligible computational cost...|$|R
40|$|In this paper, {{we propose}} a blind {{watermarking}} method integrated in the JPEG 2000 coding pipeline. Prior to the entropy coding stage, the binary watermark {{is placed in}} the independent <b>code-blocks</b> using Quan- tization Index Modulation (QIM). The quantization strategy allows to embed data in the detail subbands of low resolution {{as well as in the}} approximation image. Watermark recovery is performed without reference to the original image during image decompression. The proposed embedding scheme is robust to compression and other image processing attacks. We demonstrate two application scenarios: image authentication and copyright protection...|$|R
40|$|We have {{proposed}} a reversible information hiding for binary images. In this paper, we presents a lossless data hiding method for JPEG 2000 compressed data based on the reversible information hiding. In JPEG 2000 compression, wavelet coefficients of an image are quantized,therefore, the least significant bit plane (LSB) can be extracted. The proposed method recovers the quantized wavelet coefficients of cover images from stego images. To realize this, we embed not only secret data and the JBIG 2 bit-stream of {{a part of the}} LSB plane but also the bit-depth of the quantized coefficients on some <b>code-blocks.</b> Experimental results demonstrate the　feasibility of application of the proposed method to image alteration detection for JPEG 2000 compressed data. 2008 International Conference on Intelligent Information Hiding and Multimedia Signal Processing, 15 - 17 Aug. 2008, Harbin, Chin...|$|R
40|$|The {{landmark}} JPEG 2000 {{image compression}} standard offers not only superior compression performance, but also incredible flexibility. The compressed bitstream of JPEG 2000 can be flexibly reorganized to another bitstream of different bitrate, resolution, and spatial {{region of interest}} (ROI) {{or a combination of}} any of the above. Such flexibility is achieved by multiplexing the compressed bitstream pieces of multiple code-blocks together into a combined bitstream, with the length of the <b>code-block</b> bitstream piece (LOCB) embedded in the combined bitstream. The LOCB serves both to reorganize the bitstream, and to decode the bitstream. It represents a significant overhead, especially since there is no correlation between the neighbor LOCBs. In this work, we introduce seamless multiplexing, and separate the information needed for the reorganization, i. e., the LOCB, from the compressed bitstream itself by using the decoder pointer to multiplex the bitstream pieces. As a result, the compressed bitstream consists of <b>code-block</b> bitstream pieces seamlessly concatenated to each other. With seamless multiplexing, only the compressed bitstream (without LOCB) needs to be delivered to the receiving client. It results in better compression performance and higher granularity of access. Another benefit of seamless multiplexing is that the relative coding orders of the code-blocks are preserved in the bitstream reorganization. As a result, the seamlessly multiplexed embedded codec (SMEC) may utilize the dependencies among the code-blocks in the coding, thus further boost the compression performance. 1...|$|E
40|$|A novel {{multiple}} description {{coding scheme}} {{compatible with the}} JPEG 2000 decoder Tammam Tillo, Student Member, IEEE, Gabriella Olmo, Member, IEEE Abstract — In this letter we propose a novel technique to generate rate-distortion optimized multiple descriptions of images, exploiting the rate-allocation strategy embedded in the JPEG 2000 encoder. The proposed scheme {{can be applied to}} any encoding algorithm, given that the rate allocation is based on <b>code-block</b> truncation. The method yields excellent performance in terms of both central and side distortion, outperforming stateof-the art techniques. Moreover, the single description decoding is fully compatible with the JPEG 2000 Part 1 decoder. Permission to publish this abstract separately is granted I...|$|E
40|$|Abstract — This paper {{discusses}} the latest ISO/ITU-T still image coding standard, JPEG 2000 and the moving pictures variant, Motion JPEG 2000. These offer greater functionality and compression performance {{compared to their}} predecessors, JPEG and Motion JPEG, which are heavily used on the Internet and in digital cameras. We developed a proof-of-concept of interactive Region-Of-Interest coding within Motion JPEG 2000. For this we used several ROI coding techniques, namely coefficient scaling, <b>code-block</b> selection and tiling and a specially developed client/server architecture. Our main observation was the impact on required bandwidth and computing resources. Potential applications of interactive Region-Of-Interest coding reside in medical imaging, telemedicine networks, surveillance video and many more. I...|$|E
40|$|Abstract—Quality {{scalability}} is {{a fundamental}} feature of JPEG 2000, achieved {{through the use of}} quality layers that are optimally formed in the encoder by rate-distortion optimization techniques. Two points, related with the practical use of quality layers, may need to be addressed when dealing with JPEG 2000 code-streams: 1) the lack of quality scalability of code-streams containing a single or few quality layers, and 2) the rate-distortion optimality of windows of interest transmission. Addressing these two points, this paper proposes a mechanism that, without using quality layers, provides competitive quality scalability to code-streams. Its main key-feature is a novel characterization of the <b>code-blocks</b> rate-distortion contribution that does not use distortion measures based on the original image, or related with the encoding process. Evaluations against the common use of quality layers, and against a theoretical optimal coding perfor-mance when decoding windows of interest or when decoding the complete image area, suggest that the proposed method achieves close to optimal results. Index Terms—JPEG 2000 standard, rate-distortion optimiza-tion, quality scalability, interactive image transmission...|$|R
3000|$|... {{illustrates}} {{the comparison of}} two stochastic models of a cable manufacturing plant with varying demand. Here, it shows the comparison between a single unit system (Model 1) and a two-unit cold standby system (Model 2). In Model 1, the system is either in working state on some demand or put to shut down mode on no demand. In Model 2, at initial stage, one of the units is operative while the other is kept as cold standby. At times when the operative unit stops working due to some breakdown/failure, the standby unit instantaneously becomes operative while the repairman repairs the failed unit. In this working model, only one unit remains operative at a time. However, {{there may be a}} state when both the units fail. The comparison of systems is done by means of MTSF (mean time to system failure), steady state availability and profit function using Laplace transforms and software package <b>Code-Blocks</b> 13.12. Different graphs have been plotted to discover which model is superior to the other model under the given conditions. The system is analysed by making use of semi-Markov processes and regenerative point technique.|$|R
40|$|Today {{we use the}} Web {{for almost}} everything, even to program. There are several {{specialized}} code editors gravitating on the Web and emulating most of the features inherited from traditional IDEs, such as, syntax highlight, code folding, autocompletion and even code refactorization. One of the techniques to speed the code development {{is the use of}} snippets as predefined code blocks that can be automatically included in the code. Although several Web editors support this functionality, they come with a limited set of snippets, not allowing the contribution of new blocks of code. Even if that would be possible, they would be available only to the code 2 ̆ 7 s owner or to the editors 2 ̆ 7 users through a private cloud repository. This paper describes the design and implementation of Sni 2 ̆ 7 per, a RESTful API that allows public access for multi-language programming <b>code-blocks</b> ordered by popularity. Besides being able to access code snippets from other users and score them, we can also contribute with our own snippets creating a global network of shared code. In order to make coding against this API easier, we create a client library that reduces the amount of code required to write and make the code more robust...|$|R
40|$|A low-complexity {{three-dimensional}} {{image compression}} algorithm based on wavelet transforms and set-partitioning strategy is presented. The Subband Block Hierarchial Partitioning (SBHP) algorithm is modified and extended to three dimensions, {{and applied to}} every code block independently. The resultant algorithm, 3 D-SBHP, efficiently encodes 3 D image data by the exploitation of the dependencies in all dimensions, while enabling progressive SNR and resolution decompression and Region-of-Interest (ROI) access from the same bit stream. The <b>code-block</b> selection method by which random access decoding can be achieved is outlined. The resolution scalable and random access performances are empirically investigated. The results show 3 D-SBHP is a good candidate to compress 3 D image data sets for multimedia applications. 1...|$|E
40|$|Multithreaded {{execution}} models {{attempt to}} combine {{some aspects of}} dataflow-like execution with von Neumann model of execution, {{with the objective of}} masking the latency of inter-processor communications and remote memory accesses in multiprocessors. Two important issues in multithreading are the thread execution models, and the synchronization schemes. Depending on the execution model a thread can be either blocking or non-blocking. The synchronization can be done either at the thread or <b>code-block</b> levels. This thesis describes an experimental study of various issues in multithreaded execution. Issues of particular interest are: multithreaded code generation, the multithreaded execution models and the impact of various storage hierarchies. A new code generation scheme for the blocking model of thread execution is presented. The blocking and nonblocking models of execution are compared to study [...] ...|$|E
40|$|We {{propose a}} novel {{multimedia}} security framework {{based on a}} modification of the arithmetic coder, which is used by most international image and video coding standards as entropy coding stage. In particular, we introduce a randomized arithmetic coding paradigm, which achieves encryption by inserting some randomization in the arithmetic coding procedure; notably, and unlike previous works on encryption by arithmetic coding, this is done at no expense in terms of coding efficiency. The proposed technique {{can be applied to}} any multimedia coder employing arithmetic coding; in this paper we describe an implementation tailored to the JPEG 2000 standard. The proposed approach turns out to be robust towards attempts to estimating the image or discovering the key, and allows very flexible protection procedures at the <b>code-block</b> level, allowing to perform total and selective encryption, as well as conditional acces...|$|E
40|$|Embedded block coding with {{optimized}} truncation (EBCOT) is a {{coding algorithm}} used in JPEG 2000. EBCOT operates on the wavelet transformed data to generate highly scalable compressed bit stream. Sub-band samples obtained from wavelet transform are partitioned into smaller blocks called <b>code-blocks.</b> EBCOT encoding {{is done on}} blocks to avoid error propagation through the bands and to increase robustness. Block wise encoding provides flexibility for parallel hardware implementation of EBCOT. The encoding process in JPEG 2000 {{is divided into two}} phases: Tier 1 coding (Entropy encoding) and Tier 2 coding (Tag tree coding). This thesis deals with design space exploration and implementation of parallel hardware architecture of Tier 1 encoder used in JPEG 2000. Parallel capabilities of Tier- 1 encoder is the motivation for exploration of high performance real time image compression architecture in hardware. The design space covers the following investigations: - The effect of block-size in terms of resources, speed, and compression performance, - Computational performance. The key computational performance parameters targeted by the architecture are - significant speedup compared to a sequential implementation, - minimum processing latency and, - minimum logic resource utilization. The proposed architecture is developed for an embedded application system, coded in VHDL and synthesized for implementation on Xilinx FPGA system...|$|R
40|$|Support for virtual {{states and}} deltas {{between them is}} useful {{for a variety of}} {{database}} applications, including hypothetical database access, version management, simulation, and active databases. The Heraclitus paradigm elevates delta values to be "first-class citizens" in database programming languages, {{so that they can be}} explicitly created, accessed and manipulated. A fundamental issue concerns the trade-off between the "accuracy" or "robustness" of a form of delta representation, and the ease of access and manipulation of that form. At one end of the spectrum, <b>code-blocks</b> could be used to represent delta values, resulting in a more accurate capture of the intended meaning of a proposed update, at the cost of more expensive access and manipulation. In the context of object-oriented databases, another point on the spectrum is "attribute-granularity" deltas which store the net changes to each modified attribute value of modified objects. This paper introduces a comprehensive framework for specifying a broad array of forms for representing deltas for complex value types (tuple, set, bag, list, o-set and dictionary). In general, the granularity ofsuch deltas can be arbitrarily deep within the complex value structure. Applications of this framework in connection with hypothetical access to, and "merging" of, proposed updates are discussed...|$|R
40|$|This paper {{proposes a}} new {{architecture}} called Pipelined LookUp Grid (PLUG) that can perform data structure lookups in network processing. PLUGs are programmable and through simplicity achieve power efficiency. We {{draw upon the}} insights that data structure lookups have natural structure that can be statically determined and exploited. The PLUG execution model transforms data-structure lookups into pipelined stages of computation and associates small <b>code-blocks</b> with data. The PLUG architecture is a tiled architecture with each tile consisting predominantly of SRAMs, a lightweight no-buffering router, {{and an array of}} lightweight computation cores. Using a principle of fixed delays in the execution model, the architecture is contention-free and completely statically scheduled thus achieving high energy efficiency. The architecture enables rapid deployment of new network protocols and generalizes as a data-structure accelerator. This paper describes the PLUG architecture, the compiler, and evaluates our RTL prototype PLUG chip synthesized on a 55 nm technology library. We evaluate six diverse high-end network processing workloads including IPv 4, IPv 6, and Ethernet forwarding. We show that at a 55 nm technology, a 16 -tile PLUG occupies 58 mm 2, provides 4 MB on-chip storage, and sustains a clock frequency of 1 GHz. This translates to 1 billion lookups per second, a latency of 18 ns to 219 ns, and average power less than 1 watt...|$|R
40|$|Abstract—This paper {{presents}} a novel, effective, and efficient characterization of wavelet subbands by bit-plane extractions. Each bit plane {{is associated with}} a probability that represents the frequency of 1 -bit occurrence, and the concatenation of all the bit-plane probabilities forms our new image signature. Such a signature can be extracted directly from the <b>code-block</b> code-stream, rather than from the de-quantized wavelet coefficients, making our method particularly adaptable for image retrieval in the compression domain such as JPEG 2000 format images. Our signatures have smaller storage requirement and lower computational complexity, and yet, experimental results on texture image retrieval show that our proposed signatures are much more cost effective to current state-of-the-art methods including the generalized Gaussian density signatures and histogram signatures. Index Terms—Bit-plane probabilities, embedded block coding with optimized truncation (EBCOT), image retrieval, JPEG 2000, textures, wavelet signatures. I...|$|E
40|$|This paper details work {{undertaken}} on {{the application}} of JPEG 2000, the recent ISO/ITU-T image compression standard based on wavelet technology, to region of interest (ROI) coding. The paper briefly outlines the JPEG 2000 encoding algorithm and explains how the packet structure of the JPEG 2000 bit-stream enables an encoded image to be decoded {{in a variety of}} ways dependent up{{on the application}}. The three methods by which ROI coding can be achieved in JPEG 2000 (tiling; coefficient scaling; and codeblock selection) are then outlined and their relative performance empirically investigated. The experimental results show that there are a number of parameters that control the effectiveness of ROI coding, the most important being the size and number of regions of interest, <b>code-block</b> size, and target bit rate. Finally, some initial results are presented on the application of ROI coding to face images...|$|E
40|$|One {{interesting}} {{feature of}} image compression is support of {{region of interest}} (ROI) access, in which an image sequence can be encoded only once and then the decoder can directly extract {{a subset of the}} bitstream to reconstruct a chosen ROI of required quality. In this paper, we apply Three-dimensional Subband Block Hierarchical Partitioning (3 -D SBHP), a highly scalable wavelet transform based algorithm, for volumetric medical image compression to support ROI access. The codeblock selection method by which random access decoding can be achieved is outlined and the performance empirically investigated. The experimental results show {{that there are a number}} of parameters that affect the effectiveness of ROI access, the most important being the size of the ROI size, <b>code-block</b> size, wavelet composition level, number of filter taps and target bit rate. Finally, one possible way to optimize ROI access performance is addressed...|$|E
40|$|In this dissertation, we {{show how}} the {{state-of-the-art}} image compression techniques—JPEG 2000 —can be utilized for compressing multi-dimensional data; specifically, we look for compressions that minimize mean squared error (MSE) and maximum absolute error (MAE). We also show how to use JPEG 2000 to guarantee the given value of the reconstruction error (i. e., how to solve the corresponding inverse optimization problem) by adjusting bit error rates. As a case study, we consider meteorological data produced by the Battlescale Forecast Model. ^ In the independent 2 -D approach, the 3 -D volume of data is considered {{as a set of}} layers, which are successively compressed (and stored or transmitted). Another alternative considered is the 3 -D approach which consists of, first, preprocessing the data in the vertical direction by applying Karhunen-Loève Transform, and then compressing slice by slice the decorrelated data. The JPEG 2000 Part 2 extension includes this 3 -D approach, but does not provide a method for optimal bit rate allocation to the individual slices. One of the main contributions in this dissertation is the solution of this problem. Our first approach makes use of experimentally acquired rate-distortion data, thus providing optimal solution, but requiring a significant amount of computational effort. In the second, more computationally efficient approach, we use a mixed model approximation to the actual rate-distortion data. The model is a piece-wise combination of the traditional high-resolution model with a model that gives good approximation at low bit rates [MF 98]. ^ The optimization problem is solved for both MSE and MAE. For MSE, we use an approach similar to the Post-Compression Rate-Distortion (PCRD) optimization approach used in JPEG 2000 for selecting the optimal truncation points for <b>code-blocks</b> [TM 02]. For MAE, for 3 -D approach, we derive the upper bound on MAE and minimize this upper bound by using Lagrange multipliers; for 2 -D approach, we provide an explicit solution to the MAE optimization problem. ^ Finally, in the Appendices, we demonstrate, on the example of mammography, FLIR, and SMD images, how various task-specific quality metrics can be used to evaluate lossy compression degradation. These metrics can be extended to 3 -D meteorological data. ...|$|R
40|$|University of Minnesota Ph. D. dissertation. April 2009. Major: Electrical Engineering. Advisor: Professor Jaekyun Moon. 1 {{computer}} file (PDF); xiii, 126 pages. Ill. (some col.); portrait. The {{perpendicular magnetic recording}} channel (PMRC) is corrupted by sever intersymbol interference and data-dependent media noise, {{in addition to a}} variety of other bursty impairments. Thus far, the hard decodable symbol correcting Reed-Solomon (RS) code has been the industry standard for outer error control coding (ECC). This thesis proposes two novel ECC schemes in the migration toward next generation high density recording. The first scheme is a two-level concatenation of channel-matched turbo equalization (TE) and outer RS, replacing current inner parity correction codes. Conventional TE is matched to the channel via the incorporation of the error pattern correction code (EPCC), which works iteratively with the other constituent code in TE, whether block or convolutional, to suppress the occurrence of low-Euclidean-distance errors at the output of the channel detector. To understand this mechanism, and with no loss of generality, we derive the error Euclidean distance distribution of TE-EPCC for the Dicode channel, and show that EPCC substantially increases the interleaver gain exponent of low Euclidean weight errors. Furthermore, we derive an upper bound on the BER of TE-EPCC, and employ it to show that TE-EPCC delivers significant gains in the error floor and cliff regions compared to conventional precoded and unprecoded TE for a variety of channel conditions and code rates. The second proposed ECC system is a tensor product concatenation of EPCC and Q-ary LDPC (T-EPCC-QLDPC). This concatenation scheme enables the use of byte-long component EPCC without jeopardizing the overall code rate. Hence, the multiple error correction capability of EPCC is maintained at very low signal-to-noise ratios, while the component non-binary LDPC insures correct syndromes are available for the decoding of tensor symbols (EPCC <b>code-blocks).</b> We introduce a low complexity iterative soft decoder of T-EPCC-QLDPC, in which the component EPCC and QLDPC exchange multi-level loglikelihood ratios (mlLLR) that represent their beliefs on the reliability of error-syndromes. Moreover, we show that the two-level decoder provides a better performance-complexity tradeoff compared to single-level binary and Q-ary LDPC...|$|R
40|$|Digital {{images are}} {{becoming}} increasingly successful thanks to the development and the facilitated access to systems permitting their generation (i. e. camera, scanner, imaging software, etc). A digital image basically corresponds to a 2 D discrete set of regularly spaced samples, called pixels, where each pixel contains the light intensity information (e. g., luminance, chrominance) of a very localized spatial region of the image. In the case of natural images, pixel values are acquired through one or several arrays of MOS semiconductors (Charge Couple Devices, CCDs), each generating an electrical information proportional to the incoming light intensity. The initial finalities of digital images were the storage on a dedicated medium (e. g., camera's memory, computer's hard drive, CDROM), eventual transmissions, and final display on a screen or printing. With such a narrow scope, the principal goal of image processing and coding tools was to face storage and transmission bandwidth limitations thanks to efficient compression algorithms reducing the image representation size. However, with recent developments in computing, algorithmic and telecommunication domains, many new applications (i. e. web-publishing, remote browsing etc.) have arisen. They generally require additional and enhanced features (i. e. progressive decoding, random-access, region of interest support, robustness to transmission errors, etc.) and have motivated {{the creation of a}} new generation of coding algorithms which, besides their good compression performance, present many other useful features. Hence, digital images are almost never represented as a simple set of pixel values (i. e. raw representation) but, instead, under a specific compact way (i. e. compressed or coded representation), chosen according to features it brings to the considered application. A compressed version of an image is obtained by removing as much spatial, visual and statistical redundancies as possible, thanks to appropriate coding methods, while keeping an acceptable visual quality. Noting that natural images have most of their energy concentrated in low frequency components, recent coding algorithms generally first decompose the image into a specific frequency domain DCT, DWT, etc). The goal is to obtain a representation, where few coefficients are sufficient for reconstructing the image with a good quality. The precision of transformed coefficients is then generally reduced by quantization in order to make them more compressible by an entropy coder, aiming at removing statistical redundancies of quantization indexes. The ultimate compressed representation, called codestream, is usually obtained by a rate-allocation process that tries to achieve the best trade-off between the compression ratio and the reconstructed image quality. JPEG 2000, the new still image coding standard developed by the Joint Photographic Experts Group JPEG), is based on these state-of-the-art compression techniques, but is also designed to fulfill many requirements of recent applications. It only normalizes the decoding algorithm and consequently let some liberty for designing encoders optimized for some specific features or for building extensions that take profit from its compressed domain specifics. The success of the JPEG 2000 standard will not only depend on its intrinsic performance, but also on its ability to comply with specific demands of actual and future imaging applications. Thus, among the major concerns of image content providers are the security of the transmission over networks and of the image itself: with current facilities to instantly access on-line digital libraries from anywhere in the world, to perfectly copy and to easily modify the content of an image, solutions must be found in order to permit, at one hand, Intellectual Property Right (IPR) protection and image integrity verification and, at the other hand, the development of dedicated tools favoring exchange and purchase of images over communication networks. It is worth pointing out that some cryptographic-based solutions to these problems already exist. However, they need to be adapted to the digital image and its compressed domain representation in order to take full advantage from their specifics and to avoid restraining fields of applications. The JPEG 2000 coding algorithm is mainly based on Discrete Wavelet Transform (DWT), embedded scalar quantization and adaptive arithmetic coding. From a terminology point of view, this means that compressed domain representation can indifferently refer to either wavelet coefficients, or quantized wavelet coefficients, or bit streams (i. e. entropy coded group of quantization indexes) or the codestream (i. e. aggregation of bit streams and headers containing necessary decoding information). The choice of the appropriate compressed domain actually depends on the considered application. The quality of a JPEG 2000 image, at a given compression ratio, mainly depends on the rate-allocation procedure used at the encoder side. Such a procedure applies on entropy coded quantization indexes and favors groups of quantization indexes (i. e. <b>code-blocks)</b> offering the best rate-distortion trade-offs. However, this does not necessary correspond to the most interesting part of the image, from an end-observer point of view. Hence, the standard provides with ways to define Regions Of Interest (ROI) which are prioritized during the encoding process in order to exhibit a higher quality than the rest (i. e. background) at any decoding time. This feature applies either in the quantized wavelet domain or at the bit stream level, but its parameters are generally not very explicit for a standard end-user and only provide with a rough control of the decoded ROI quality. Consequently, the first objective of this thesis is to create and also extend compressed domain tools for controlling the quality of a JPEG 2000 ROI. In the meantime, several image processing techniques, such as watermarking, are applied directly in the spatial domain or in a specific transform space defined from the spatial domain. However, since digital images are preferably available under a compressed/encoded format, which we assume to be JPEG 2000 in this thesis, their implementation first imply decompressing the image, then apply the considered processing task and finally re-encode the resulting image. Such a scheme has two main drawbacks: First, it generally implies time and complexity overheads compared to equivalent methods (if they exist) in the JPEG 2000 compressed domain. Such effects become important when the scheme is repeatedly applied on multiple images. Second, since encoding and decoding operations are generally lossy, the introduced distortion can become non negligible whenever several processing tasks are repeated on a same image. These observations lead to the second objective of this thesis, which is to adapt or create a watermarking algorithm dedicated to the JPEG 2000 compressed domain. Finally, there are imaging algorithms, such as authentication and access control, that already exist and could be directly applied to JPEG 2000 images but, because they do not take into account the coding algorithm specifics, they either decrease compression performance or remove useful features of the encoded representation (scalability, random access, etc). This leads to the third objective of this thesis, which is to adapt, create and combine authentication and access control algorithms with JPEG 2000 coding and decoding. Thus, the common goal of the three objectives described above is the deep integration of selected processing and security algorithms into a JPEG 2000 codec in order to provide with minimum complexity, JPEG 2000 compliant codestreams and an unified framework for many imaging applications...|$|R
40|$|The {{objective}} of multithreaded execution models is masking the latency of inter processor communications and remote memory accesses in large-scale multiprocessors. Several such models combine aspects of dataflow-like execution with the von Neumann model {{in an attempt}} to provide both efficient synchronization (as in the dataflow model) and efficient exploitation of program locality (as in the von Neumann model). We refer to these models as data-driven multithreading models. One of the factors that distinguishes these models is the thread execution strategy: A thread can be either non-blocking or blocking. Another factor is the architectural support for dynamic synchronization: The locality present within and among threads can potentially be exploited by a proper storage hierarchy for synchronization store (operand storage). Two storage models have been proposed for data-driven multithreaded execution. One is frame based, in which all the threads belonging to a <b>code-block</b> share one stora [...] ...|$|E
40|$|This paper {{describes}} a 158 MS/s JPEG 2000 codec with an embedded block coder (EBC) based on bit-plane and pass-parallel architecture. The EBC contains bit-plane coders (BPCs) corresponding to each bit-plane in a <b>code-block.</b> The upper {{and the lower}} bit-plane coding overlap in time with a 1 -stripe and 1 -column gap. The bit-modeling passes in the bit-plane coding also overlap {{in time with the}} same gap. These methods increase throughput by 30 times in comparison with the conventional. In addition, the methods support not only vertically causal mode, but also regular mode, which enhances the image quality. Furthermore, speculative decoding is adopted to increase throughput. This codec LSI was designed using 0. 18 /Limprocess. The core area is 4. 7 x 4. 7 mm 2 and the frequency is 160 MHz. A system including the codec enables image transmission of PC desktop with 8 ms delay. Copyright © 2008 The Institute of Electronics, Information and Communication Engineers...|$|E
40|$|This {{thesis is}} {{concerned}} with maximizing the coding efficiency, random accessibility and visual performance of scalable compressed video. The unifying theme behind this work {{is the use of}} finely embedded localized coding structures, which govern {{the extent to which these}} goals may be jointly achieved. The first part focuses on scalable volumetric image compression. We investigate 3 D transform and coding techniques which exploit inter-slice statistical redundancies without compromising slice accessibility. Our study shows that the motion-compensated temporal discrete wavelet transform (MC-TDWT) practically achieves an upper bound to the compression efficiency of slice transforms. From a video coding perspective, we find that most of the coding gain is attributed to offsetting the learning penalty in adaptive arithmetic coding through 3 D <b>code-block</b> extension, rather than inter-frame context modelling. The second aspect of this thesis examines random accessibility. Accessibility refers to the ease with which a region of interest is accessed (subband samples needed for reconstruction are retrieved) from a compressed video bitstream, subject to spatiotemporal <b>code-block</b> constraints. We investigate the fundamental implications of motion compensation for random access efficiency and the compression performance of scalable interactive video. We demonstrate that inclusion of motion compensation operators within the lifting steps of a temporal subband transform incurs a random access penalty which depends on the characteristics of the motion field. The final aspect of this thesis aims to minimize the perceptual impact of visible distortion in scalable reconstructed video. We present a visual optimization strategy based on distortion scaling which raises the distortion-length slope of perceptually significant samples. This alters the codestream embedding order during post-compression rate-distortion optimization, thus allowing visually sensitive sites to be encoded with higher fidelity at a given bit-rate. For visual sensitivity analysis, we propose a contrast perception model that incorporates an adaptive masking slope. This versatile feature provides a context which models perceptual significance. It enables scene structures that otherwise suffer significant degradation to be preserved at lower bit-rates. The novelty in our approach derives from a set of "perceptual mappings" which account for quantization noise shaping effects induced by motion-compensated temporal synthesis. The proposed technique reduces wavelet compression artefacts and improves the perceptual quality of video...|$|E
40|$|Ultra {{reliable}} and low latency communication (URLLC) is a newly introduced service category in 5 G to support delay-sensitive applications. In {{order to support}} this new service category, 3 rd Generation Partnership Project (3 GPP) sets an aggressive requirement that a packet should be delivered with 10 - 5 block error rate within 1 msec transmission period. Since the current wireless standard designed to maximize the coding gain by transmitting capacity achieving long <b>code-block</b> is not relevant for this purpose, entirely new strategy is required. In this paper, we propose {{a new approach to}} deliver control information, called sparse vector coding (SVC). Key idea behind the proposed method is to transmit the control channel information after the sparse vector transformation. By mapping the control information into the position of nonzero elements and then transmitting it after the random spreading, we obtain underdetermined sparse system for which the principle of compressed sensing can be applied. Comment: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessibl...|$|E
40|$|Low-density parity-check (LDPC) codes {{characterized}} by minimum Hamming distances proportional to block sizes have been demonstrated. Like the codes {{mentioned in the}} immediately preceding article, the present codes are error-correcting codes suitable {{for use in a}} variety of wireless data-communication systems that include noisy channels. The previously mentioned codes have low decoding thresholds and reasonably low error floors. However, the minimum Hamming distances of those codes do not grow linearly with <b>code-block</b> sizes. Codes that have this minimum-distance property exhibit very low error floors. Examples of such codes include regular LDPC codes with variable degrees of at least 3. Unfortunately, the decoding thresholds of regular LDPC codes are high. Hence, {{there is a need for}} LDPC codes {{characterized by}} both low decoding thresholds and, in order to obtain acceptably low error floors, minimum Hamming distances that are proportional to <b>code-block</b> sizes. The present codes were developed to satisfy this need. The minimum Hamming distances of the present codes have been shown, through consideration of ensemble-average weight enumerators, to be proportional to code block sizes. As in the cases of irregular ensembles, the properties of these codes are sensitive to the proportion of degree- 2 variable nodes. A code having too few such nodes tends to have an iterative decoding threshold that is far from the capacity threshold. A code having too many such nodes tends not to exhibit a minimum distance that is proportional to block size. Results of computational simulations have shown that the decoding thresholds of codes of the present type are lower than those of regular LDPC codes. Included in the simulations were a few examples from a family of codes characterized by rates ranging from low to high and by thresholds that adhere closely to their respective channel capacity thresholds; the simulation results from these examples showed that the codes in question have low error floors as well as low decoding thresholds. As an example, the illustration shows the protograph (which represents the blueprint for overall construction) of one proposed code family for code rates greater than or equal to 1. 2. Any size LDPC code can be obtained by copying the protograph structure N times, then permuting the edges. The illustration also provides Field Programmable Gate Array (FPGA) hardware performance simulations for this code family. In addition, the illustration provides minimum signal-to-noise ratios (Eb/No) in decibels (decoding thresholds) to achieve zero error rates as the code block size goes to infinity for various code rates. In comparison with the codes mentioned in the preceding article, these codes have slightly higher decoding thresholds...|$|E
40|$|JPEG 2000 is a wavelet {{transform}} based image compression and coding standard. It provides superior rate-distortion performance {{when compared to}} the previous JPEG standard. In addition JPEG 2000 provides four dimensions of scalability—distortion, resolution, spatial, and color. These superior features make JPEG 2000 ideal for use in power and bandwidth limited mobile applications like urban search and rescue. Such applications require a fast, low power JPEG 2000 encoder to be embedded on the mobile agent. This embedded encoder needs to also provide superior subjective quality to low bitrate images. This research addresses these two aspects of enhancing the performance of JPEG 2000 encoders. The JPEG 2000 standard includes a perceptual weighting method based on the contrast sensi-tivity function (CSF). Recent literature shows that perceptual methods based on subband standard deviation are also effective in image compression. This research presents two new perceptual weight-ing methods that combine information from both the human contrast sensitivity function as well as the standard deviation within a subband or <b>code-block.</b> These two new sets of perceptual weights are compared to the JPEG 2000 CSF weights. The results indicate that our new weights performed better than the JPEG 2000 CSF weights for high frequency images. Weights based solely on sub...|$|E
