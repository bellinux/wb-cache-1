378|10000|Public
5000|$|<b>Correctness</b> <b>and</b> <b>completeness</b> of the dataset has to {{be checked}} several times ...|$|E
50|$|The Department of Safeguards is {{responsible}} for carrying out this mission, through technical measures designed to verify the <b>correctness</b> <b>and</b> <b>completeness</b> of states' nuclear declarations.|$|E
50|$|The UPDM plugin {{supports}} the latest OMG UPDM Specification 2.0 version. It unifies MoDAF 1.2, DoDAF 1.5 and DoDAF 2.0, NAF 3. It has support for all DoDAF and MoDAF modeling artifacts {{based on the}} DoDAF and MoDAF Architecture Frameworks, with reports, wizards, model <b>correctness</b> <b>and</b> <b>completeness</b> validation constraints, as well as usability features.|$|E
30|$|There is no {{correlation}} between the <b>correctness</b> indicator <b>and</b> the <b>completeness</b> indicator.|$|R
40|$|To {{validate}} the <b>correctness,</b> consistency <b>and</b> <b>completeness</b> of software requirements and designs, software engineers require a precise {{understanding of the}} functions to be performed by the system {{and the degree to}} which those functions satisfy soft-ware requirements. In addition, requirements engineers must also consider the rol...|$|R
5000|$|Richard P. Gabriel {{suggests}} that a key advantage of Unix was that it embodied a design philosophy he termed [...] "worse is better", in which simplicity of both the interface and the implementation {{are more important than}} any other attributes of the system—including <b>correctness,</b> consistency, <b>and</b> <b>completeness.</b> Gabriel argues that this design style has key evolutionary advantages, though he questions the quality of some results.|$|R
50|$|Those {{universities}} that have signed MoUs with INFLIBNET Centre {{are required to}} identify a senior academic {{to serve as a}} University Coordinator to liaise with the University and the Centre. Responsibilities of the Coordinator include timely submission of soft copies of PhD theses submitted to the University to Shodhganga and to verify the <b>correctness</b> <b>and</b> <b>completeness</b> of these soft copies.|$|E
50|$|In {{transaction}} processing software, traceability implies {{use of a}} unique piece of data (e.g., order date/time or a serialized sequence number) which can be traced through the entire software flow of all relevant application programs. Messages and files {{at any point in}} the system can then be audited for <b>correctness</b> <b>and</b> <b>completeness,</b> using the traceability key to find the particular transaction. This is also sometimes referred to as the transaction footprint.|$|E
50|$|In food {{processing}} software, traceability systems imply {{the use of}} a unique piece of data (e.g., order date/time or a serialized sequence number, generally through {{the use of a}} barcode / RFID) which can be traced through the entire production flow, linking all sections of the business, including suppliers & future sales through the supply chain. Messages and files at any point in the system can then be audited for <b>correctness</b> <b>and</b> <b>completeness,</b> using the traceability software to find the particular transaction and/or product within the supply chain.|$|E
3000|$|We {{introduce}} four {{indicators for}} the data quality assessment: amount of data, <b>correctness,</b> <b>completeness,</b> <b>and</b> time correlation index measure. We also provide detailed measurement {{for the relationship between}} different indicators.|$|R
40|$|This paper answers open {{questions}} about the <b>correctness</b> <b>and</b> the <b>completeness</b> of Dart-Zobel algorithm for testing the inclusion relation between two regular types. We show that the algorithm is incorrect for regular types. We also prove that the algorithm is complete for regular types as well as correct for tuple distributive regular types. Also presented is a simplified version of Dart-Zobel algorithm for tuple distributive regular types. Comment: 16 page...|$|R
40|$|The Expert Systems Validation Associate (EVA), a {{validation}} {{system under}} {{development at the}} Lockheed Artificial Intelligence Center {{for more than a}} year, provides a wide range of validation tools to check the <b>correctness,</b> consistency <b>and</b> <b>completeness</b> of a knowledge-based system. A declarative meta-language (higher-order language), is used to create a generic version of EVA to validate applications written in arbitrary expert system shells. The architecture and functionality of EVA are presented. The functionality includes Structure Check, Logic Check, Extended Structure Check (using semantic information), Extended Logic Check, Semantic Check, Omission Check, Rule Refinement, Control Check, Test Case Generation, Error Localization, and Behavior Verification...|$|R
50|$|The masks used {{to process}} and {{manufacture}} SSI, MSI and early LSI and VLSI devices (such as the microprocessors {{of the early}} 1970s) were mostly created by hand, often using Rubylith-tape or similar. For large or complex ICs (such as memories or processors), this was often done by specially hired layout people under supervision {{of a team of}} engineers, who would also, along with the circuit designers, inspect and verify the <b>correctness</b> <b>and</b> <b>completeness</b> of each mask. However, modern VLSI devices contain so many transistors, layers, interconnections, and other features that it is no longer feasible to check the masks or do the original design by hand. The engineer depends on computer programs and other hardware aids to do most of this work.|$|E
50|$|On 27 June, {{a second}} draft was published. It now offered a precise “join and destroy” option for nuclear armed states: States joining the treaty “shall submit, {{no later than}} sixty days after the {{submission}} of its declaration, a time-bound plan for the verified and irreversible destruction of its nuclear weapons programme to be negotiated with the States Parties” (Article 4, 1). A second “destroy and join” option (Article 4, 5) only provides cooperation with the IAEA in order to verify the <b>correctness</b> <b>and</b> <b>completeness</b> of the inventory of nuclear material, no verification of the elimination. This has been changed in the final text. A further discussed topic was the explicit acceptance of the “use of nuclear energy for peaceful purposes without discrimination”. The respective affirmation remained part of the final preamble.|$|E
50|$|Terms of {{the pact}} and {{consequent}} agreements included the shutdown {{of the pilot}} Yongbyon nuclear reactor, abandoning the construction of two larger nuclear power plants, and the canning and sealing, under IAEA monitoring, of spent fuel {{that could have been}} reprocessed to create plutonium for a nuclear weapon.In exchange two light water reactors would be constructed in North Korea by 2003 at a cost of $4 billion, primarily supplied by South Korea. In the interim, North Korea would be supplied with 500,000 tons of heavy fuel oil annually, at no cost, {{to make up for lost}} energy production. North Korea was required to come into full compliance with its IAEA safeguards agreement, allowing the IAEA to verify the <b>correctness</b> <b>and</b> <b>completeness</b> of its initial declaration, before key nuclear components of the reactor would be delivered. When the LWR plants were completed, North Korea would dismantle its other nuclear reactors and associated facilities.|$|E
40|$|The {{introduction}} of software formal inspections (Fagan Inspections) at JPL for finding and fixing defects {{early in the}} software development life cycle are reviewed. It is estimated that, by the year 2000, some software efforts will rise {{to as much as}} 80 percent of the total. Software problems are especially important at NASA as critical flight software must be error-free. It is shown that formal inspections are particularly effective at finding and removing defects having to do with clarity, <b>correctness,</b> consistency, <b>and</b> <b>completeness.</b> A very significant discovery was that code audits were not as effective at finding defects as code inspections...|$|R
40|$|High quality {{information}} {{has a significant}} impact on improving hospital efficiency and patient satisfaction, as well as resolving patient disputes. In this paper, {{on the basis of the}} analysis of the research status of information quality and operation performance, information quality is considered as an important contributory factor in improving patient throughput. A theoretical framework and main content are presented. Through the establishment of quantitative information, quality indicators such as <b>correctness,</b> timeliness <b>and</b> <b>completeness,</b> <b>and</b> the impact on operation performance (registered queue length, waiting time, utilization of hospital facilities), together with the cost of the operating process, are analyzed from the theoretical aspect and then verified by simulation technology. Finally, the significance and feasibility of this study are clarified...|$|R
40|$|NUMERICA is a {{modeling}} {{language for}} stating and solving global optimization problems. It {{makes it possible}} to express these problems in a notation close to the way these problems are stated in textbooks or scientific papers. In addition, the constraint-solving algorithm of NUMERICA, which combines techniques from numerical analysis and artificial intelligence, provides many guarantees about <b>correctness,</b> convergence, <b>and</b> <b>completeness.</b> This paper is a gentle introduction to NUMERICA. It highlights some of the main difficulties of global optimization and illustrates the functionality of NUMERIC A by contrasting it to traditional methods. It also presents the essence of the constraint-solving algorithm of NUMERICA in a novel...|$|R
30|$|The {{following}} theorems state, respectively, <b>correctness</b> <b>and</b> <b>completeness</b> of the {{constraint set}} satisfiability algorithms presented in Fig.  3, {{with respect to}} the entailment relation.|$|E
40|$|This work {{presents}} a formal treatment of <b>correctness</b> <b>and</b> <b>completeness</b> {{for a set}} of seven uninterpreted Register Transfer Level (RTL) transformations. The completeness property ensures that a transformational derivation system based on this set is able to explore the entire design space, for a well-defined class of designs. The formalization for behavior specifications, RTL implementations and RTL transformations, as well as the mechanized proofs for <b>correctness</b> <b>and</b> <b>completeness</b> are conducted within the higher-order logic of the Prototype Verification System (PVS). ...|$|E
40|$|Two {{types of}} {{precedence}} relationship representations for mechanical assembly sequences are presented: precedence {{relationships between the}} establishment of one connection between two parts {{and the establishment of}} another connection, and precedence relationships between the establishment of one connection and states of the assembly process. Precedence relationship representations have the advantage of being very compact. The problem with these representations was how to guarantee their <b>correctness</b> <b>and</b> <b>completeness.</b> Two theorems are presented each of which leads to the generation of one type of precedence relationship representation guaranteeing its <b>correctness</b> <b>and</b> <b>completeness</b> for a class of assemblies...|$|E
40|$|In this paper, we {{described}} {{an approach to}} model Fuzzy Hybrid Rule/Frame-based Expert Systems (FHES). The approach is based on Coloured Petri Nets (CPN) and Controlled State Tokens (CST). First, we defined the properties of a FHES and illustrate how the various features are being modelled by CPN. Secondly, we applied our approach to a practical personnel selection system currently being used in Hong Kong. The detection {{and analysis of the}} anomalies of proposed model is done by constructing and examining the reachability tree spanned by the knowledge inference. Lastly, our approach can provide formal verification of the <b>correctness,</b> consistency, <b>and</b> <b>completeness</b> of the FHES...|$|R
30|$|Integrity mainly {{includes}} the storage {{integrity of the}} medical data administrator and the query integrity of the medical staff or patient personnel. Storage integrity means that the tampering, additions, and deletions of stored data by cloud service providers or other attackers are perceived by medical data administrators. Query integrity {{includes the}} <b>correctness,</b> <b>completeness,</b> <b>and</b> freshness of search results.|$|R
40|$|Abstract. Parsing {{algorithms}} {{for various}} types of languages are represented in a formal logic framework as deduction systems, where items (formulas) describe the grammatical status of strings, and inference rules level, Parsing Deduction Systems reflect the structure of parsers in a clear and concise manner and provide unified tools for the proof of <b>correctness,</b> <b>completeness</b> <b>and</b> complexity analysis...|$|R
30|$|Sahar et al. {{evaluated}} completeness and correctness {{for their}} work, and quality measures are not calculated [18]. From their measures, the maximum value of completeness is 92 % and correctness is 98 %, which is lesser than the proposed work maximum values shown in Table  3. Boshir et al. [27] evaluated correctness measures in their work; completeness and quality measures are not given. From those measures, minimum value of correctness is 92.82 % and average value is 95.71 % which is lesser than the proposed work. Maximum value is 100 % but proposed work has 99 %. Pankaj et al. calculated <b>correctness</b> <b>and</b> <b>completeness</b> values {{for only one}} image [24]. The <b>correctness</b> <b>and</b> <b>completeness</b> values are 96.52 % and 95.32 %, respectively. From Table  1, for test image 4, the <b>correctness</b> <b>and</b> <b>completeness</b> values are 98 % and 95 %, respectively. In some work, performance measures are not evaluated [17, 19, 23, 31, 32, 34]. So those works are not taken for comparison with the proposed work.|$|E
40|$|This paper {{analyses}} four representations for assembly sequences {{which are}} based on directed graphs, on AND/OR graphs, on establishment conditions, and on precedence relationships. The latter includes two types: precedence relationships between the establishment of one connection between parts and the establishment of another connection, and precedence relatioships between the establishment of one connection and states of the assembly process. The paper discusses how each representation is related to the others. The <b>correctness</b> <b>and</b> <b>completeness</b> of these representations are also addressed. The results presented are needed to prove the <b>correctness</b> <b>and</b> <b>completeness</b> of algorithms for the generation of mechanical assembly sequences. 1...|$|E
40|$|Abstract: A {{problem of}} {{incompatible}} and contradictory of results of making decisions in industrial organizationand-technical complexes (OTC) is outlined. A system-and-goal approach and a semiotic approach to make system of decisions satisfying requirements of logical <b>correctness</b> <b>and</b> <b>completeness</b> are concretized. A concept model of knowledge-based dialog system of analysis and synthesis of goals (DS ASG) to make system of goals is considered. Using the DS ASG decision-makers work out {{not only the}} system of goals but corresponding plan of goal-achieving satisfying requirements of logical <b>correctness</b> <b>and</b> <b>completeness</b> and in this way solve the problem of incompatible and contradictory of results of making decisions in OTC...|$|E
40|$|We {{present a}} new model for {{automated}} verification of security protocols, permitting {{the use of an}} unbounded number of protocol runs. We prove its <b>correctness,</b> <b>completeness</b> <b>and</b> also that it terminates. It has been implemented and its efficiency is clearly shown by the number of protocols successfully studied. In particular, we present an attack previously unreporte- d on the Denning-Sacco symmetric key protocol...|$|R
40|$|We {{deal with}} the problem how to gain {{information}} from many heterogeneous data sources. As a solution we propose a general framework for mediated information gain: it minimizes and supports the human burden, clearly identifies the algorithmically tractable subtasks, and assures a well-understood level of quality. We identify a "mediation task" as an extended amalgamation of searching, querying and updating in traditional information systems, and we introduce the notion of a "formal information state" which unifies and extends the classical notions of a schema, an instance, a query with additional specifications and a returned answer. The quality is concerned with <b>correctness,</b> precision <b>and</b> <b>completeness.</b> <b>Correctness</b> is guaranteed by maintaining internal consistency. Precision is achieved by embeddings identifying only relevant parts of the sources. <b>And</b> <b>completeness</b> is reached by user interactions with revisions and by iteration. The whole framework is characterized by eighteen rules whic [...] ...|$|R
40|$|Rapport interne. In this paper, {{we propose}} general {{guidelines}} {{that could be}} considered to define transformation rules when programming using a rule-based approach. We apply the proposed steps for solving some typical problems in Computer Science. Through these examples, we also show how clear and easy it is to prove properties such as <b>correctness,</b> <b>completeness</b> <b>and</b> termination following these guidelines when a rule-based paradigm is used...|$|R
40|$|Five {{types of}} {{representations}} for assembly sequences are reviewed: the directed graph of feasible assembly sequences, the AND/OR graph of feasible assembly sequences, {{the set of}} establishment conditions, and two types of sets of precedence relationships. (precedence relationships between the establishment of one connection between parts {{and the establishment of}} another connection, and precedence relationships between the establishment of one connection and states of the assembly process). The mappings of one representation into the others are established. The <b>correctness</b> <b>and</b> <b>completeness</b> of these representations are established. The results presented are needed in the proof of <b>correctness</b> <b>and</b> <b>completeness</b> of algorithms for the generation of mechanical assembly sequences...|$|E
40|$|The {{main goal}} of Model Driven Architecture (MDA) is the {{automation}} of software development process. According this technology, {{we need to}} create platform independent model (PIM), after that transform it to platform specific model (PSM); from PSM model we can generate program code. To do that, we need to ensure static and dynamic completeness of PIM. All these models are described using UML modeling language. In this work <b>correctness</b> <b>and</b> <b>completeness</b> of PIM is achieved by analyzing compatibility of class diagrams and state machines, as well as <b>correctness</b> <b>and</b> <b>completeness</b> of state machines themselves. To solve this problem, algorithms were created and implemented in a plug-in for MagicDraw CASE tool...|$|E
40|$|Jointly {{deployed}} aspects may {{interact with}} each other. While some interactions might be intended, unintended interactions (interferences) can break a program. Detecting and resolving interferences is particularly hard if aspects are developed independently, without knowledge of each other. Work on interference detection has focused so far on the correctness of weaved programs. In this paper {{we focus on the}} <b>correctness</b> <b>and</b> <b>completeness</b> of aspect weaving. We show that a large class of interferences result from incorrect or incomplete weaving and present a language independent <b>correctness,</b> <b>and</b> <b>completeness</b> Our technique can check aspect interferences independent of any base program and is applicable to aspects that contain implicit mutual dependencies in their implementation, without needing special purpose program annotations or formal specifications of aspect semantics. 1...|$|E
30|$|In {{real-time}} {{systems that}} have to adapt themselves in bound periods of time, any optimization algorithm must meet <b>correctness,</b> <b>completeness,</b> <b>and</b> efficiency requirements, preferably {{with a high degree}} of optimality. Algorithm correctness is fundamental since it is impracticable to deploy an invalid configuration, i.e., a configuration that does not fulfill feature constraints <b>and</b> resource restrictions. <b>Completeness</b> <b>and</b> time efficiency are required under time constraints. Finally, although an optimal solution is not mandatory, it is desirable to compute “good-enough” solutions. The proposed Configuration Selection Algorithm (Sanchez et al. 2013) meets these requirements.|$|R
40|$|Software {{quality is}} of primary concern in all {{large-scale}} expert system development efforts. Building appropriate validation and test tools for ensuring software reliability of expert systems is therefore required. The Expert Systems Validation Associate (EVA) is a validation system under {{development at the}} Lockheed Artificial Intelligence Center. EVA provides {{a wide range of}} validation and test tools to check <b>correctness,</b> consistency, <b>and</b> <b>completeness</b> of an expert system. Testing a major function of EVA. It means executing an expert system with test cases with the intent of finding errors. In this paper, we describe many different types of testing such as function-based testing, structure-based testing, and data-based testing. We describe how appropriate test cases may be selected in order to perform good and thorough testing of an expert system...|$|R
40|$|Abstract. Information system models {{commonly}} describe {{organizations in}} terms {{of the structure of the}} data they use, the organization of the processes they perform and the operations that will be executed. Yet to date, the models tend to neglect the rules and constraints under which the business operates. Frequently these are not represented or even documented, but hard-coded or hidden into implementations. Compliance with regulations and verification of business rules is difficult when a considerable amount of business knowledge is only represented in code or database constructions and is not modeled in an explicit way that is easy to understand and verify. When properly defined, decision tables offer a solution to several issues in the modeling and verification of regulations, such as guaranteeing consistency, <b>correctness,</b> non-redundancy <b>and</b> <b>completeness...</b>|$|R
