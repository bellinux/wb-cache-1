365|5724|Public
25|$|Event chain {{methodology}} is an uncertainty {{modeling and}} schedule network analysis technique that {{is focused on}} identifying and managing events and event chains that affect project schedules. Event chain methodology is the next advance beyond <b>critical</b> <b>path</b> <b>method</b> and critical chain project management.|$|E
25|$|In 1959, Mauchly left Sperry Rand {{and started}} Mauchly Associates, Inc. One of Mauchly Associates' notable {{achievements}} was {{the development of}} the <b>Critical</b> <b>Path</b> <b>Method</b> (CPM) which provided for automated construction scheduling. Mauchly also set up a consulting organization, Dynatrend, in 1967 and worked as a consultant to Sperry UNIVAC from 1973 until his death in 1980.|$|E
25|$|The need {{to break}} German codes in World War II led to {{advances}} in cryptography and theoretical computer science, with the first programmable digital electronic computer being developed at England's Bletchley Park with the guidance of Alan Turing and his seminal work, On Computable Numbers. At the same time, military requirements motivated advances in operations research. The Cold War meant that cryptography remained important, with fundamental advances such as public-key cryptography being developed in the following decades. Operations research remained important as a tool in business and project management, with the <b>critical</b> <b>path</b> <b>method</b> being developed in the 1950s. The telecommunication industry has also motivated advances in discrete mathematics, particularly in graph theory and information theory. Formal verification of statements in logic has been necessary for software development of safety-critical systems, and advances in automated theorem proving have been driven by this need.|$|E
40|$|There {{are several}} methods for solving fuzzy <b>critical</b> <b>path</b> {{problems}} in which ranking approaches {{are used to}} compare fuzzy numbers. In this paper, some fuzzy <b>critical</b> <b>path</b> problems are chosen {{to show that the}} results obtained with fuzzy <b>critical</b> <b>path</b> <b>methods</b> that use existing ranking approaches are not appropriate according to real life situations. To obtain appropriate results, a new ranking approach the Mehar ranking approach, is proposed for comparing LR flat fuzzy numbers. To show the advantages of the Mehar ranking approach over existing approaches, selected fuzzy <b>critical</b> <b>path</b> problems are solved by using the existing methods together with the Mehar ranking approach. It is shown that the obtained results are appropriate. ...|$|R
50|$|In the {{implementation}} field several techniques are used. A well-known method, and specifically oriented on {{the implementation}} field, is the Regatta method by Sogeti. Other techniques are the SAP Implementation method, which {{is adapted to}} implementing SAP systems. Systems are installed in several different ways. Different organizations may have their own methods, When implementing a system, it is considered a project and thus must be handled as such. Well known theories and methods {{are used in the}} field such as the PRINCE2 method with all of its underlying techniques, such as a PERT diagram, Gantt chart and <b>critical</b> <b>path</b> <b>methods.</b>|$|R
40|$|We propose and {{evaluate}} a hybrid task scheduling method {{in order to}} reduce elapse time of parallel and distributed programs. Our task scheduling <b>method</b> combines the <b>critical</b> <b>path</b> (CP) scheduling <b>method</b> and the list scheduling <b>method.</b> The <b>critical</b> <b>path</b> scheduling <b>method</b> is efficient to assign tasks to processors, but it needs long time. The list scheduling methods is not efficient, but the scheduling time is very short. We have implemented a hybrid method which changes these scheduling methods by the number of tasks and workers. In the result, when we execute parallel and distributed programs based on 180 task graphs which include 100 tasks, faster than the original task scheduling methods at many cases. 情報処理学会研究報告　高知県高知市(2006...|$|R
500|$|Construction {{started on}} August 13, 1963, with an {{elaborate}} ceremony hosted by Mayor Drapeau on barges {{anchored in the}} St. Lawrence River. Ceremonially, construction began when Prime Minister Lester B. Pearson pulled a lever that signalled a front-end loader to dump the first batch of fill to enlarge Saint Helen's Island, and Quebec premier Jean Lesage spread the fill with a bulldozer. Of the 25 million tons of fill needed to construct the islands, 10-12% {{was coming from the}} Montreal Metro's excavations, a public works project that was already under construction before Expo was awarded to Montreal. The remainder of the fill came from quarries on Montreal and the South Shore, however even with that it was insufficient and so bodies of water on both islands were added (lakes and canals) {{to reduce the amount of}} fill required. Expo's initial construction period mainly centered on enlarging Saint Helen's Island, creating the artificial island of Notre Dame Island and lengthening and enlarging the Mackay Pier which became the Cité du Havre. While construction continued, the land rising out of Montreal harbour was not owned by the Expo Corporation yet. After the final mounds of earth completed the islands, the grounds that would hold the fair were officially transferred from the City of Montreal to the corporation on June 20, 1964. This gave Colonel Churchill only 1042 days to have everything built and functioning for opening day. To get Expo built in time, Churchill used the then new project management tool known as the <b>critical</b> <b>path</b> <b>method</b> (CPM). [...] On April 28, 1967, opening day, everything was ready, with one exception: Habitat 67, which was then displayed as a work in progress.|$|E
2500|$|Dr. Mauchly stayed {{involved}} in computers {{for the rest}} of his life. [...] He was a founding member and president of the Association for Computing Machinery (ACM) and also helped found the Society for Industrial and Applied Mathematics (SIAM), serving as its fourth president. The Eckert-Mauchly Corporation was bought by Remington Rand in 1950 and for ten years Dr. Mauchly remained as Director of Univac Applications Research. [...] Leaving in 1959 he formed Mauchly Associates, a consulting company that later introduced the <b>critical</b> <b>path</b> <b>method</b> (CPM) for construction scheduling by computer. In 1967 he founded Dynatrend, a computer consulting organization. In 1973 he became a consultant to Sperry Univac.|$|E
5000|$|Defining {{critical}} and non-critical tasks using <b>critical</b> <b>path</b> <b>method</b> ...|$|E
40|$|Several {{approaches}} to post-silicon adaptation require feedback from {{a replica of}} the nominal <b>critical</b> <b>path,</b> whose variations are intended to reflect those of the entire circuit after manufacturing. For realistic circuits, where the number of <b>critical</b> <b>paths</b> can be large, the notion of using a single <b>critical</b> <b>path</b> is too simplistic. This paper overcomes this problem by introducing the idea of synthesizing a representative <b>critical</b> <b>path</b> (RCP), which captures these complexities of the variations. We first prove that the requirement on the RCP is that it should be highly correlated with the circuit delay. Next, we present two novel algorithms to automatically build the RCP. Our experimental results demonstrate that over a number of samples of manufactured circuits, the delay of the RCP captures the worst case delay of the manufactured circuit. The average prediction error of all circuits is shown to be below 2. 8 % for both approaches. For both our approach and the <b>critical</b> <b>path</b> replica <b>method,</b> it is essential to guard-band the prediction to ensure pessimism: our approach requires a guard band 30 % smaller than for the <b>critical</b> <b>path</b> replica <b>method...</b>|$|R
40|$|The {{performance}} of a fast fault simulation algorithm for combina-tional circuits, such as the <b>critical</b> <b>path</b> tracing <b>method,</b> is deter-mined primarily by the efficiency with which it can deduce the detectability of stem faults (stem analysis). We propose a graph based approach to perform stem analysis. A dynamic data struc-ture, called the criticality constraint graph, is used during the backward pass to carry information related to self masking and multiple-path sensitization of stem faults. The structure is updated {{in such a way}} that when stems are reached their critical-ity can be found by looking at the criticality constraints on their fanout branches. Compared to the <b>critical</b> <b>path</b> tracing <b>method,</b> our algorithm is exact and does not require forward propagation of individual stem faults. Several examplca are given to illustrate the power of the algorithm. Preliminary data on an implement& tion is also provided. 1...|$|R
40|$|Abstract — In {{nanoscale}} {{technologies that}} experience large levels of process variation, post-silicon adaptation {{is an important}} step in circuit design. These adaptation techniques are often based on measurements on a replica of the nominal <b>critical</b> <b>path,</b> whose variations are intended to reflect those of the entire circuit after manufacturing. For realistic circuits, where the number of <b>critical</b> <b>paths</b> can be large, the notion of using a single <b>critical</b> <b>path</b> is too simplistic. This paper overcomes this problem by introducing the idea of synthesizing a representative <b>critical</b> <b>path</b> (RCP), which captures these complexities of the variations. We first prove that the requirement on the RCP is that it should be highly correlated with the circuit delay. Next, we present three novel algorithms to automatically build the RCP. Our experimental results demonstrate that over a number of samples of manufactured circuits, the delay of the RCP captures the worst case delay of the manufactured circuit. The average prediction error of all circuits is shown to be below 2. 8 % for all three approaches. For both our approach and the <b>critical</b> <b>path</b> replica <b>method,</b> it is essential to guard-band the prediction to ensure pessimism: on average our approach requires a guard band 31 % smaller than for the <b>critical</b> <b>path</b> replica <b>method.</b> Index Terms — Algorithms, circuit analysis, design automation, timing. I...|$|R
5000|$|Program Evaluation and Review Technique (PERT) & <b>Critical</b> <b>path</b> <b>Method</b> (CPM) ...|$|E
50|$|NetPoint {{is capable}} of calculating {{schedules}} using both the <b>Critical</b> <b>Path</b> <b>Method</b> (CPM) {{as well as the}} Graphical Path Method (GPM).|$|E
50|$|First {{developed}} by the United States Navy in the 1950s, {{it is commonly used}} in conjunction with the <b>critical</b> <b>path</b> <b>method</b> (CPM).|$|E
40|$|Many {{algorithms}} for scheduling DAGs on multiprocessors {{have been}} proposed, {{but there has}} been little work done to determine their effectiveness. Since multi-processor scheduling is an NP-hard problem, no exact tractible algorithm exists, and no baseline is available from which to compare the resulting schedules. Furthermore, performance guarantees have been found for only a few simple DAGs. This paper is an attempt to quantify the differences in a few of the heuristics. New classification criteria are defined for the DAGs, and the differences between the heuristics are noted for various criteria. The comparison is made between a graph based <b>method,</b> and two <b>critical</b> <b>path</b> <b>methods.</b> The empirical performance of the three heuristics is compared when they are applied to the randomly generated DAGs. This work is supported by NSF grant number CCR- 9203319 0 A Numerical Comparative Analysis of Partitioning Heuristics for Scheduling Task Graphs on Multiprocessors Abstract Many algorithms to [...] ...|$|R
40|$|ABSTRACT In {{this paper}} a new {{approach}} to determine the <b>critical</b> <b>path</b> in a fuzzy project network based on the maxi-mum total float of the path is proposed. The advantage of the proposed method is explained through a numeri-cal example and compared with existing methods. The comparisons reveal that the proposed approach is an improved method over existing methods and more efficient in finding the <b>critical</b> <b>path.</b> An optimal <b>method</b> to find <b>critical</b> <b>path</b> in fuzzy project network...|$|R
40|$|Competition in {{industrial}} sector does not only apply to garment and electronic subsectors {{but also to}} food & beverage subsectors. Accordingly Mujur Jaya—a medium enterprise operating in food & beverage subsector—is required to maintain its existence, growth, and competitiveness. This can be accomplished through production decision which leads to production efficiency and product quality improvement. The objective {{of this study is}} to analyze time efficiency and cost of production processes within the firm. Network analysis was applied. Primary data regarding chronological order, time, and costs of production processes were collected and directly measured at the location of the firm. <b>Critical</b> <b>Path</b> <b>Methods,</b> in particular methods of Earliest Start Time & Earliest Finish and Latest Start Time & Latest Finish Time were used to analyze data. Results of the study indicated that production process cost can be reduced up to IDR 1870 per production cycle by cutting production time by approximately 2 minutes. In the long term, this improvement (i. e., time and cost of production process) would be of beneficials for the firm...|$|R
50|$|X-Pert {{is geared}} to {{producing}} <b>critical</b> <b>path</b> <b>method</b> and PERT schedules for major, long-term projects (in one case a duration now approaching 40 years).|$|E
5000|$|Analysis of Complex Projects by <b>Critical</b> <b>Path</b> <b>Method</b> (Analýza složitých procesů metodou kritické cesty), Ekonomicko-matematická laboratoř při EÚ ČSAV, Výzkumná publikace č. 4, Praha, 1964, p. 130.|$|E
5000|$|Tools: Schedule Network Analysis, <b>Critical</b> <b>path</b> <b>method,</b> {{schedule}} compression, {{what if scenario}} analysis, resources leveling, {{critical chain}} method, project management software, applying calendars, adjusting leads and lags, schedule model ...|$|E
40|$|Different {{categories}} of delay {{and the different}} types of delay found on construction projects are identified. Existing methods for assessing the effect of delaying events are reviewed and the results of an industry survey presented. None of the commonly recognized methods for delay analysis allow the assessment of three important issues at the same time: the progress of the project at the time the delay occurred; the changing nature of the critical path; and the effects of action taken (or that should have been taken) to minimize potential delays. A new method of delay analysis is presented. This method takes into consideration all these issues, and is a clear, straightforward step-by-step approach to the calculation of the expected delay in the completion of the project as a result of delays in activities encountered during the project. It uses a dynamic model considered by both experts and practitioners to be the best method to take account of progress and form the basis of delay claims. Project Delays, Claims, Delay Analysis, <b>Critical</b> <b>Path</b> <b>Methods,...</b>|$|R
40|$|We {{present an}} equation-based {{transistor}} size optimizer that minimizes delay of custom circuits. Our method uses static timing analysis {{to find the}} <b>critical</b> <b>paths</b> and numerical <b>methods</b> to optimize transistor sizes continuously without using simulation. Consequently, it is faster than simulation-based optimizers, and more general than standard cell optimizers. We demonstrate its efficacy and accuracy on a dynamic adder, where we achieve a 54 % speed-up and final <b>critical</b> <b>path</b> delay that matches Spice within 1 %. 1...|$|R
40|$|Abstract—Smartphones {{are facing}} a grand {{challenge}} in extending their battery life to sustain an increasing level of processing demand while subject to miniaturized form factor. Dynamic Voltage Scaling (DVS) {{has emerged as a}} critical technique to leverage power management by lowering the supply voltage and frequency of processors. In this paper, based on the DVS technique, we propose a novel Energy-aware Dynamic Task Scheduling (EDTS) algorithm to minimize total energy consumption for smartphones while satisfying stringent time constraints for applications. This al-gorithm utilizes the results from a static scheduling algorithm and aggressively reduces energy consumptions on the fly. Experimental results indicate that the EDTS algorithm can significantly reduce energy consumption for smartphones, compared to the <b>critical</b> <b>path</b> scheduling <b>method</b> and the parallelism-based scheduling algorithm. Keywords-Smartphone, DVS, dynamic scheduling, <b>critical</b> <b>path,</b> real-time I...|$|R
50|$|The <b>Critical</b> <b>Path</b> <b>Method</b> (CPM) is the {{traditional}} mathematical algorithm used for schedule logic computation. GPM utilizes a different algorithm than CPM and thus produces its own distinct schedule attributes.|$|E
50|$|The PERT {{distribution}} {{variation of}} the beta distribution is frequently used in PERT, <b>critical</b> <b>path</b> <b>method</b> (CPM) and other project management methodologies to characterize the distribution of an activity’s time to completion.|$|E
50|$|CPM-GOMS is a {{variation}} of the GOMS technique in human computer interaction. CPM-GOMS stands for two things: Cognitive Perceptual Motor and the project planning technique <b>Critical</b> <b>Path</b> <b>Method</b> (from which it borrows some elements).|$|E
40|$|Abstract—Critical path {{management}} {{is an important}} way for controlling the schedule of an engineering project, it always difficult to describe by a precise manner for the uncertain characteristic. In this paper, the interval number is applied to describe the uncertain characteristic, and a preference indexes λ are introduced to solve the <b>critical</b> <b>path</b> on linear programming problem. Meanwhile, the control priorities of sub-path can be also analyzed by a sensitivity coefficient δ. The result shows that compared with traditional probability methods and other interval <b>critical</b> <b>path</b> studies, the proposed method can improve the precision of the project time control. From the example and the result analysis, traditional probability method {{can deal with the}} uncertain problem of the network, but it may optimistically estimate the project duration time, namely, the duration time is underestimating. But interval <b>critical</b> <b>path</b> solving <b>method</b> could give more accurate duration time and <b>critical</b> <b>path,</b> and the proposed method could give more exact result and the calculation procedure is simpler. Keywords- critical path; uncertain management; engineering time control; interval number; activity management I...|$|R
40|$|Project {{distress}} predictions {{are essential}} in project management. Developing appropriate methods to classify projects and building prediction models for multi-criteria decisions requires empirical methods to minimise misclassification errors. This paper carries out multi-criteria analysis to classify projects risks using a preference disaggregation method, utilités additives discriminantes - UTADIS. The UTADIS requires predefined classification which is implemented using <b>critical</b> <b>path</b> analysis. The <b>methods</b> are applied on three projects {{and result in}} no misclassification error and an effective prediction model. project risks; uncertainty; multicriteria decision making; MCDM; classifications; utilités additives discriminantes; UTADIS; <b>critical</b> <b>path</b> analysis; CPA; risk classification; project management; prediction models; modelling...|$|R
40|$|An ecient {{approach}} towards nding a directed, {{shortest path}} or a directed longest {{path from the}} source to all other nodes in a directed network is described in this paper. Application of this approach with respect to CPM project networks is also considered. The approach {{is based on a}} minimum incoming weight labelling method and determines a <b>critical</b> <b>path</b> for a given project activity network. Although many algorithms exist in the operational research literature that may be used to nd <b>critical</b> <b>paths,</b> the <b>method</b> discussed in this paper has an interesting application in determining optimal crash limits for various activities in a CPM network. In case of network topology changes due to any reason, such as that an activity has to be completed in crash duration or the actual duration of an activity takes longer than scheduled, a new <b>critical</b> <b>path</b> and new associated oats can be computed without analyzing the complete network all over again. This is achieved by recycling part of the available information. The algorithm presented in this paper is illustrated by means of numerical examples...|$|R
50|$|The <b>{{critical}}</b> <b>path</b> <b>method</b> (CPM), or {{critical path}} analysis (CPA), is an algorithm for scheduling a set of project activities. It is commonly {{used in conjunction with}} the program evaluation and review technique (PERT).|$|E
50|$|Event chain {{methodology}} is an uncertainty {{modeling and}} schedule network analysis technique that {{is focused on}} identifying and managing events and event chains that affect project schedules. Event chain methodology is the next advance beyond <b>critical</b> <b>path</b> <b>method</b> and critical chain project management.|$|E
50|$|The {{triangular}} distribution, {{along with}} the PERT distribution, is also widely used in project management (as an input into PERT and hence <b>critical</b> <b>path</b> <b>method</b> (CPM)) to model events which take place within an interval defined by a minimum and maximum value.|$|E
40|$|Information about {{instruction}} criticality {{can be used}} {{to control}} the application of micro-architectural resources efficiently. To this end, several groups have proposed methods to predict critical instructions. In this paper, we present a framework which allows us to directly measure the criticality of individual dynamic instructions. This allows us to (1) measure the accuracy of proposed <b>critical</b> <b>path</b> predictors, (2) quantify the amount of slack present in non-critical instructions and (3) provide a new metric, called tautness, which ranks critical instructions by their dominance on the <b>critical</b> <b>path.</b> We investigate <b>methods</b> for improving <b>critical</b> <b>path</b> predictor accuracy, study the distribution of slack and tautness in programs, and show how our tautness metric {{can be used to}} improve the effectiveness of a load-latency-reducing, microarchitectural optimization...|$|R
40|$|After {{recalling the}} basic {{algorithms}} published by NIST for implementing the hash functions SHA- 256 (384, 512), a basic circuit {{characterized by a}} cascade of full adder arrays is given. Implementation options are discussed and two methods for improving speed are exposed: the delay balancing and the pipelining. An application of the former is first given, obtaining a circuit that reduces {{the length of the}} <b>critical</b> <b>path</b> by a full adder array. A pipelined version is then given, obtaining a reduction of two full adder arrays in the <b>critical</b> <b>path.</b> The two <b>methods</b> are afterwards combined and the results obtained through hardware synthesis are exposed, where a comparison between the new circuits is also given. 1...|$|R
40|$|<b>Critical</b> <b>path</b> {{analysis}} {{is a traditional}} way to study the parallelism of conservative parallel simulation. In this paper, we propose a new technique called state causality analysis to accurately model the conservative parallel network simulation. A theorem of simulation time advancement is presented and proved. Different from <b>critical</b> <b>path</b> analysis, our <b>method</b> focuses on the dependency of the logical process states instead of the unpredictable events. The effects of many algorithm-independent factors, such as lookahead, I/O overhead, physical transfer delay, processor speed and event distribution, are all taken into consideration. With this method, the complicated relations between parallel performance and the algorithm-independent factors can be revealed distinctly, which however cannot be revealed by <b>critical</b> <b>path</b> analysis in most cases. The theoretical analyses and experiments show that our method always predict a stricter upper bound {{on the performance of}} a given network simulation task than <b>critical</b> <b>path</b> analysis, which may provide a baseline to evaluate the conservative synchronization algorithms excluding the effects of algorithm-independent factors...|$|R
