1|4|Public
40|$|In {{order to}} {{transfer}} the Chinese <b>Classified</b> <b>Thesaurus</b> (CCT) into a machine-processable format and provide CCT-based Web services, a pilot study has been conducted in which a variety of selected CCT classes and mapped thesaurus entries are encoded with SKOS. OWL and RDFS are also used to encode the same contents {{for the purposes of}} feasibility and cost-benefit comparison. CCT is a collected effort led by the National Library of China. It is an integration of the national standards Chinese Library Classification (CLC) 4 th edition and Chinese Thesaurus (CT). As a manually created mapping product, CCT provides for each of the classes the corresponding thesaurus terms, and vice versa. The coverage of CCT includes four major clusters: philosophy, social sciences and humanities, natural sciences and technologies, and general works. There are 22 main-classes, 52, 992 sub-classes and divisions, 110, 837 preferred thesaurus terms, 35, 690 entry terms (non-preferred terms), and 59, 738 pre-coordinated headings (Chinese <b>Classified</b> <b>Thesaurus,</b> 2005) Major challenges of encoding this large vocabulary comes from its integrated structure. CCT {{is a result of the}} combination of two structures (illustrated in Figure 1) : a thesaurus that uses ISO- 2788 standardized structure and a classification scheme that is basically enumerative, but provides some flexibility for several kinds of synthetic mechanism...|$|E
40|$|AbstractSoftware {{documentation}} {{is usually}} expressed in natural languages, which contains much useful information. Therefore, establishing the traceability links between documentation and source code {{can be very}} helpful for software engineering management, such as requirement traceability, impact analysis, and software reuse. Currently, the recovery of traceability links is mostly based on information retrieval techniques, for instance, probabilistic model, vector space model, and latent semantic indexing. Previous work treats both documentation and source code as plain text files, but the quality of retrieved links can be improved by imposing additional structure using that they are software engineering documents. In this paper, we present four enhanced strategies to improve traditional LSI method based on the special characteristics of documentation and source code, namely, source code clustering, identifier <b>classifying,</b> similarity <b>thesaurus,</b> and hierarchical structure enhancement. Experimental {{results show that the}} first three enhanced strategies can increase the precision of retrieved links by 5 %∼ 16 %, while the the fourth strategy is about 13 %...|$|R
40|$|In summer 1994 EELS {{was one of}} {{the first}} SBIGs (Subject Based Information Gateways) to be {{published}} on the Internet. It is sponsorerd by the consortium of the Swedish universities of technology libraries and set up and supported by the NetLab in Lund university. The scanning of resources into EELS is decentralized within a network of subject editors in these university libraries. EELS applies the quality model of EU Telematics for research in DESIRE project. The resources are <b>classified</b> using the <b>thesaurus</b> and classification codes of Engineering Information, Inc. The main subject covered by EELS is, of course, engineering. In 1996 an experimental full-text index of international engineering resources, "All"Engineering, was added to EELSGodkänd; 2000; 20080806 (pafi...|$|R
40|$|The Decision Tree Learning Algorithnm (DTLAs) {{are getting}} keen {{attention}} from the natural langnagc processing research cmnmunity, {{and there have been}} a series of attempts to apply them to verbal case frame acquisition. However, a DTLA cannot handle structured at- tributes like nouns, which are <b>classified</b> under a <b>thesaurus.</b> In this paper, we prcscnt a new DTLA that can rationally handle the structured attributes. In the process of tree generation, the algorithm generalizes each attribute optimally using a given thesaurus. We apply titis algorithm to a bilingual corpus and show that it successfully learned a generalized decision tree for classifying the verb "take" and that the tree was smaller with more prediction power on the open data than the tree learned by the conventional DTLA...|$|R
40|$|Thesauri {{are used}} to provide {{controlled}} vocabularies for resource classification. Their use can greatly assist document discovery because thesauri man date a consistent shared terminology for describing documents. A particular <b>thesaurus</b> <b>classifies</b> documents according to an information community's needs. As a result, {{there are many different}} thesaural schemas. This has led to a proliferation of schema-specific thesaural systems. In our research, we exploit schematic regularities to design a generic thesaural ontology and specify it as a markup language. The language provides a common representational framework in which to encode the idiosyncrasies of specific thesauri. This approach has several advantages: it offers consistent syntax and semantics in which to express thesauri; it allows general purpose thesaural applications to leverage many thesauri; and it supports a single thesaural user interface by which information communities can consistently organise, store and retrieve electronic documents. Keywords: Electronic Documents, Metadata, Ontology, Thesaurus, XML...|$|R

