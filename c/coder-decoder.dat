37|0|Public
50|$|A codec is {{a device}} or {{computer}} program for encoding or decoding a digital data stream or signal. Codec is a portmanteau of <b>coder-decoder.</b>|$|E
5000|$|... the AN/MSQ-18's <b>coder-decoder</b> group (CDG) as {{the trailer}} van at each battery to [...] "decode the digital data {{coming to the}} battery use by the guided misslle system." ...|$|E
50|$|The {{compact disc}} had been {{initiated}} and {{pushed by the}} audio department, although NatLab researcher Kees Schouhamer Immink played an instrumental role in its design. For the industry group 'Audio' and the NatLab {{the development of a}} small optical audio disc started early in 1974 3. The sound quality of this disc had to be superior to that of the large and vulnerable vinyl record. To realize this, Lou Ottens, technical director of 'Audio', formed a seven-person project group. Vries and Diepeveen were members of this project group. In March 1974, during an Audio-VLP meeting Peek and Vries recommended a digital audio registration because an error-correcting code could be included 3. At the end of 1977, 'Audio' recognized that this {{was the only way to}} achieve a compact disc with a superior sound quality. Vries and Diepeveen built an error-correcting <b>coder-decoder</b> that was delivered in the summer of 1978. The decoder was included in the CD prototype player that was presented to the international press{4}. To commemorate this breakthrough, Philips received an IEEE Milestone Award on March 6, 2009 5. This breakthrough was also appreciated by Sony and they started a cooperation with Philips that resulted in June 1980 in a common CD system standard.|$|E
40|$|This paper {{describes}} {{how to use}} a CODEC (<b>COder-DECoder)</b> to encode, decode, synthesize and play AU-format audio data. The AU format use a μ-law (pronounced mu-law) compression technique improving dynamic range over the linear encoding of audio. The μ-law CODEC dates from 1965, yet is still in common use today. ...|$|E
40|$|Abstract:- Image {{compression}} {{often leads}} to undesired artifacts. This paper shows an image restoration approach to correct for these artifacts by an associative memory. Investigations for modeling the <b>coder-decoder</b> process by a linear system are described. A compensating system consisting of a neural network based associative memory has been developed to process the degraded result after compression/decompression. For an effective implementation of this method, the image contents must be considered. This way, shortcomings of the mentioned image compression algorithms can be reduced...|$|E
40|$|Abstract. This paper {{introduces}} an {{implementation and}} structure of a video codec (<b>coder-decoder)</b> using Vector Quantization (VQ) in the program Matlab. It also aims on the VQ weak and strong features and it describes an ex-periment with turning the advantages into concrete profit in video data compression. We would like to aim also on usage in video – MPEG 4 (Motion Picture Experts Group) standard – or in compressing the scientific (high resolu-tion) images. In conclusion we are comparing our ap-proach with other image compression methods with objec-tive and subjective criteria of image quality perception...|$|E
40|$|Over 70 % of {{the bridges}} in Costa Rica have {{critical}} {{elements in the}} structure. Wireless sensor networks {{have been used for}} structural monitoring because of their short installation time and low economic cost {{due to the lack of}} wiring.   In this research is presented a wireless sensor network which uses a compressed sensing algorithm for vibration monitoring on bridges. The network design is proposed expecting data integrity and energy harvesting.   The algorithm performs downsampling by <b>coder-decoder</b> pairs. The selected pair is conditional encoder and predictive decoder because this combination has elements in common that can shared to get better estimates in few steps.   Two <b>coder-decoder</b> pairs variants are presented: variable-fixed and fixed-variable. The first one proposes a constant factor compression during each sampling period while the second presents variable compression that depends of the signal behavior over time.   After an experimental test using Waspmotes the fixed-variable variant has a 56. 58 % reduction of power consumption by introducing a maximum error ± 0. 00195 g and compress in 52. 44 % the amount of samples. This algorithm increased the network energy autonomy from 17 hours to 26. 5 hours. Through mathematical analysis, the variable-fixed technique reduces in 74. 81 % the power consumption in sensing nodes transmissions and decrease in 90 % the number of samples...|$|E
40|$|Abstract — This paper compares {{two-dimensional}} (2 -D) and threedimensional (3 -D) object modeling {{in terms}} of their capabilities and performance (peak signal-to-noise-ratio and visual image quality) for very low bitrate video coding. We show that 2 -D object-based coding with affine/perspective transformations and triangular mesh models can simulate almost all capabilities of 3 -D object-based approaches using wireframe models {{at a fraction of the}} computational cost. Furthermore, experiments indicate that a 2 -D mesh-based <b>coder–decoder</b> performs favorably compared to the new H. 263 standard {{in terms of}} visual quality. Index Terms—Model-based coding, MPEG Phase 4, three-dimensional wireframe, two-dimensional mesh, video compression. I...|$|E
40|$|Inmost block-transform based codecs (<b>coder-decoder)</b> the {{compressed}} image is reconstructed using only the transmitted data. In this paper, the reconstruction is formulated as a regularized image recovery problem where both the transmitted data and prior {{knowledge about the}} properties of the original image are used. This is accomplished by minimizing an objective function, using iterative algorithms, which captures the smoothness properties of the original image. Experimental results are presented which demonstrate that the proposed regularized algorithms yield reconstructed images with superior quality, both visually and using objective distance metrics, to that of traditional decoders that use only the transmitted transform coefficients. ...|$|E
40|$|International audienceInformed Source Separation (ISS) {{techniques}} enable {{manipulation of}} the source signals that compose an audio mixture, based on a <b>coder-decoder</b> configuration. Provided the source signals are known at the encoder, a low-bitrate side-information {{is sent to the}} decoder and permits to achieve efficient source separation. Recent research has focused on a Coding-based ISS framework, which has an advantage to encode the desired audio objects, while exploiting their mixture in an information-theoretic framework. Here, we show how the perceptual quality of the separated sources can be improved by inserting perceptual source coding techniques in this framework, achieving a continuum of optimal bitrate-perceptual distortion trade-offs...|$|E
40|$|Pipeline {{architectures}} {{are often}} considered in VLSI designs that require high throughput. The draw-backs for traditional pipelined architectures are the increased area, power, and latency required for implementation. However, with increased design effort, wave-pipelining {{can be applied}} {{as an alternative to}} a pipelined circuit to reduce the pipeline area, power, and latency while maintaining the original functionality and timing of the overall circuit. The objective of this paper is the successful application of the theories of wave-pipelining in a practical digital system. To accomplish this, the pipelined portion of an Multi-Channel Adaptive Differential Pulse Code Modulation (ADPCM) <b>Coder-Decoder</b> (CODEC) is replaced with a wave pipeline design...|$|E
40|$|Informed Source Separation (ISS) {{techniques}} enable manip-ulation of {{the source}} signals that compose an audio mix-ture, based on a <b>coder-decoder</b> configuration. Provided the source signals are known at the encoder, a low-bitrate side-information {{is sent to the}} decoder and permits to achieve efficient source separation. Recent research has focused on a Coding-based ISS framework, which has an advantage to encode the desired audio objects, while exploiting their mix-ture in an information-theoretic framework. Here, we show how the perceptual quality of the separated sources can be improved by inserting perceptual source coding techniques in this framework, achieving a continuum of optimal bitrate-perceptual distortion trade-offs. Index Terms — Informed source separation, source cod-ing, perceptual models 1...|$|E
40|$|This paper {{describes}} the INRIA Videoconferencing System (IVS), a low bandwidth tool for real-time video between workstations on the Internet using UDP datagrams and the IP multicast extension. The video <b>coder-decoder</b> (codec) is a software {{implementation of the}} UIT-T recommendation H. 261 originally developed for the Integrated Services Digital Network (ISDN). Our focus in this paper is on adapting this codec for the Internet environment. We propose a packetization scheme, an error control scheme and an output rate control scheme that adapts the image coding process based on network conditions. This work shows {{that it is possible}} to maintain videoconferences with reasonable quality across packet-switched networks without requiring special support from the network such as resource reservation or admission control...|$|E
40|$|Many users {{consider}} telemedicine {{a partial}} solution to problems of delivering {{health care to}} remote areas orareas underservedby cmi-cians. Current telemedical technol-ogy benefits from recent develop-ments such as the decreased cost and improved quality ofthe <b>coder-decoder</b> (codec) equipment used in interactive digital video systems {{and the expansion of}} fiber-optic cable nettcorks. The authors out-line some pioneering telemedicine programs of the 1960 s and 1970 s anddescribe two recently activated systems in Texas. One network, serving the western two-fifths of the state, links faculty members f rom four campuses of Texas Tech University Health Sciences Center with almost 40 rural communities. The other connects the state hospi-tal and three other facilities in Austin with four health care sites in the town ofGiddings, 65 miles Dr. Preston is clinical associate professor of psychiatry at Baylo...|$|E
30|$|The encoder {{uses the}} pooling layer {{to reduce the}} spatial {{dimension}} of the feature map. The decoder gradually restores the target details and the corresponding spatial dimension through the deconvolution layer. For example, Noh et al. [16] use deconvolution to up-sample low-resolution feature responses. U-Net [17] connects the encoder features to the corresponding decoder though the jump layer connection to help the decoder recover the target details better. SegNet [18] recorded the pooling index of the feature map in the encoder. The decoder extracted the information and mapped it to its original location. LRR [19] uses Laplace Pyramid to reconstruct the network and multiplicative gating integrates effectively the underlying position information and higher-level semantic information. Recently, RefineNet [20] demonstrated {{the validity of the}} semantic segmentation problem based on the <b>coder-decoder</b> structure, which has also been practiced in the target detection.|$|E
40|$|A wavelet-based <b>coder-decoder</b> (codec) {{structure}} is defined for baseband waveform coding. Numerical results for bandwidth efficiency are given, and {{a comparison between}} several different wavelets is presented. Moreover, it is shown that wavelets obey the Nyquist pulse shaping condition and provide a unified framework for analog pulse shaping concepts of communications. 1 Introduction Wavelets have recently become popular in many different scientific fields, including signal processing. A wavelet is a signal or a waveform having desirable characteristics, such as localization in time and frequency, and orthogonality across scale and translation [1],[2]. Because of these appealing properties, wavelets appear to be promising waveforms in communications. Motivation {{for the use of}} wavelets for waveform coding {{stems from the fact that}} the two ideal waveforms often used to benchmark analog pulse shaping performance, namely the time-limited rectangular pulse and the band-limited sinc pulse, ar [...] ...|$|E
40|$|International audienceIn this paper, {{we propose}} a new watermark-assisted method for the {{analysis}} of audio source signals present in a mixture. This work is motivated by the issue of quality-constrained source parameters estimation in under-determined mixtures where the blind approach is not efficient. Our method uses a specific <b>coder-decoder</b> configuration where the separated source signals are assumed to be known at the coder. The necessary information required by a classic blind estimator to reach a target quality is imperceptibly embedded into the mixture signal. At the decoder, where the isolated source signals are unknown, the analysis process is assisted by the extra information embedded into the mixture signal. Thus, this technique aims at opening new perspectives for quality-based audio applications like active listening, sound feature extraction, or high quality audio effects with a minimal amount of side information...|$|E
40|$|Output {{feedback}} controlled synchronization {{problems for}} a class of nonlinear unstable systems under information constraints imposed by limited capacity of the communication channel are analyzed. A binary time-varying <b>coder-decoder</b> scheme is described and a theoretical analysis for multi-dimensional master-slave systems represented in Lurie form (linear part plus nonlinearity depending only on measurable outputs) is provided. An output feedback control law is proposed based on the Passification Theorem. It is shown that the synchronization error exponentially tends to zero for sufficiantly high transmission rate (channel capacity). The results obtained for synchronization problem can be extended to tracking problems in a straightforward manner, if the reference signal is described by an {external} ({exogenious}) state space model. The results are applied to controlled synchronization of two chaotic Chua systems via a communication channel with limited capacity. Comment: 8 pages, 2 figure...|$|E
40|$|In {{the future}} {{multimedia}} {{technology will be}} able to provide video frame rates equal to or better than 30 frames-per-second (FPS). Until that time the hearing impaired community will be using band-limited communication systems over un-shielded twisted pair copper wiring. As a result multimedia communication systems will use a <b>coder-decoder</b> (CODEC) to compress the video and audio signals for transmission. For these systems to be usable by the hearing impaired community, the algorithms within the CODEC have to be designed to account for the perceptual boundaries of the hearing impaired. In this paper we investigate the perceptual boundaries of speechreading and multimedia technology, which are the constraints that effect speechreading performance. We analyze and draw conclusions on the relationship between viseme groupings, accuracy of viseme recognition, and presentation rate. These results are critical in the design of multimedia systems for the hearing impaired...|$|E
40|$|This report {{describes}} a low-bandwidth videoconferencing application on the Internet using the IP multicast extensions and the User Datagram Protocol (UDP) transport protocol. The video <b>coder-decoder</b> is a software {{implementation of the}} CCITT recommendation H. 261 originally developped for the Integrated Services Digital Network (ISDN). Until now, H. 261 codecs have been implemented in hardware. We find that the mean output rate of the coder is less than 30 kb/s, thus making videoconferencing applications possible over low-speed networks such as the Internet. After {{a brief overview of}} the different data compression techniques and a description of the recommendation H. 261, we describe in more details IVS, our videoconferencing application which is freely available in the public domain. Keywords codec, CCITT recommendation H. 261, data compression, Internet, IP multicast extensions, UDP, videoconferencing. 1 Introduction Videoconference applications have often been described as requiring [...] ...|$|E
40|$|This paper compares 2 -D and 3 -D object {{modeling}} {{in terms}} of their capabilities and performance (peaksignal -to-noise-ratio and visual image quality) for very low bitrate video coding. We show that 2 -D objectbased coding with affine/perspective transformations and triangular mesh models can simulate almost all capabilities of 3 -D object-based approaches using wireframe models {{at a fraction of the}} computational cost. Furthermore, experiments indicate that a 2 -D mesh-based <b>coder-decoder</b> performs favorably compared to the new H. 263 standard {{in terms of}} visual quality. 1 Introduction Establishment of audio-visual conversational services over very low bitrate channels, such as the public switched telephone network (PSTN) and wireless media, is an important emerging application for the telecommunications industry. As a short-term solution to the world-wide standardization of very low bitrate video (less than 64 kbits/s) compression/decompression, the International Telecommunications Union (I [...] ...|$|E
40|$|Tone Mapping Operators (TMOs) {{transform}} High Dynamic Range (HDR) contents {{to address}} Low Dynamic Range (LDR) displays. However, {{before reaching the}} end-user, these contents are usually compressed using a codec (<b>coder-decoder)</b> for broadcasting or storage purposes. Achieving the best trade-off between rendering and compression efficiency is of prime importance. Any TMO includes a rounding quan-tization to convert floating point values to integer ones. In this work, we propose to modify this quantization to increase the compression efficiency of the tone mapped content. By using a motion compensation, our technique preserves the render-ing intent of the TMO while maximizing the correlations be-tween successive frames. Experimental results show that we can save up to 12 % of the total bit-rate {{as well as an}} average bit-rate reduction of 8. 5 % for all the test sequences. We show that our technique can be applied to other applications such as denoising...|$|E
40|$|Abstract: This article proposes, a {{reconfigurable}} FEC {{system based}} on Reed-Solomon codec for DVB and WiMax networks. The proposed architecture implements various programmable primitive polynomials. A lot of VLSI implementations have been described in literature. This paper introduces a highly parametrical RS-coder-decoder on FPGAs. The implementation, written in a hardware description language (HDL), {{is based on an}} Berlekamp massey, Chain and Formey Algorithms. We have defined an advanced RS encoder-decoder architecture based on parameterization approach which is a key solution for software defined radio (SDR) systems. Our parameterization approach is used in order to implement on FPGA a generic RS <b>coder-decoder</b> for DVB and WiMax networks. IEEE Std. 802. 16 specifies that the codec performs a variable number of check symbols in a codeword (ranges from 0 to 32, inclusive). The value of check symbols are specified for each burst profile by the MAC layer according to cross layer concept...|$|E
40|$|International audienceIn this paper, {{we address}} the issue of under-determined source {{separation}} of non-stationary audio sources from a stereo (i. e. 2 -channel) linear instantaneous mixture. This problem is addressed with a specific <b>coder-decoder</b> configuration. At the coder, source signals are assumed to be available before the mixing is processed. A time-frequency (TF) analysis of each source enables to select the one or two predominant sources (among I> 2) in each TF region, and a corresponding source(s) index code is imperceptibly embedded into the mix signals using a watermarking technique. At the decoder level, where the original sources signals are unknown, the extraction of the watermark enables to locally reduce the under-determined configuration to an (over) determined configuration. Sources signals can then be estimated using a classical (over) determined separation technique. Thereby several instruments or voice signals can be separated from stereo mixtures, enabling separate manipulation of the source signals during restitution (i. e. remastering) ...|$|E
40|$|International audienceIn this paper, {{we address}} the issue of underdeter- mined source {{separation}} of I non-stationary audio sources from a J-channel linear instantaneous mixture (J < I). This problem is addressed with a specific <b>coder-decoder</b> configuration. At the coder, source signals are assumed to be available before the mixing is processed. A time-frequency (TF) joint analysis of each source signal and mixture signal enables to select the subset of sources (among I) leading to the best separation results in each TF region. A corresponding source(s) index code is imperceptibly embedded into the mix signal using a watermarking technique. At the decoder, where the original source signals are unknown, the extraction of the watermark enables to invert the mixture in each TF region to recover the source signals. With such informed approach, it is shown that 5 instruments and singing voice signals can be efficiently separated from 2 -channel stereo mixtures, with a quality that significantly overcomes the quality obtained by a semi-blind reference method and enables separate manipulation of the source signals during stereo music restitution (i. e. remixing) ...|$|E
40|$|Codecs (<b>coder-decoder)</b> perform {{encoding}} and decoding on video, speech, {{music and}} text. They scale, reorder, decompose and reconstitute perceptible {{images and sounds}} {{so that they can}} get through information networks and electronic media. Codecs are intimately associated with changes in the “spectral density, ” the distribution of energy, of sound and image in electronic media. Codecs pose numerous analytical problems for software scholars. They are mathematically deep. Coming to grips with them may entail lengthy immersion in technical details. Although they are responsible for displaying many moving images on screens today, codecs themselves often hide in hardware and lower-level code. They come to light occasionally, usually {{in the form of an}} error message saying that something is missing: the right codec has not been installed. Despite or perhaps because of their convoluted obscurity, they catalyze new relations between people, things, spaces and times in events and forms. Patent pools and codec floods Video codecs such as MPEG- 1, MPEG- 4, H. 261. H. 263, the important H. 264, theora, dirac...|$|E
40|$|Abstract—In this paper, {{we address}} the issue of underdetermined source {{separation}} of I non-stationary audio sources from a J-channel linear instantaneous mixture (J < I). This problem is addressed with a specific <b>coder-decoder</b> configuration. At the coder, source signals are assumed to be available before the mixing is processed. A time-frequency (TF) joint analysis of each source signal and mixture signal enables to select the subset of sources (among I) leading to the best separation results in each TF region. A corresponding source(s) index code is imperceptibly embedded into the mix signal using a watermarking technique. At the decoder, where the original source signals are unknown, the extraction of the watermark enables to invert the mixture in each TF region to recover the source signals. With such informed approach, it is shown that 5 instruments and singing voice signals can be efficiently separated from 2 -channel stereo mixtures, with a quality that significantly overcomes the quality obtained by a semi-blind reference method and enables separate manipulation of the source signals during stereo music restitution (i. e. remixing). Index Terms—under-determined source separation, watermarking, audio processing, remixing. I...|$|E
40|$|Briefly speaking, {{there are}} two popular Voice over Internet Protocol (VoIP) {{standard}}s, H. 323 and Session Initiation Protocol (SIP). The first standard was designed by ITU and has become {{the basis for the}} widespread implementation of VoIP systems although it was not specifically designed for it. The second standard, SIP, was proposed by IETF and it is designed to connect, communicate and exchange data with the internet applications. In order to deliver voice conversation through packet-switching networks, codecs (<b>coder-decoder)</b> should be implemented to compress and later decompress those packets. In this paper, some of compression algorithms will be compared and analyzed based on its performances in SIP based VoIP system. The codecs that was used in this experiment are SJ Lab GSM 6. 10, SJ Lab iLBC- 30 ms, SJ Lab iLBC- 20 ms, Microsoft CCITT G. 711 A-law and Microsoft CCITT G. 711 u-law. These codecs are tested in terms of its ability to deal with jitter buffer variations. The result shows that SJ Lab iLBC- 20 ms gives the best performance in terms of jitter buffer variation on LAN environment while SJ Lab GSM 6. 10 shows the highest performance on wireless networks testing...|$|E
40|$|The {{class of}} {{controlled}} synchronization systems under information constraints imposed by limited information {{capacity of the}} coupling channel is analyzed. It is shown that the framework proposed in A. L. Fradkov, B. Andrievsky, R. J. Evans, Physical Review E 73, 066209 (2006) is suitable not only for observer-based synchronization but also for controlled master-slave synchronization via communication channel with limited information capacity. A simple first order <b>coder-decoder</b> scheme is proposed and a theoretical analysis for multi-dimensional master-slave systems represented in the Lurie form (linear part plus nonlinearity depending only on measurable outputs) is provided. An output feedback control law is proposed based on the Passification theorem. It is shown that the upper bound of the limit synchronization error {{is proportional to the}} upper bound of the transmission error. As a consequence, both upper and lower bounds of limit synchronization error are proportional to the maximum rate of the coupling signal and inversely proportional to the information transmission rate (channel capacity). The results are applied to controlled synchronization of two chaotic Chua systems coupled via a controller and a channel with limited capacity. Comment: 7 pages, 8 figure...|$|E
40|$|The {{hardware}} {{implementation of}} a proposed high dimensional discrete torus knot code was successfully realized on an ASIC chip. The code has been worked {{on for more than}} a decade since then at Aichi Prefectural University and Nagoya Institutes of Technology, both in Nagoya, Japan. The hardware operation showed the ability to correct the errors about five to ten times the burst length, compared to the conventional codes, as expected from the code configuration and theory. The result in random error correction was also excellent, especially at a severely degraded error rate range of one hundredth to one tenth, and also for high grade characteristic exceeding 10 - 6. The operation was quite stable at the worst bit error rate and realized a high speed up to 50 Mbps, since the <b>coder-decoder</b> configuration consisted merely of an assemblage of parity check code and hardware circuitry with no critical loop path. The hardware architecture has a unique configuration and is suitable for large scale ASIC design. The developed code can be utilized for wider applications such as mobile computing and qualified digital communications, since the code will be expected to work well in both degraded and high grade channel situations...|$|E
40|$|International audienceIn this paper, {{the issue}} of audio source {{separation}} from a single channel is addressed, i. e. the estimation of several source signals from a single observation of their mixture. This challenging problem is tackled with a specific two levels <b>coder-decoder</b> configuration. At the coder, source signals {{are assumed to be}} available before the mix is processed. Each source signal is characterized by a set of parameters that provide additional information useful for separation. We propose an original method using a watermarking technique to imperceptibly embed this information about the source signals into the mix signal. At the decoder, the watermark is extracted from the mix signal to enable an end-user who has no access to the original sources to separate these signals from their mixture. Hence, we call this separation process Informed Source Separation (ISS). Thereby, several instruments or voice signals can be segregated from a single piece of music to enable post-mixing processing such as volume control, echo addition, spatialization, or timbre transformation. Good performances are obtained for the separation of up to four source signals, from mixtures of speech or music signals. Promising results open up new perspectives in both under-determined source separation and audio watermarking domains...|$|E
40|$|International audienceAudio {{streaming}} over wireless {{local area}} network (WLAN) has an ability to transmit audio data ranging from short messages to music station. A popular application of this is in setting up a community radio, which is in demand, especially in developing countries where frequency spectrum management is still problematic. Audio streaming has to assure high quality of content reception without any packet loss, if possible. One such suitable <b>coder-decoder</b> (CODEC) is the Advanced Audio Coding (AAC). Packetization interval for audio streaming is usually set at a default value of 20 ms. This work develops an AAC Audio Streaming over WLAN 802. 11 g. It studies the effect of different packetization intervals has on the maximum number of possible connections (i. e. receivers) without any packet loss. The simulated environment comprises of 1 access point with 300 m 2. Connecting media server to access point via wireless link is adopted for its mobility and ease of deployment in rural area. The range of packetization intervals under this study is from 20 ms. to 200 ms. The study reveals that the commonly use default value of 20 ms. can manage up to 13 connections. Under the scenario of this study, {{it was found that the}} best packetization interval is 70 ms. with the maximum of 33 connections. The study also reveals that packet delay is not an issue in this approach as all are under the acceptable standard of 400 ms...|$|E
40|$|Codecs perform {{encoding}} and decoding on a {{data stream}} or signal, {{usually in the}} interest of compressing data. They scale, reorder, decompose and reconstitute perceptible images and sounds in information networks and electronic media. They are intimately associated with changes in the “spectral density, ” the distribution of energy, transported by sound and image in electronic media. Software such as codecs pose serious analytical problems. On the one hand, they are monstrously complex. Coming to grips with them may entail tedious excursions into technical details. They tax one’s affection for complexity. On the other hand, although they are responsible for displaying most images on screens today, codecs themselves are relatively concealed entities. Only occasionally, when they come to light, it is usually {{in the form of an}} error message that something has broken down: the right codec has not been installed. Despite or perhaps because of their convoluted banal obscurity, they are critically important catalysts of people, things, spaces and times in events and forms (Rabinow, 1999, 180). Codecs: sensation and distribution MPEG- 2 (a. k. a. H. 262) designates a well-established set of encoding and decoding procedures for digital audio and video formalised as a standard (ISO/IEC 13818). The standard in fact defines a ‘transport ’ system rather than just a codec. Standards, as sociological work has argued, mix physical entities with conventional arrangements (Bowker and Star, 1999, 39). Software implementations of such standards are known as codecs (<b>coder-decoder).</b> Video codecs for different standards (MPEG- 1, MPEG- 4, H. 261. H. 263, the important H. 264, theora, dirac, DivX, MJPEG, WMV, RealVideo, etc) are strewn across the milieu of sound and image associated with networked electronic media. Because codecs often borrow techniques and strategies of processing sound and image, their genealogy is tangled. Leaving aside the tangle of relations between different codecs and video technologies, even one codec, the well-established and uncontentious MPEG- 2 coding standard, is extraordinarily complex. Algorithmically it combines several distinct compression techniques (converting signals from time domain to frequency domain usin...|$|E
40|$|Telecommunication {{networks}} become major {{parts in}} modern complex control systems recently. They provide many advantages over conventional point-to-point connections, {{such as the}} simplification on installation and maintenance with comparatively low cost and the nature requirement of wireless communication in remote control systems. In practice, limited resource networks are shared by multiple controllers, sensors and actuators, and they may need to serve some other information unrelated to control purpose. Consequently, the control system design in networked control systems should be revised by taking communication constraints, for example, finite precision data, time delay and noise in transmission, into account. This thesis studies the robust control and state estimation of uncertain systems, when feedback information is sent via limited capacity communication channels. It focuses {{on the problem of}} finite precision data due to the communication constraints. The proposed schemes are based on the robust set-valued state estimation and the optimal control techniques. A state estimation problem of linear uncertain system is studied first. In this problem, we propose an algorithm called <b>coder-decoder</b> for uncertain systems. The coder encodes the observed output into a finite-length codeword and sends it to the decoder that generates the estimated state based on the received codeword. As an illustration, we apply the results in state estimation problem to a precision missile guidance problem using sensor fusion. In this problem, the information obtained from remote sensors is transmitted through limited capacity communication networks to the guided missile. Next, we study a stabilization problem of linear uncertain systems with state feedback. In this problem, the coder-controller scheme is developed to asymptotically stabilize the uncertain systems via limited capacity communication channels. The coder encodes the full state variable into a finite-length codeword and sends it to the controller that drives the system state to the origin. To achieve the asymptotic stability, we use a dynamic quantizer so that quantization noise converges to zero. The results in both state estimation and stabilization problems can handle the problem of finite data rate communication networks in control systems...|$|E
40|$|Today, modern System-on-a-Chip (SoC) {{systems have}} grown rapidly {{due to the}} {{increased}} processing power, while maintaining {{the size of the}} hardware circuit. The number of transistors on a chip continues to increase, but current SoC designs {{may not be able to}} exploit the potential performance, especially with energy consumption and chip area becoming two major concerns. Traditional SoC designs usually separate software and hardware. Thus, the process of improving the system performance is a complicated task for both software and hardware designers. The aim of this research is to develop hardware acceleration workflow for software applications. Thus, system performance can be improved with constraints of energy consumption and on-chip resource costs. The characteristics of software applications can be identified by using profiling tools. Hardware acceleration can have significant performance improvement for highly mathematical calculations or repeated functions. The performance of SoC systems can then be improved, if the hardware acceleration method is used to accelerate the element that incurs performance overheads. The concepts mentioned in this study can be easily applied to a variety of sophisticated software applications. ^ The contributions of SoC-based hardware acceleration in the hardware-software co-design platform include the following: (1) Software profiling methods are applied to H. 264 <b>Coder-Decoder</b> (CODEC) core. The hotspot function of aimed application is identified by using critical attributes such as cycles per loop, loop rounds, etc. (2) Hardware acceleration method based on Field-Programmable Gate Array (FPGA) is used to resolve system bottlenecks and improve system performance. The identified hotspot function is then converted to a hardware accelerator and mapped onto the hardware platform. Two types of hardware acceleration methods – central bus design and co-processor design, are implemented for comparison in the proposed architecture. (3) System specifications, such as performance, energy consumption, and resource costs, are measured and analyzed. The trade-off of these three factors is compared and balanced. Different hardware accelerators are implemented and evaluated based on system requirements. 4) The system verification platform is designed based on Integrated Circuit (IC) workflow. Hardware optimization techniques are used for higher performance and less resource costs. ^ Experimental results show that the proposed hardware acceleration workflow for software applications is an efficient technique. The system can reach 2. 8 X performance improvements and save 31. 84 % energy consumption by applying the Bus-IP design. The Co-processor design can have 7. 9 X performance and save 75. 85 % energy consumption. ...|$|E

