29|18|Public
50|$|Zagdanski’s first novel, Les intérêts du temps, {{published}} in 1996 by Gallimard {{in the series}} L’infini, explores the global ravages of <b>computerized</b> <b>society</b> through the itinerary of a learned young man named “Martin Heidegger”, who is interested in ancient Greek literature, and who is confronted with the grotesque behind the scenes machinations of a culture magazine. The novel’s narrator interweaves dubious and nihilist characters in the guises of the period he describes.|$|E
50|$|Humanistic {{informatics}} departments {{were generally}} {{started in the}} 1990s when universities rarely taught humanities-based approaches to the rapidly developing <b>computerized</b> <b>society.</b> For this reason, the field was quite broadly defined, and included courses in humanities computing, basic introductions to how computers work, historical developments of technology, technology and learning, digital art and literature and digital culture. Today several departments have declared more specialized areas of research, such as digital arts and culture at the University of Bergen, and socio-cultural communication with and without technology at the University of Aalborg.|$|E
40|$|In article the {{problems}} of mutual adapting of the humans and computer environment are reviewed. Features of image-intuitive and physical-mathematical modes of perception and thinking are investigated. The problems of choice of means and methods of the differential education the <b>computerized</b> <b>society</b> are considered...|$|E
50|$|A {{man named}} Jenkins {{is put on}} trial after accidentally {{damaging}} a computer system that potentially could have a disastrous effect on the totally <b>computerized</b> underground <b>society</b> in which he lives. The trial, which is carried out by computers programmed with prosecution and defence arguments, finds Jenkins guilty of equipment damage, a major crime by the society's laws. He is sentenced to permanent exile, a punishment considered harsher than execution.|$|R
40|$|This essay {{warns of}} eroding {{accountability}} in <b>computerized</b> <b>societies.</b> It argues that assumptions about computing and features of {{situations in which}} computers are produced create barriers to accountability. Drawing on philosophical analyses of moral blame and responsibility, four barriers are identified: (1) the problem of many hands, (2) the problem of bugs, (3) blaming the computer, and (4) software ownership without liability. The paper concludes with {{ideas on how to}} reverse this trend. If a builder has built a house for a man and has not made his work sound, and the house which he has built has fallen down and so caused the death of the householder, that builder shall be put to death. If it destroys property, he shall replace anything that it has destroyed; and, because he has not made sound the house which he has built and it has fallen down, he shall rebuild the house which has fallen down from his own property. If a builder has built a house for a man and does not make his wor [...] ...|$|R
5000|$|The novel {{plays with}} a number of {{literary}} themes and elements, but can be broadly categorized as [...] "postcyberpunk", i.e. science fiction dealing with the consequences of a drastically <b>computerized</b> and networked <b>society,</b> however with much more direct experience with IT and without thematic limitations of first-wave cyberpunk.|$|R
40|$|The paper {{analyzes}} the approaches of scientists {{to determine the}} structure of the teacher's media competence, are sanctified by the methodological aspects of media education students in the study of natural sciences, the proposed mechanism of diagnosing readiness for the implementation of the teacher to prepare young people for life in a <b>computerized</b> <b>society...</b>|$|E
40|$|In {{this paper}} are {{analyzed}} the major transformations that, {{in the recent}} past, have changed the technical information, developed by companies as promotional practical tool for the construction. Technical information, indeed, has been adapted to the increasingly complex process. Reviewing these changes {{is an opportunity to}} reflect on the relevant evolutionary processes that have affected the building sector practice in an advanced context with new market rules in a global and <b>computerized</b> <b>society...</b>|$|E
40|$|ABSTRACT: In a <b>computerized</b> <b>society,</b> {{the volume}} of data grows unexpectedly, making {{assessing}} their processing time a very difficult task. A priority has become the processing of data in useful information and knowledge. Thus {{we can say that}} data mining is a result of technological developments. The interpretation of spatial data has constituted a subject of research over time, reaching now a large variety of instruments and software products for representation and interpretation. What we need to understand beyond the facilities offered by one system or another, proprietary or open source solutions, is how they work and interact with spatial data...|$|E
40|$|The {{vocation}} and {{the appropriate}} specialized training {{are the source of}} competence, love and passion for the work that they perform. Workers from libraries and other information structures can act convincingly and efficiently than being in possession of a thorough knowledge of librarianship - able to put in permanent relationship miscellaneous information that guides and stimulates the reader approach. Progressive <b>computerized,</b> the <b>society</b> had qualified intermediaries needs exchange of information, specialists in guiding users on all types of sources, able to synthesize an array of information according to expressed demand, build networks and systems...|$|R
40|$|Thirty years ago, the French {{philosopher and}} {{literary}} theorist Jean-François Lyotard published a pre-scient report on knowledge called The Postmodern Condition. Originally {{commissioned by the}} Conseil des Universitiés {{of the government of}} Quebec, the report was an investigation of the status of knowledge in <b>computerized</b> <b>societies</b> (3). His working hypothesis was that the nature of knowledgehow we know, what we know, how knowledge is communicated, what knowledge is communicated, and, finally, who we as knowers arehad changed in light of the new technological, social, and economic transformations that have ushered in the post-industrial age, what he calls, in short, postmodernism. Much more than just a pe-riodizing term, postmodernism, for Lyotard, bespeaks a new cultural-economic reality as well as a condition in which grand narratives or meta-narratives no longer hold sway: the progress of science, the liberation of humanity, the spread of Enlightenment and rationality, and so forth are meta-narratives that have lost their cogency. This itself is not an original observation: after all, Nietzsche, Benjamin, Adorno, Horkheimer, Foucault, and others have variously shown where the fully enlightened world ends up. What sets Lyotard apart is his focus on how knowledge has been transformed into many small (and even competing and contradictory) narratives and how scientific knowledge in particular has become transformed into bits of information with the rise of cybernetics, informatics, information storage and databanks, and telematics...|$|R
40|$|<b>Computerized</b> modern <b>societies</b> {{are highly}} fragile to {{software}} bugs. Traditional testing methods hardly scale up for large safety critical systems {{as found in}} avionics, automotive, healthcare, e-commerce and security industry. As a viable alternative, static analysis consists in determining and verifying statically dynamic properties of programs. This is completely automatic (since programs are not actually executed) and covers all possible cases (as opposed to testing). This approach has had significant success stories and its industrialization recently started. Since the program total verification problem is undecidable, the key idea is that interpretation. The scope of application of abstract interpretation ranges from the theoretical design of hierarchies of the semantics of programming languages to the practical design of generic program static analyzers...|$|R
40|$|The {{learning}} process {{is subject to}} continuous changes due to the changing needs of users. These changes are most often accompanied by new technologies which are constantly appearing in our modern <b>computerized</b> <b>society.</b> The Internet has allowed the optimization of the {{learning process}} through supported learning platforms, in addition to this, the dissemination of mobile devices such as P. D. A. ´s allows for a new stage in this process, mLearning. This {{is based on the}} ability of the user to access their course information at any time and place without the restriction of needing to have a computer in their possession, this along with other characteristics will determine the direction of ubiquitous learning...|$|E
40|$|Although it was {{predicted}} that bank branches would quickly become obsolete in a <b>computerized</b> <b>society,</b> the reality is that many full-service branches are not closing but rather evolving to meet changing needs. The role of the branch manager is crucial, and is also changing. In particular, managers are expected to take a lead in marketing activities. A questionnaire study was carried out to examine managers’ changing roles, using two samples of branch managers, one from Canada and one from Spain. Managers were asked to rate 21 function variables on their importance in bank management and in facing new market trends. Differences were found between the two samples, as were similarities: both identified managerial ability, strategic autonomy of the branch and business development through increased marketing ability, as important building blocks for the future role of branches and their managers...|$|E
40|$|Includes bibliographical references. Computers. They {{used to be}} mystical and magical in appearance. We {{were all}} amazed at their capabilities. [While] {{that was at the}} infancy of the {{computer}} [then], the students in the schools today are growing up in a <b>computerized</b> <b>society.</b> We bank through computers, we play games with computers, we make reservations through computers, and computers even connect our phone calls. There is no question that we must produce computer literate adults, and our schools have adapted their curriculum to reflect that need. One of the additions to the curriculum has been computer programming even though most do not believe that it is necessary for computer literacy. But, if computer programming is going to be taught {{we need to look at}} why we are teaching it and what our goals should be. B. S. (Bachelor of Science...|$|E
30|$|In {{the context}} of smart {{sustainable}} cities, wireless solutions are set to proliferate {{in ways that are}} hard to imagine, as ICT continues to be fast embedded and interwoven into the very fabric of current smart and sustainable cities in terms of their systems and processes in an increasingly <b>computerized</b> urban <b>society.</b> This is a future world of pervasive computing infrastructures and communication networks. Countless sensors will use various wirelessly ad-hoc and mobile networks to provide cities with all kinds of data necessary {{for a wide variety of}} applications and services. In particular, the widespread diffusion of wireless network technologies will, as a by-product of their normal operations, enable to sense, collect, and coordinate massive repositories of spatiotemporal data pertaining to urban systems, which represent city-wide proxies for all kinds of activities and operating and organizing processes.|$|R
5000|$|In 1993, in his {{capacity}} as [...] "Finance Director", he was tasked by the Chairman and by the CEO of Equity Building Society (EBS), to wind up the insolvent organisation which was losing KSh5 million (approx. US$60,000 then), annually and had, at that time, accumulated total losses of KSh33 million (approx. US$380,000 then). However the young Mwangi, aged about 31 at the time, began by motivating the 27 staff members to give better customer care to the 27,000 customers they had then. He also encouraged them to use 25% of their salaries to buy EBS shares. When things began to brighten up at EBS, the society began to sell shares to customers and to pay annual dividends in 1997. In 2000, the <b>society</b> <b>computerized</b> their operations. They were able to raise more capital and attract more customers. On 31 August 2004, Equity Building Society became Equity Bank Kenya.|$|R
40|$|With {{the ever}} {{increasing}} role of <b>computerized</b> machines in <b>society,</b> Human Computer Interaction (HCI) system {{has become an}} increasingly {{important part of our}} daily lives. HCI determines the effective utilization of the available information flow of the computing, communication, and display technologies. In recent years, there has been a tremendous interest in introducing intuitive interfaces that can recognize the user's body movements and translate them into machine commands. For the neural linkage with computers, various biomedical signals (biosignals) can be used, which can be acquired from a specialized tissue, organ, or cell system like the nervous system. Examples include Electro-Encephalogram (EEG), Electrooculogram (EOG), and Electromyogram (EMG). Such approaches are extremely valuable to physically disabled persons. Many {{attempts have been made to}} use EMG signal from gesture for developing HCI. EMG signal processing and controller work is currently proceeding in various direction including the development of continuous EMG signal classification for graphical controller, that enables the physically disabled to use word processing programs and other personal compute...|$|R
40|$|The {{usefulness}} of machine translation (MT) {{seems to have}} regained public attention in this highly <b>computerized</b> <b>society,</b> and active research projects concerning the subject are currently under way especially in the E. C., Canada and Japan. Now, some strong theoretical foundations must be established to have MT develop soundly. But, so far, excessive attention {{seems to have been}} paid to researcher's intuition and to his personal verbal experience. Much more attention should be paid to the linguistic facts and the results of theoretical considerations concerning linguistic translation. This paper is intended to contribute along these lines. First, the history of MT is briefly reviewed from the viewpoint of translation theory. A theoretical consideration of translation proposed by a French linguist G. Mounin is then summarized. Finally, a new thoretical framework is presented to aid in a sound development of MT...|$|E
40|$|Abstract. Privacy-preserving {{techniques}} are increasingly important in our highly <b>computerized</b> <b>society</b> where privacy is both precious and elusive. Affiliation-Hiding Authenticated Key Exchange (AH-AKE) protocols offer an appealing service: authenticated key agreement coupled with privacy of group memberships of protocol participants. This {{type of service}} is essential in privacy-conscious p 2 p systems, mobile ad hoc networks and social networking applications. Prior work has succeeded in constructing a number of secure and efficient AH-AKE protocols which all assume full trust in the Group Authority (GA) — the entity that sets up the group as well as registers and (optionally) revokes members. In this paper, we argue that, for many anticipated application scenarios, the trusted GA model should be relaxed to allow for certain types of malicious behavior. We examine the consequences of malicious GAs and explore the design of stronger AH-AKE protocols that withstand GA attacks. Our results demonstrate that such protocols are both feasible and practical...|$|E
40|$|Abstract: Researchers {{have been}} active in the field of {{software}} engineering measurement over more than 30 years. The software quality product is becoming increasingly important in the <b>computerized</b> <b>society.</b> Target setting in software quality function and usability deployment are essential since they are directly related to development of high quality products with high customer satisfaction. Software quality can be measured as {{the degree to which a}} particular software program complies with consumer demand regarding function and characteristics. Target setting is usually subjective in practice, which is unscientific. Therefore, this study proposes a quantity model for controlling and measuring software quality via the expert decision-making al-gorithm-based method for constructing an evaluation method can provide software in relation to users and purchasers, thus enabling administrators or decision makers to identify the most appropriate software quality. Importantly, the proposed model can provide s users and purchasers a reference material, making it highly applicable for academic and government purposes...|$|E
40|$|In our highly <b>computerized</b> and {{networked}} <b>society,</b> {{privacy of}} individuals is precious and becomes increasingly important. Problems particularly {{arise in the}} context of authentication protocols where, as a general rule, entities actively reveal their re-spective identities to each other. To encounter this issue, different privacy-preserving authentication methods have been developed in the last decades. The list of these techniques comprises, apart from identity escrow, ring authentication, hidden and anonymous credentials, and several others, the concept of affiliation-hiding authen-tication (AHA). Such protocols offer the appealing and seemingly contradictory service to enable users to authenticate each other as members of a certain group without revealing their affiliation to group outsiders. In AHA protocols (also known as Secret Handshakes), users become group mem-bers by registering with group authorities (GAs) and obtaining individual member-ship credentials. Group members then use their credentials to privately authenticate each other, optionally also establishing a secure session key. The pivotal privac...|$|R
40|$|Abstract. Among {{the plethora}} of privacy-friendly {{authentication}} techniques, affiliationhiding (AH) protocols are valuable {{for their ability to}} hide not only identities of communicating users behind their affiliations (memberships to groups), but also these affiliations from non-members. These qualities become increasingly important in our highly <b>computerized</b> user-centric information <b>society,</b> where privacy is an elusive good. Only little work on practical aspects of AH schemes, pursuing optimized implementations and deployment, has been done so far, and the main question a practitioner might ask — whether affiliation-hiding schemes are truly practical today — remained widely unanswered. Improving upon recent advances in the area of AH protocols, in particular on pioneering results in the multi-affiliation setting, we can give an affirmative answer to this question. To this end, we propose numerous algorithmic optimizations to a recent AH scheme leading to a remarkable performance gain. Our results are demonstrated not only at theoretical level, but we also offer implementations, performance measurements, and comparisons. At the same time, our improvements advance the area of efficient polynomial interpolation in finite fields, which is one of our building blocks...|$|R
40|$|This article {{analyzes}} public registry, {{the laws}} {{about it and}} the several computational concepts applied to them. We analyze the Brazilian Public Key Infrastructure - ICP-Brazil, the institution by the provisional measure 2. 200 of 08 / 24 / 2001, the structure, the critical to this model and the current applications with such certificates in this structure as well. Finally, there is a comparison between the services and expertise provided by the ICP-Brazil and by the notary service, in a <b>computerized</b> and connected <b>society,</b> called Information Society. We present the common services and the others, {{as well as new}} services to be provided by the digital notary office. The theme has full relevance in a world with multiples applications that appear growing every day based in digital certification and also with conected notarial databases. At the end, we discussed the roles to be performed by each of them, the digital notary and ICP- Brazil, and the care to be taken for the scanning of the information from public records. Analisam-se legislatração, estrutura e competência dos cartórios e da ICP-Brasil, abordando-se temas relativos à registros públicos, conceitos de informática, a exemplo de assinatura e certificação digital, enfantizando-se o cuidado que se deve ter na conversão da forma e disponibilidade das informações submetivas à registro público, na Sociedade da Informação...|$|R
40|$|Computer {{security}} is a topic of growing concern because, on the one hand, the power of computers continues to increase at exponential speed and all computers are virtually connected {{to each other and}} because, on the other hand, the lack of reliability of software systems may cause dramatic and unrecoverable damage to computer systems and hence to the newly emerging <b>computerized</b> <b>society.</b> Among the possible approaches to improve the current situation, expert systems have been advocated to be an important one. Typical tasks that such expert systems attempt to achieve include finding system vulnerabilities and detecting malicious behaviours of users. In this paper, we extend our intrusion detection system ASAX with a deductive subsystem that allows us to assess the security level of a software configuration on a real time basis. By coupling the two subsystems [...] - intrusion detection and configuration analysis [...] - we moreover achieve a better tuning of the intrusion detection since the syst [...] ...|$|E
40|$|Computer {{security}} is a topic of growing concern because, on the one hand, the power of computers continues to increase at exponential speed and all computers are virtually connected {{to each other and}} because, on the other hand, the lack of reliability of software systems may cause dramatic and unrecoverable damage to computer systems and hence to the newly emerging <b>computerized</b> <b>society.</b> Among the possible approaches to improve the current situation, expert systems have been advocated to be an important one. Typical tasks that such expert systems can achieve include evaluating the security level of a software configuration and detecting malicious or incorrect behaviors of users. Logic programming provides a powerful formalism for knowledge representation and deductive reasoning and is therefore a good choice to build such expert systems. However general implementations of logic programming (e. g., Prolog) can be too complex and too inefficient to be used in a security context, where all [...] ...|$|E
40|$|Researchers {{have been}} active in the field of {{software}} engineering measurement over more than 30 years. The software quality product is becoming increasingly important in the <b>computerized</b> <b>society.</b> Target setting in software quality function and usability deployment are essential since they are directly related to development of high quality products with high customer satisfaction. Software quality can be measured as {{the degree to which a}} particular software program complies with consumer demand regarding function and characteristics. Target setting is usually subjective in practice, which is unscientific. Therefore, this study proposes a quantity model for controlling and measuring software quality via the expert decision-making algorithm-based method for constructing an evaluation method can provide software in relation to users and purchasers, thus enabling administrators or decision makers to identify the most appropriate software quality. Importantly, the proposed model can provide s users and purchasers a reference material, making it highly applicable for academic and government purposes...|$|E
50|$|In {{our modern}} <b>society,</b> <b>computerized</b> or {{digital control systems}} {{have been used to}} {{reliably}} automate many of the industrial operations that we take for granted, from the power plant to the automobiles we drive. However, the complexity of these systems and how the designers integrate them, the roles and responsibilities of the humans that interact with the systems, and the cyber security of these highly networked systems has led to a new paradigm in research philosophy for next generation control systems. Resilient Control Systems consider all of these elements and those disciplines that contribute to a more effective design, such as cognitive psychology, computer science, and control engineering to develop interdisciplinary solutions. These solutions consider such things such as how to tailor the control system operating displays to best enable the user to make an accurate and reproducible response, how to design in cyber security protections such that the system defends itself from attack by changing its behaviors, and how to better integrate widely distributed computer control systems to prevent cascading failures that result in disruptions to critical industrial operations. In the context of cyber-physical systems, resilient control systems are an aspect that focuses on the unique interdependencies of a control system, as compared to information technology computer systems and networks, due to its importance in operating our critical industrial operations.|$|R
40|$|We {{live in a}} <b>computerized</b> and {{networked}} <b>society</b> {{where many}} of our actions leave a digital trace and affect other people’s actions. This has lead {{to the emergence of}} a new data-driven research field: mathematical methods of computer science, statistical physics and sociometry provide insights on a wide range of disciplines ranging from social science to human mobility. A recent important discovery is that search engine traffic (i. e., the number of requests submitted by users to search engines on the www) can be used to track and, in some cases, to anticipate the dynamics of social phenomena. Successful examples include unemployment levels, car and home sales, and epidemics spreading. Few recent works applied this approach to stock prices and market sentiment. However, it remains unclear if trends in financial markets can be anticipated by the collective wisdom of on-line users on the web. Here we show that daily trading volumes of stocks traded in NASDAQ- 100 are correlated with daily volumes of queries related to the same stocks. In particular, query volumes anticipate in many cases peaks of trading by one day or more. Our analysis is carried out on a unique dataset of queries, submitted to an important web search engine, which enable us to investigate also the user behavior. We show that the query volume dynamics emerges from the collective but seemingly uncoordinated activity of many users. These findings contribute to th...|$|R
40|$|Machine-learning {{algorithms}} are transforming large {{segments of}} the economy, underlying everything from product marketing by online retailers to personalized search engines, and from advanced medical imaging to the software in self-driving cars. As machine learning’s use has expanded across all facets of society, anxiety has emerged about the intrusion of algorithmic machines into facets of life previously dependent on human judgment. Alarm bells sounding over the diffusion of artificial intelligence throughout the private sector only portend greater anxiety about digital robots replacing humans in the governmental sphere. A few administrative agencies have already begun to adopt this technology, while others have the clear potential in the near-term to use algorithms to shape official decisions over both rulemaking and adjudication. It is no longer fanciful to envision a future in which government agencies could effectively make law by robot, a prospect that understandably conjures up dystopian images of individuals surrendering their liberty to the control of <b>computerized</b> overlords. Should <b>society</b> be alarmed by governmental use of machine learning applications? We examine this question by considering whether the use of robotic decision tools by government agencies can pass muster under core, time-honored doctrines of administrative and constitutional law. At first glance, the idea of algorithmic regulation might appear to offend one or more traditional doctrines, such as the nondelegation doctrine, procedural due process, equal protection, or principles of reason-giving and transparency. We conclude, however, that when machine-learning technology is properly understood, its use by government agencies can comfortably fit within these conventional legal parameters. We recognize, of course, that the legality of regulation by robot is only one criterion by which its use should be assessed. Obviously, agencies should not apply algorithms cavalierly, even if doing so might not run afoul of the law, and in some cases, safeguards may be needed for machine learning to satisfy broader, good-governance aspirations. Yet in contrast with the emerging alarmism, we resist any categorical dismissal of a future administrative state in which key decisions are guided by, and even at times made by, algorithmic automation. Instead, we urge that governmental reliance on machine learning should be approached with measured optimism over the potential benefits such technology can offer society by making government smarter and its decisions more efficient and just...|$|R
40|$|Part 1 : Policies and PerformanceInternational audienceIn today’s ever <b>computerized</b> <b>society,</b> Cloud Data Centers {{are packed}} with {{numerous}} online services to promptly respond to users and provide services on demand. In such complex environments, guaranteeing throughput of Virtual Machines (VMs) is crucial to minimize performance degradation for all applications. vmBBThrPred, our novel approach in this work, is an application-oblivious approach to predict performance of virtualized applications based on only basic Hypervisor level metrics. vmBBThrPred is different from other approaches in the literature that usually either inject monitoring codes to VMs or use peripheral devices to directly report their actual throughput. vmBBThrPred, instead, uses sensitivity values of VMs to cloud resources (CPU, Mem, and Disk) to predict their throughput under various working scenarios (free or under contention); sensitivity values are calculated by vmBBProfiler that also uses only Hypervisor level metrics. We {{used a variety of}} resource intensive benchmarks to gauge efficiency of our approach in our VMware-vSphere based private cloud. Results proved accuracy of 95  % (on average) for predicting throughput of 12 benchmarks over 1200  h of operation...|$|E
40|$|Many {{teachers}} {{and people in}} educational institutions consider it necessary prepare children for living in a <b>computerized</b> <b>society.</b> The Internet offers an incredible number of opportunities for teachers. The Web offer of e-learning open source platforms reached an impressive configuration. In this article, we present an educational software for developing digital skills conducted on the open source platform Moodle. The educational software has been developed for the grade’s XII students of a technical profile. It covers the unit Databases, namely, the Access application. The contents are particularly adapted for the students of Technical Colleges. We will also present a pedagogical research that demonstrates effectiveness of such an approach. We conducted the research at Technical College "Iuliu Maniu", Carei, Satu-Mare County. The research hypothesis was: Using a software to develop skills of using Microsoft Access application leads to much better results among students from technological high-schools. Our research proves {{that the use of}} an educational software and computer assisted instruction in teaching and learning contributes to a significant increase of efficiency of education...|$|E
40|$|The Twentieth Century {{will inevitably}} bring with it an {{enormous}} surge of technological innovations. Today, as the worlds technology sector continues to expand, {{more and more}} people are compelled to become involved. The Internet, specifically, the World Wide Web, provides a medium in which the average individual can experiment with the benefits associated with the emerging <b>computerized</b> <b>society</b> we are in the midst of developing. Within the last five years, both the growth and usage of the Internet has expanded at an enormous rate. It can be safe to assume that the Internet and the terms associated with it are becoming a part of the vernacular. Unless you have been living under a rock or have been the unfortunate owner of a dinosaur, the term used to describe an old and antiquated personal computer, you have probably come into some contact with the Internet. Whether youve been exposed to it or not is a totally different issue. As public usage of the Internet continues to expand, many new Internet based services are in need of development and at a price the average user can afford. Recently, there have been numerous amounts of Internet-related companies providing such services at...|$|E
40|$|In our highly <b>computerized</b> and {{networked}} <b>society,</b> {{privacy of}} individuals is precious and becomes increasingly important. Problems particularly {{arise in the}} context of authentication protocols where, as a general rule, entities actively reveal their respective identities to each other. To encounter this issue, different privacy-preserving authentication methods have been developed in the last decades. The list of these techniques comprises, apart from identity escrow, ring authentication, hidden and anonymous credentials, and several others, the concept of affiliation-hiding authentication (AHA). Such protocols offer the appealing and seemingly contradictory service to enable users to authenticate each other as members of a certain group without revealing their affiliation to group outsiders. In AHA protocols (also known as Secret Handshakes), users become group members by registering with group authorities (GAs) and obtaining individual membership credentials. Group members then use their credentials to privately authenticate each other, optionally also establishing a secure session key. The pivotal privacy property that contrasts AHA with classical authentication or authenticated key establishment is that parties learn each other's affiliations to groups and compute common session keys if and only if their groups match. Prior work has succeeded in constructing AHA protocols that offer different degrees of security, privacy, and efficiency. However, a set of essential problems have been left open. These include a close study of the level of trust that intrinsically has to be placed into participants of such systems (including into GAs), the extension of the single-group setting with only one GA to a setting where users are affiliated to multiple groups and, through AHA, want to discover matching ones, and certainly the question of efficient implementability. We argue that all these topics are highly relevant for practical deployment of privacy-preserving authentication in general, and AHA in particular. In this thesis, the author concretizes and cryptographically models these challenges, and offers provably secure solutions. Furthermore, this thesis treats privacy-related challenges that are posed {{in the context of}} network-based social interactions. Without doubt, online social networks, that help participants to build and reflect their social relations to other participants, have taken an essential role in people's daily life. A key step in the constitution of new links between participants consists of the reconciliation of shared contacts or friends. The author develops techniques to discover common contacts in social networks in a privacy-aware manner, i. e., without disclosing non-matching contacts. Besides formalizing this task and offering appropriate solutions, the thesis analyzes an interesting connection between AHA protocols and the challenge of private discovery of common contacts. By identifying and solving a variety of relevant open problems in the context of privacy-aware authentication, this thesis contributes to wide-scale deployment of methods that respect and regain user privacy in p 2 p systems, mobile ad hoc networks, and social networking applications...|$|R
40|$|The {{fundamental}} idea developed throughout {{this work is}} {{the introduction of new}} metrics in Social Sciences (Economics, Finance, opinion dynamics, etc). The concept of metric, that is the concept of measure, is usually neglected by mainstream theories of Economics and Finance. Financial Markets are the natural starting point of such an approach to Social Sciences because a systematic approach can be undertaken and the methods of Physics has shown to be very effective. In fact since a decade there exists a very huge amount of high frequency data from stock exchanges which permit to perform experimental procedures as in Natural Sciences. Financial markets appear as a perfect playground where models can be tested and where repeatability of empirical evidences are well-established features differently from, for instance, Macro-Economy and Micro-Economy. Thus Finance has been the first point of contact for the interdisciplinary application of methods and tools deriving from Physics and it has been also the starting point of this work. We investigated the origin of the so-called Stylized Facts of financial markets (i. e. the statistical properties of financial time series) in the framework of agent-based models. We found that Stylized Facts can be interpreted as a finite size effect {{in terms of the number}} of effectively independent agents (i. e. strategy) which results to be a key variable to understand the self-organization of financial markets. As a second issue we focused our attention on the order book dynamics both from a theoretical and a data oriented point of view. We developed a zero intelligence model in order to investigate the role of vanishing liquidity in the price response to incoming orders. Within the framework of this model we have analyzed the effect of the introduction of strategies pointing out that simple strategic behaviors can explain bursts of intermittency and long memory effects. On the other hand we quantitatively showed that there exists a feedback effect in markets called self-fulfilling prophecy which is the mechanism through which technical trading can exist and work. This feature is a very interesting quantitative evidence of a self-reinforcement of agents’ belief. Last but not least nowadays we live in a <b>computerized</b> and networked <b>society</b> where many of our actions leave a digital trace and affect other people’s actions. This has lead to the emergence of a new data-driven research field. In this work we highlighted how non financial data can be used to track financial activity, in detail we investigate query log volumes, i. e. the volumes of searches for a specific query done by users in a search engine, as a proxy for trading volumes and we find that users’ activity on Yahoo! search engine anticipates trading volume by one-two days. Differently from Finance, Economics is far from being an ideal candidate to export the methodology of Natural Sciences because of the lack of empirical data since controlled (and repeatable) experiments are totally artificial while real experiments are almost incontrollable and non repeatable due to a high degree of non stationarity of economical systems. However, the application of method deriving from complexity to the Economics of Growth is one of the more important achievement of the work here developed. The basic idea is to study the network defined by international trade flows and introduce a (non-monetary) metric to measure the complexity and the competitiveness of countries’ productive system. In addition we are able to define a metric for products’ quality which overcomes traditional economic measure for the quality of products given in terms of hours of qualified labour needed to produce a good. The method developed provides some impressive results in predicting economical growth of countries and offers many opportunities of improvements and generalizations...|$|R
40|$|When {{we think}} of ethical {{problems}} involving computing probably none is more paradigmatic than the issue of privacy. Given the ability of computers to manipulate information- to store endlessly, to sort efficiently, and to locate effortlessly- we are justifiably concerned that in a <b>computerized</b> <b>society</b> our privacy may be invaded and that information harmful to us will be revealed. Of course, we are reluctant {{to give up the}} advantages of speedy and convenient computerized information. We appreciate the easy access to computerized data when making reservations, using automatic teller machines, buying new products on the web, or investigating topics in computer data bases. Our challenge is to take advantage of computing without allowing computing to take advantage of us. When information is computerized, it is greasedto slide easily and quickly to many ports of call. This makes information retrieval quick and convenient. But legitimate concerns about privacy arise when this speed and convenience lead to the improper exposure of information. Greased information is information that moves like lightning and is hard to hold onto. Consider, for example, listed telephone numbers which have been routinely available through a telephone operator and a telephone book but which now are available along with address information in giant electronic phone books on the internet. Th...|$|E
