487|746|Public
25|$|Because R2 is {{the square}} of the norm of the {{standard}} bivariate normal variable (X, Y), it has the <b>chi-squared</b> <b>distribution</b> with two degrees of freedom. In the special case of two degrees of freedom, the <b>chi-squared</b> <b>distribution</b> coincides with the exponential distribution, and the equation for R2 above is a simple way of generating the required exponential variate.|$|E
25|$|The {{information}} entropy of the Weibull and Lévy distributions, and, implicitly, of the <b>chi-squared</b> <b>distribution</b> {{for one or}} two degrees of freedom.|$|E
25|$|Wilks’ Theorem {{provides}} a means of estimating {{the size and shape}} of the region of roughly equally-probable estimates for the population's parameter values, using the information from a single sample, using a <b>chi-squared</b> <b>distribution.</b>|$|E
3000|$|... {{represent}} a central <b>chi-square</b> <b>distribution</b> and a non-central <b>chi-square</b> <b>distribution</b> with 2 w {{degrees of freedom}} and the non-centrality parameter 2 γ, respectively.|$|R
3000|$|... 1) denote {{respective}} {{values of}} logarithmic likelihood function under the specification of null and alternative hypothesis. Under the null hypothesis, the statistic λ has approximately a <b>Chi-square</b> <b>distribution</b> (or mixed <b>Chi-square</b> <b>distribution)</b> with {{degrees of freedom}} equal {{to the number of}} restrictions.|$|R
40|$|The {{main purpose}} of this article is to do approximations {{graphically}} and mathematically the four-parameter generalized log-logistic distribution, denoted by G 4 LL(α,β,m_ 1,m_ 2), to the one-parameter <b>Chi-square</b> <b>distribution</b> with υ degrees of freedom. In order to achieve this purpose, this article creates graphically the probability density functions of both distribution and derives mathematically the MGF of the both distributions. To prove the MGF of Chi-square as a special case of the MGF of G 4 LL distribution, we utilized an expansion of the MacLaurin series. The results show that graphically, the <b>Chi-square</b> <b>distribution</b> can be approximated by the generalized log-logistic distribution. Moreover, by letting α= 1,β=-ln⁡(2 m_ 2),m_ 1 =v/ 2 and m_ 2 →∞, the MGF of the G 4 LL distribution can be written in the form of the MGF of the <b>Chi-square</b> <b>distribution.</b> Thus, the <b>Chi-square</b> <b>distribution</b> is a limiting or special case distribution of the generalized log-logistic distribution. The {{main purpose of}} this article is to do approximations graphically and mathematically the four-parameter generalized log-logistic distribution, denoted by G 4 LL(α,β,m_ 1,m_ 2), to the one-parameter <b>Chi-square</b> <b>distribution</b> with υ degrees of freedom. In order to achieve this purpose, this article creates graphically the probability density functions of both distribution and derives mathematically the MGF of the both distributions. To prove the MGF of Chi-square as a special case of the MGF of G 4 LL distribution, we utilized an expansion of the MacLaurin series. The results show that graphically, the <b>Chi-square</b> <b>distribution</b> can be approximated by the generalized log-logistic distribution. Moreover, by letting α= 1,β=-ln⁡(2 m_ 2),m_ 1 =v/ 2 and m_ 2 →∞, the MGF of the G 4 LL distribution can be written in the form of the MGF of the <b>Chi-square</b> <b>distribution.</b> Thus, the <b>Chi-square</b> <b>distribution</b> is a limiting or special case distribution of the generalized log-logistic distribution...|$|R
25|$|Two-sided normal {{regression}} {{tolerance intervals}} {{can be obtained}} based on the noncentral <b>chi-squared</b> <b>distribution.</b> This enables the calculation of a statistical interval within which, with some confidence level, a specified proportion of a sampled population falls.|$|E
25|$|If X ~ Gamma(ν/2, 1/2)(shape -scale parametrization), then X is {{identical}} to χ2(ν), the <b>chi-squared</b> <b>distribution</b> with ν degrees of freedom. Conversely, if Q ~ χ2(ν) and c is a positive constant, then cQ ~ Gamma(ν/2, 2c).|$|E
25|$|It may be {{difficult}} to determine S for some distributions. This is usually because a closed form for the median is not known: examples of such distributions include the gamma distribution, inverse-chi-squared distribution, the inverse-gamma distribution and the scaled inverse <b>chi-squared</b> <b>distribution.</b>|$|E
3000|$|... is a Gaussian signal, {{note that}} the {{kurtosis}} of a Gaussian signal in the power spectral domain is 6. This is because a Gaussian signal in the time domain obeys the <b>chi-square</b> <b>distribution</b> with {{two degrees of freedom}} in the power spectral domain; for such a <b>chi-square</b> <b>distribution,</b> [...]...|$|R
40|$|This article {{provides}} an alternative method to derive the noncentral <b>chi-square</b> <b>distribution.</b> This method is neat and elegant {{compared with the}} traditional way of deriving the noncentral <b>chi-square</b> <b>distribution.</b> Thus it can be adopted by any mathematical statistics courses in undergraduate {{as well as in}} postgraduate levels. Department of Applied Mathematic...|$|R
5000|$|The Wald {{test can}} be {{evaluated}} against a <b>chi-square</b> <b>distribution.</b>|$|R
25|$|One can {{generate}} Student-t samples {{by taking the}} ratio of variables from the normal distribution and the square-root of <b>chi-squared</b> <b>distribution.</b> If we use instead of the normal distribution, e.g., the Irwin–Hall distribution, we obtain over-all a symmetric 4-parameter distribution, which includes the normal, the uniform, the triangular, the Student-t and the Cauchy distribution. This is also more flexible than some other symmetric generalizations of the Gaussian distribution.|$|E
25|$|One-tailed {{tests are}} used for {{asymmetric}} distributions that have a single tail, such as the <b>chi-squared</b> <b>distribution,</b> which are common in measuring goodness-of-fit, or for {{one side of a}} distribution that has two tails, such as the normal distribution, which is common in estimating location; this corresponds to specifying a direction. Two-tailed tests are only applicable when there are two tails, such as in the normal distribution, and correspond to considering either direction significant.|$|E
25|$|The {{derivation}} above {{has been}} presented for the case of uninformative priors for μ and σ2; {{but it will be}} apparent that any priors that lead to a normal distribution being compounded with a scaled inverse <b>chi-squared</b> <b>distribution</b> will lead to a t-distribution with scaling and shifting for P(μ|D,I), although the scaling parameter corresponding to s2/n above will then be influenced both by the prior information and the data, rather than just by the data as above.|$|E
5000|$|... {{where the}} Yi {{represent}} random variables having (different) noncentral <b>chi-squared</b> <b>distributions,</b> where Z0 has a standard normal distribution, and where all these random variables are independent. Some important special cases relating {{to this particular}} form either omit the additional standard normal term and/or have central rather than non-central <b>chi-squared</b> <b>distributions</b> for {{the components of the}} summation.|$|R
50|$|The <b>chi-square</b> <b>distribution</b> if {{the number}} of degrees of freedom is >= 2.|$|R
5000|$|U1 and U2 have <b>chi-squared</b> <b>distributions</b> with d1 and d2 {{degrees of}} freedom respectively, and ...|$|R
25|$|The p-value was {{introduced}} by Karl Pearson in the Pearson's chi-squared test, where he defined P (original notation) as {{the probability that the}} statistic would be at or above a given level. This is a one-tailed definition, and the <b>chi-squared</b> <b>distribution</b> is asymmetric, only assuming positive or zero values, and has only one tail, the upper one. It measures goodness of fit of data with a theoretical distribution, with zero corresponding to exact agreement with the theoretical distribution; the p-value thus measures how likely the fit would be this bad or worse.|$|E
500|$|... {{and where}} [...] follows a <b>chi-squared</b> <b>distribution</b> with [...] degrees of freedom. Therefore, {{to test the}} {{hypothesis}} [...] that a random sample of [...] values comes from the distribution , the statistic [...] can be calculated. Then [...] should be rejected with significance [...] if the value is greater than the critical value of the appropriate <b>chi-squared</b> <b>distribution.</b>|$|E
2500|$|The p-value {{was first}} {{formally}} introduced by Karl Pearson, in his Pearson's chi-squared test, using the <b>chi-squared</b> <b>distribution</b> and notated as capital P. The p-values for the <b>chi-squared</b> <b>distribution</b> (for various values of χ2 and degrees of freedom), now notated as P, was calculated in , collected in [...]|$|E
2500|$|The {{cumulative}} distribution {{functions of the}} Poisson and <b>chi-squared</b> <b>distributions</b> are related in the following ways: ...|$|R
40|$|The normal {{distribution}} based likelihood ratio (LR) statistic {{is widely used}} in structural equation modeling. Under a sequence of local alternative hypotheses, this statistic {{has been shown to}} asymptotically follow a noncentral <b>chi-square</b> <b>distribution.</b> In practice, the population mean vector and covariance matrix as well as the model and sample size are always fixed. It is hard to justify the validity of the noncentral <b>chi-square</b> <b>distribution</b> for the resulting LR statistic even when data are normally distributed and sample size is large. By extending results in the literature, this paper develops {{normal distribution}}s to describe the behavior of the LR statistic for mean and covariance structure analysis. A sequence of local alternative hypotheses is not necessary for the proposed distributions to be asymptotically valid. When the effect size is medium and above or when the model is not trivially misspecified, empirical results indicate that a refined normal distribution describes the behavior of the LR statistic better than the commonly used noncentral <b>chi-square</b> <b>distribution,</b> as measured by the Kolmogorov-Smirnov distance. Quantile-quantile plots are also provided to better understand the different distributions. Kolmogorov-Smirnov distance Noncentral <b>chi-square</b> <b>distribution</b> Normal distribution Quantile-quantile plot Structural model...|$|R
40|$|For a {{class of}} multivariate skew normal distributions, the noncentral skew <b>chi-square</b> <b>distribution</b> is studied. The {{necessary}} and sufficient conditions under which a sequence of quadratic forms is generalized noncentral skew chi-square distributed random variables are obtained. Several examples are given to illustrate the results. primary, 62 H 10 secondary, 62 E 17 Multivariate skew normal <b>distribution</b> Noncentral skew <b>chi-square</b> <b>distribution</b> Necessary and sufficient conditions Cochran's theorem Moment generating function...|$|R
2500|$|The {{above is}} also a scaled inverse <b>chi-squared</b> <b>distribution</b> where ...|$|E
2500|$|... {{a normal}} {{distribution}} and a scaled inverse <b>chi-squared</b> <b>distribution</b> respectively, where [...] and ...|$|E
2500|$|To {{approximate}} the <b>chi-squared</b> <b>distribution,</b> the non-centrality parameter, , {{is set to}} zero, yielding ...|$|E
40|$|The most {{frequent}} use of the <b>chi-square</b> <b>distribution</b> is {{in the area of}} goodness-of-fit of a distribution. The likelihood ratio test is a commonly used test statistic as the maximum likelihood estimate in statistical inferences. The recently revised versions of the likelihood ratio test statistics are used in estimating the parameter in the <b>chi-square</b> <b>distribution.</b> The estimates are compared with the commonly used method of moments and the maximum likelihood estimate...|$|R
30|$|Open {{image in}} new window is the upper α percentiles of the <b>chi-square</b> <b>distribution</b> with a- 1 degrees of freedom.|$|R
2500|$|Again {{using the}} {{relation}} between the central and noncentral <b>chi-squared</b> <b>distributions,</b> the cumulative distribution function (cdf) can be written as ...|$|R
2500|$|... {{essentially}} approximating the normalized <b>chi-squared</b> <b>distribution</b> X / k as {{the cube}} of a Gaussian.|$|E
2500|$|The <b>chi-squared</b> <b>distribution</b> [...] is {{approximately}} normal with mean [...] and variance , for large [...]|$|E
2500|$|... has a <b>chi-squared</b> <b>distribution</b> with [...] {{degrees of}} freedom (by Cochran's theorem). [...] It is readily shown that the {{quantity}} ...|$|E
5000|$|Log-linear {{analysis}} uses a {{likelihood ratio}} statistic : {{that has an}} approximate <b>chi-square</b> <b>distribution</b> when the sample size is large: ...|$|R
3000|$|... τ/N. χ _ 2 u^ 2 {{follows a}} central <b>chi-square</b> <b>distribution</b> with 2 u {{degrees of freedom}}, and χ _ 2 u^ 2 (2 γ _p,k^n) follows a non-central <b>chi-square</b> <b>distribution</b> with 2 u degrees of freedom and a non {{centrality}} parameter 2 γ _p,k^n [4]. And γ _p,k^n is the instantaneous signal-noise ratio (SNR) of the received signal from the pth PU at the kth sensing node on the subcarrier n.|$|R
30|$|As {{the number}} of {{datasets}} increases, the statistical test can {{be approximated by using}} the <b>chi-square</b> <b>distribution</b> with c- 1 degrees of freedom [72]. Then, if the F_R computed value is larger than the critical value for the <b>chi-square</b> <b>distribution</b> the null hypothesis is rejected. This null hypothesis states that ranks obtained per dataset are globally similar. Accordingly, rejecting the null hypothesis means that there are significant differences in the ranks across datasets. It is important to note that, in general, the critical value is obtained with significance level α= 0.05. Synthesizing, the null hypothesis should be rejected if F_R > X^ 2 _α, where X^ 2 _α is the critical value verified in the <b>chi-square</b> <b>distribution</b> table with c- 1 degrees of freedom and α equals 0.05.|$|R
