50|45|Public
3000|$|... {{denote the}} ideal SPN and the <b>composite</b> <b>error,</b> {{which is caused}} during the SPN {{estimation}} process, corresponding to SPN_i, respectively. The lower energy of Ω [...]...|$|E
40|$|The {{analysis}} of variance technique was applied, and {{it was found that}} the interaction term between two factors was significant. Thereafter, the tests on the simple effects for each of the factors must be made. The appropriate use of the error term is different in four cases. It is as follows: Case 1 : p×q or p×q×r factorial experiment with no repeated measures. Case 2 : p×q factorial experiment with repeated measures on one factor. Case 3 : p×q×r factorial experiment with repeated measures on last two factors. Case 4 : p×q×r factorial experiment with repeated measures on the last factor. As for the Case 1, the usual error term which has been resulted from the {{analysis of}} variance is used. As for the Case 2, the error(w) term is used for the tests of the simple effects on the repeated factor, and the <b>composite</b> <b>error</b> term which is consisted of error(b) and error(w) is used for the tests on non-repeated factor. As for the Case 3, the error(w 1) or the error(w 2) term is used; or the one of <b>composite</b> <b>error</b> terms which are consisted of error(b) and error(w 1); error(b) and error(w 2); error (w 1) and error(w 3); and error(w 2) and error(w 3) is used. Or the <b>composite</b> <b>error</b> term which is consisted of error(b), error(w 1), error(w 2), and error(w 3) is used. As for the Case 4, the error(b) or the error(w) term is used; or the <b>composite</b> <b>error</b> term which is consisted of error(b) and error(w) is used...|$|E
40|$|The {{traceability}} of measurements {{performed to}} gauge {{information about the}} operation of electrical power systems under nonsinusoidal conditions is an important and live issue, usually neglected by the technical Standards. A generalized definition of the “composite error” is proposed in this paper as a meaningful index for the metrological characterization of voltage and current transducers in steady-state nonsinusoidal conditions. In this connection, the relationship among the <b>composite</b> <b>error,</b> the classical ratio and phase-angle errors and other parameters characterizing the waveforms of the transduced quantities are pointed out. Finally, problems involved in the measurement of the <b>composite</b> <b>error</b> in steady-state distorted conditions are dealt with {{and the results of}} some experimental work are presented...|$|E
3000|$|I replace both variances in the {{denominator}} {{with the}} covariance of sibling <b>composite</b> <b>errors,</b> as explained above. I replace the numerator with the covariance of sibling <b>composite</b> <b>errors</b> for different outcomes, i.e., Cov (ε _ifst^k, ε _i'fs't^ℓ) for i≠i [...]...|$|R
3000|$|... 1 in Eq. (5) as <b>composite</b> <b>errors</b> differ {{except for}} x_i=x̂_i. One {{feasible}} expectation {{of the magnitude}} of ρ [...]...|$|R
3000|$|The {{variance}} {{of the family}} innovation {{is equal to the}} co{{variance of}} the <b>composite</b> <b>errors</b> as long as two conditions hold. First, the family innovation must be uncorrelated with the idiosyncratic error, i.e., Cov (ξ ^k_ft,ν ^k_ifst)= 0. This condition holds by construction since I model the family innovation as a fixed effect. Second, the idiosyncratic errors for siblings in the same year must be unrelated, i.e., Cov (ν ^k_ifst,ν ^k_i'fs't)= 0 for i≠i [...]...|$|R
40|$|Abstract: This paper {{deals with}} <b>composite</b> <b>error</b> {{determination}} of current transformers (CT) for purpose of nonharmonic periodic current measurements. The experiment was performed for typical nonharmonic periodic waveforms {{that occur in}} power grids with frequency 50 Hz. The area of measurements was extended also for higher fundamental frequency values...|$|E
3000|$|... 12 Note {{that in my}} {{estimation}} of the educational production function, I estimate the family-year effect as a fixed effect. However, I use the <b>composite</b> <b>error</b> defined in Eq. (2), which {{is the sum of}} the family innovation {{and the rest of the}} error, in my {{estimation of}} the dispersion of family innovations.|$|E
40|$|The 24 -hour 500 -mb. {{barotropic}} forrcasts {{prepared by}} the Joint Numerical Weather Prediction Unit (JNWPU) have been investigated in 30 cases of rapid sea level cyclogenesis. <b>Composite</b> <b>error</b> maps are presented for the region of cyclogenesis. The 500 -mb. errors are found to bc significantly larger when the solenoidal field at that level is strong than when it is weak. 1...|$|E
40|$|Use of {{the flat}} type strain wave gearing {{increases}} in industrial robots owing to its shortness and its high torsional stiffness. However, the vibration characteristic {{of the flat}} type strain wave gearing {{has not yet been}} adequately clarified. In this paper, the robot system, which was composed of an input motor, the flat type strain wave gearing and an output arm, was operated to clarify the effects of <b>composite</b> <b>errors</b> in meshing between the flexible spline and two circular splines and the misalignment of two circular splines to the flexible spline on the amplitude of vibrating displacement of the output arm...|$|R
30|$|Since the DOA-BB-MUSIC and QQ-MUSIC estimators cannot {{estimate}} the polarization angles, we only calculate the <b>composite</b> RMS <b>errors</b> of the polarization estimations of the DOA-PB-BB-MUSIC and ESPRIT estimators with different SNRs under both the noncoherent and coherent noise conditions.|$|R
40|$|Stochastic {{disturbance}} {{terms in}} an econometric model encompass {{two types of}} error: 1) specification error resulting from an econometric model that is simpler than the “true ” economic model, and 2) stochastic innovations to the “true ” economic model. It is standard practice to minimize these <b>composite</b> <b>errors</b> to estimate the econometric model. In many interesting cases, however, the “true ” model forecasts or errors {{can be regarded as}} observed through futures markets, prediction markets, or surveys of professional forecasters. When the true model forecasts or errors are observed, econometric estimation can be improved by minimizing the distance from the econometric model’s residuals to the true model errors, rather than to a vector of zeros. This paper derives the theory and applies the method to estimate simple time series models. The error-matching estimation method prescribed by this paper avoids overweighting large model errors that were unforecastable ex ante, and reduces standard errors substantially, by about 20 – 40 % for the simple time series examples considered...|$|R
40|$|AbstractThe {{temporal}} {{instability of}} parallel viscous two-phase mixing layers is extended to current-fluid mud by considering a <b>composite</b> <b>error</b> function velocity profile. The influence of viscosity ratio, Reynolds number, and Froude {{number on the}} instability of the system are discussed and a new phenomenon never discussed is investigated based on our numerical results. It is shown that viscosity can enlarge the unstable wave number range, cause new instability modes, and certainly reduce {{the growth rate of}} Kelvin—Helmholtz (K—H) instability...|$|E
40|$|An optimal {{choice of}} segment {{boundaries}} in piecewise approximation {{is shown to}} be soluble {{by means of a}} dynamic programme in a scalar state variable. It is shown that the <b>composite</b> <b>error</b> function is continuous in modulus, and that for approximation of concave or convex functions by linear segments, the composite approximation is continuous. Provided the residual term satisfies certain requirements, the solution has uniqueness properties and can be found by means of a grid search method. 1...|$|E
40|$|This paper {{considers}} {{design of}} filterbank-based transceivers. A <b>composite</b> <b>error</b> criterion is proposed to capture all the three traditional distortions. Incorporating noise attenuation and filter bandlimiting properties into this error criterion, an optimal design procedure is developed {{and applied to}} a transceiver design example, yielding an FIR transceiver that has good frequency-selective properties and is close to perfect reconstruction. As a least-squares solution is given in closed form in each iteration, the algorithm is easy to implement...|$|E
40|$|Comments are welcome. Please do not quote Abstract: A panel {{data model}} which {{combines}} an autoregressive fixed effects panel data equation and a static equation with measurement errors is considered. Examples of models for pure time series data with errors in variables are the starting point. Two {{versions of the}} Generalized Method of Moments (GMM) for this panel data model are considered, with fixed effects accounted for by, respectively, (i) transforming the equation to differences and keeping the instruments in levels, (ii) keeping the equation in levels and transforming the instruments to differences. In specifying the set of valid moment conditions and instrument sets, patterns with finite memory of disturbances, of latent regressors and of the measurement errors are discussed. Focus is on examining how the potential instrument sets which satisfy both the rank conditions for the instrumented variables and the orthogonality conditions for the <b>composite</b> <b>errors</b> and disturbances, change when the model’s memory pattern changes. Sometimes the joint occurrence of measurement errors and long memories implies that the potential instrument sets becomes too small to make consistent estimation possible. An application based on panel data for capital stock and output from Norwegian manufacturing firms is presented...|$|R
40|$|Currently, {{evaluating}} {{the performance of}} internally cooled/heated liquid desiccant dehumidification system (ICHLD) is complicated, requiring {{a large number of}} iterative calculations. This study developed a simplified numerical model, for obtaining the outlet parameters of all fluids (air, solution and heating/cooling fluid) and system performance of dehumidifiers and regenerators in ICHLD accurately and directly. The model was built by defining three kinds of effectiveness, i. e. enthalpy effectiveness, moisture effectiveness and temperature effectiveness. Based on the parameter study under different design and operation conditions, the correlations of effectiveness were obtained by multiple linear regressions, using heat and mass transfer gradients and other related parameters as variables. Comparisons between our results and those by existing mathematical models show that the <b>composite</b> <b>errors</b> are acceptable, with an average difference of 14. 5 % for dehumidifiers and 6. 83 % for regenerators. Similar trends were also observed in comparisons with previous experimental data, with average errors about 20 %. The inlet solution temperature has a greatest impact on the enthalpy effectiveness and moisture effectiveness. The temperature effectiveness is mainly determined by the temperature difference between cooling/heating fluid and solution. This simplified model is very useful for researchers and engineers, especially those interested in the dynamic performance of ICHLD systems. Department of Building Services Engineerin...|$|R
40|$|A {{theoretical}} {{formulation of}} differential and <b>composite</b> OMEGA <b>error</b> {{is presented to}} establish hypotheses about the functional relationships between various parameters and OMEGA navigational errors. Computer software developed to provide for extensive statistical analysis of the phase data is described. Results from the regression analysis used to conduct parameter sensitivity studies on differential OMEGA error tend to validate the theoretically based hypothesis concerning the relationship between uncorrected differential OMEGA error and receiver separation range and azimuth. Limited results of measurement of receiver repeatability error and line of position measurement error are also presented...|$|R
40|$|The {{temporal}} {{instability of}} parallel viscous two-phase mixing layers is extended to current-fluid mud by considering a <b>composite</b> <b>error</b> function velocity profile. The influence of viscosity ratio, Reynolds number, and Froude {{number on the}} instability of the system are discussed and a new phenomenon never discussed is investigated based on our numerical results. It is shown that viscosity can enlarge the unstable wave number range, cause new instability modes, and certainly reduce {{the growth rate of}} Kelvin—Helmholtz (K—H) instability...|$|E
40|$|François Laisney, Michael Lechner and Winfried Pohlmeier This paper {{presents}} {{an extension of}} Chamberlain's approach to modelling correlated random effects in a dichotomous model estimated on panel data. The extension consists in relaxing the arbitrary normality assumption made on all error terms and on specifying instead a SNP distribution for the <b>composite</b> <b>error</b> term resulting in each wave from Chamberlain's treatment of correlated random effects. An application to a panel of 1325 German firms illustrates the feasibility of the approach. ...|$|E
40|$|The {{performance}} and theory of operation for the High Speed Hard Decision Sequential Decoder are delineated. The decoder is a {{forward error correction}} system which is capable of accepting data from binary-phase-shift-keyed and quadriphase-shift-keyed modems at input data rates up to 30 megabits per second. Test {{results show that the}} decoder is capable of maintaining a <b>composite</b> <b>error</b> rate of 0. 00001 at an input E sub b/N sub o of 5. 6 db. This performance has been obtained with minimum circuit complexity...|$|E
40|$|The {{problems}} of robustness of parametric hypotheses testing are {{considered for the}} cases of simple and <b>composite</b> hypotheses. Conditional <b>error</b> probabilities and expected sample sizes of sequential tests are evaluated. Asymptotic expansions for these characteristics are obtained under the distortions of Tukey–Huber type. Robust sequential tests are constructed with the minimax criterion...|$|R
40|$|Abstract—Cogging {{effect is}} a serious {{disadvantage}} of the permanent magnetic synchronous motor (PMSM), and cogging force is a position-dependent periodic disturbance. In our previous work [1], a dual high-order periodic adaptive learning compensation (DHO-PALC) method for state-dependent periodic disturbance was presented, where the long term stability issue was not addressed. Due {{to the impact of}} the high frequency components, when applying the DHO-PALC for a long time, the tracking errors may grow, and the system may become even unstable eventually. This phenomenon also appears in many other practical motion control systems using ILC or RC strategies. In this paper, in order to achieve long term stability, we propose a dual high-order dynamic adaptive learning compensation (DHO-D-PALC) method for cogging effect. In this method, stored information of more than one previous periods are included for both the <b>composite</b> tracking <b>error</b> and the estimate of cogging force. Particularly, since we use a dynamic learning control law to update the current estimate of cogging, the long term stability can be guaranteed. Extensive simulation results are included to demonstration, 1) high-order in <b>composite</b> tracking <b>error</b> offers faster convergence, 2) high-order in cogging estimate better accommodates the case of varying reference signal, 3) dual high-order scheme has the potential of much better performance over the conventional first-order scheme, 4) the introduction of dynamic learning updating scheme helps achieving the long term stability of the adaptive learning controller. Index Terms—State-dependent disturbance, adaptive control, dual-high-order periodic adaptive learning control, dynamic learning com-pensation, PMSM motor. I...|$|R
40|$|For a {{lithography}} {{process with}} a design rule of 0. 18 um and beyond, {{the most critical}} issue is the gate critical dimension (CD) control because it affects the circuit performance especially for high-end logic devices directly. The gate CD variation is generally categorized into two folds. The first one is CD variation affected by optical and process proximity effects, which is pattern layout dependent errors and can be treated by automatic optical proximity effect correction (OPC). The second one is across chip line width variation (ACLV) caused by aberration of stepper/scanner exposure tools and mask writing error. Although both two have been studied in detail respectively, the <b>composite</b> <b>errors</b> are not studied sufficiently. This is because these errors can not necessarily be quantified for all the pattern layout configuration exists in the actual device chips. In this paper we will propose a new analysis method to quantify OPC performance and gauge capability of in-process quality control(IPQC) monitors. The method consists of two parts. The first is to verify validity of OPC by using IPQC monitor patterns {{and the second is}} to quantify gauge capability of the IPQC monitor patterns for representing ACLV. Our new analysis method is significantly effective and persuasive for verifying OPC performance as well as gauge capability of IPQC monitor patterns. It is also quite useful to optimize OPC parameters and methods to reduce ACLV that is most critical to obtain high-end logic devices with a design rule of 0. 15 um and below...|$|R
40|$|AbstractA novel {{adaptive}} regularization parameter selection (ARPS) {{method is}} proposed {{in this paper}} to enhance {{the performance of the}} regularization method. The proposed ARPS method enables a gradient descent type training to tunnel through some of the undesired sub-optimal solutions on the <b>composite</b> <b>error</b> surface by means of changing the value of the regularization parameter. Undesired sub-optimal solutions are introduced inherently from regularized objective functions. Hence, the proposed ARPS method is capable of enhancing the regularization method without getting stuck at these sub-optimal solutions...|$|E
40|$|AbstractGears {{as the key}} {{transmission}} {{parts of}} machineries are widely used in mechanical and aeronautical fields. GeoSpelling, a framework standardized as ISO 17450 - 2005 supported tolerancing process {{by a set of}} concepts and mathematical algorithms, is a coherent and complete model to manage the shape variations of geometrical parts. Considering current gear specification standards incline to inspection convenient ignoring tolerance quality control, so it is a realistic necessary to develop a coherent gear specification model based on GeoSpelling language. This model should not only consider integrating functional requirements involved in design stage but also be able to correspond to manufacturing and inspection procedures. In this paper a new approach based on discrete geometry is introduced to design gear specification model. It is also focus on a perspective discussion on how SSA (Statistical Shape Analysis) method could be a solution to analyze the gear integrated errors into different individual errors, such as tangential <b>composite</b> <b>error,</b> tooth-to-tooth tangential <b>composite</b> <b>error,</b> pitch accumulated deviation, tooth-to-tooth pitch accumulated deviation and geometric eccentricity. The fundamentals mentioned above provide a reliable methodology for analyzing gear manufacturing errors. As a result it is able to enhance the computational processing capability and metrological traceability for gear quality control...|$|E
30|$|The {{equations}} {{are estimated}} using a random effects panel data routine, where the regional unobservable effects {{are considered to}} be part of a <b>composite</b> <b>error</b> term and are not necessarily fixed over time. This specification also includes time dummies. The choice of the random effects model is made following the pretest estimator based on the Hausman and Taylor model. 15 This pretest estimator reverts to the random effects estimator if the standard Hausman test based on the FE versus the RE estimators is not rejected, as it is the case in our model.|$|E
3000|$|To {{evaluate}} {{the performances of}} direction findings and polarization estimations, we define the <b>composite</b> root-mean-square (RMS) <b>errors</b> of the direction and polarization angles for the K sources as { [∑ _k = 1 ^K (Δθ _k^ 2 +Δϕ _k^ 2)]/ (2 K)}^ 1 / 2 and{ [∑ _k = 1 ^K (Δγ _k^ 2 +Δη _k^ 2)]/ (2 K)}^ 1 / 2, respectively, where Δ [...]...|$|R
30|$|Manufacturing and {{assembling}} {{errors are}} indispensible in engineering practice, which {{need to be}} considered and analyzed to ensure the performance and precision of products. Liu et al. propose an error equivalent model of revolute joints considering both radial and axial clearances for investigating the pointing accuracy of the antenna pointing mechanisms. Yuan et al. develop a precisely generalized finite element model of helical gear system to investigate the influences of manufacturing errors including short term and long term components on the quasi-static and dynamic behaviors of the system, and conclude that both of these manufacturing errors have notable influence on mesh stiffness, loaded static transmission <b>error</b> and loaded <b>composite</b> mesh <b>error</b> of helical gears.|$|R
40|$|This paper aims {{to present}} a study on validating a passive and {{quantitative}} infrared thermography (PQIRT) methodology to estimate the size of debond in rendered and tile composites in external walls of high-rise buildings. The accuracies of the PQIRT methodology were evaluated under four conditions, including (i) nine depths of debonds, (ii) three temperature ranges, (iii) two types of composites and (iv) changes of heat flux during a 2 -h diurnal and continuous monitoring. The method processed infrared images with a simple spatial pixel differentiation algorithm to determine temperature gradients, where the absolute temperature value was no longer required. The methodology was validated by comparing the estimated and the actual sizes of the debonds by statistical error analysis. In the rendered <b>composites,</b> the percentage <b>error</b> was at most 15 % for cover depths of debond {{up to 20 mm}} for all temperature ranges. In the tile <b>composites,</b> the <b>error</b> was at most 30 % for cover depths of debond up to 9 mm at all temperature conditions, while the estimation was erroneous for cover depths beyond 9 mm. These results will form the basis to develop guidelines for using PQIRT for inspection of high-rise building envelops in densely populated cities, which supplement the current qualitative inspection adopted by a handful of few accredited laboratories in Hong Kong and elsewhere. Department of Civil and Environmental Engineerin...|$|R
3000|$|... is not {{controlled}} for and instead is let {{to be part}} of the <b>composite</b> <b>error,</b> the estimators will be inconsistent. In particular, the inequality measures and other explanatory variables can be correlated with the unobserved individual effects so that without dealing with these effects, the estimation will suffer from omitted-variables problem. A fixed effect estimator (FE) will take away the individual unobserved effects by subtracting the time means of each variable for every individual in the model. The interesting point is that this procedure will allow the unobserved effect α [...]...|$|E
40|$|AbstractThis paper {{presents}} a new regularization method based on dynamic tunneling for enhancing generalization capability of multilayered neural networks. The proposed method enables escape through undesired sub-optimal solutions on the <b>composite</b> <b>error</b> surface {{by means of}} dynamic tunneling. Undesired sub-optimal solutions may be increased or introduced from regularized objective function. Hence, the proposed method is capable of enhancing the regularization property without getting stuck at sub-optimal values in search space. The regularization property and escape from the sub-optimal values have been demonstrated through computer simulations on two examples...|$|E
40|$|Using {{the maximum}} {{likelihood}} method, {{in order to}} estimate Half-Normal stochastic frontier production models, entails several practical di 1 culties that, perhaps, have not been su 1 ciently emphasised. In employing FRONTIER software, we analyse the {{case in which the}} estimation obtained suggests the absence of random factors in the <b>composite</b> <b>error</b> term. We have proved that there are reasons to doubt the validity of the parameter estimates and especially of its standard errors. On the other hand, no estimation is obtained in the previous situation, with LIMDEP software, but an error message...|$|E
40|$|I {{describe}} {{a strategy for}} structural estimation that uses simulated maximum likelihood (SML) to estimate the structural parameters appearing in a model's first-order conditions (FOCs). Generalized method of moments (GMM) is often the preferred method for estimation of FOCs, as it avoids distributional assumptions on stochastic terms, "provided" all structural errors enter the FOCs additively, giving a single <b>composite</b> additive <b>error.</b> But SML has advantages over GMM in models where multiple structural errors enter the FOCs nonadditively. I develop new simulation algorithms required to implement SML based on FOCs, and I illustrate the method using a model of U. S. multinational corporations. Copyright � (2009) by the Economics Department of the University of Pennsylvania and the Osaka University Institute of Social and Economic Research Association. ...|$|R
40|$|See Hornberger for a {{scientific}} commentary on this article (doi: 10. 1093 /brain/awv 027). Verbal initiation, suppression and strategy generation/use are cognitive processes widely {{held to be}} supported by the frontal cortex. The Hayling Test was designed to tap these cognitive processes within the same sentence completion task. There are few studies specifically investigating the neural correlates of the Hayling Test {{but it has been}} primarily used to detect frontal lobe damage. This study investigates the components of the Hayling Test in a large sample of patients with unselected focal frontal (n = 60) and posterior (n = 30) lesions. Patients and controls (n = 40) matched for education, age and sex were administered the Hayling Test as well as background cognitive tests. The standard Hayling Test clinical measures (initiation response time, suppression response time, suppression errors and overall score), <b>composite</b> <b>errors</b> scores and strategy-based responses were calculated. Lesions were analysed by classical frontal/posterior subdivisions as well as a finer-grained frontal localization method and a specific contrast method that is somewhat analogous to voxel-based lesion mapping methods. Thus, patients with right lateral, left lateral and superior medial lesions were compared to controls and patients with right lateral lesions were compared to all other patients. The results show that all four standard Hayling Test clinical measures are sensitive to frontal lobe damage although only the sup-pression error and overall scores were specific to the frontal region. Although all frontal patients produced blatant suppression errors, a specific right lateral frontal effect was revealed for producing errors that were subtly wrong. In addition, frontal patients overall produced fewer correct responses indicative of developing an appropriate strategy but only the right lateral group showed...|$|R
40|$|Verbal initiation, {{suppression}} {{and strategy}} generation/use are cognitive processes widely {{held to be}} supported by the frontal cortex. The Hayling Test was designed to tap these cognitive processes within the same sentence completion task. There are few studies specifically investigating the neural correlates of the Hayling Test {{but it has been}} primarily used to detect frontal lobe damage. This study investigates the components of the Hayling Test in a large sample of patients with unselected focal frontal (n = 60) and posterior (n = 30) lesions. Patients and controls (n = 40) matched for education, age and sex were administered the Hayling Test as well as background cognitive tests. The standard Hayling Test clinical measures (initiation response time, suppression response time, suppression errors and overall score), <b>composite</b> <b>errors</b> scores and strategy-based responses were calculated. Lesions were analysed by classical frontal/posterior subdivisions as well as a finer-grained frontal localization method and a specific contrast method that is somewhat analogous to voxel-based lesion mapping methods. Thus, patients with right lateral, left lateral and superior medial lesions were compared to controls and patients with right lateral lesions were compared to all other patients. The results show that all four standard Hayling Test clinical measures are sensitive to frontal lobe damage although only the suppression error and overall scores were specific to the frontal region. Although all frontal patients produced blatant suppression errors, a specific right lateral frontal effect was revealed for producing errors that were subtly wrong. In addition, frontal patients overall produced fewer correct responses indicative of developing an appropriate strategy but only the right lateral group showed a significant deficit. This problem in strategy attainment and implementation could explain, at least in part, the suppression error impairment. Contrary to previous studies there was no specific frontal effect for verbal initiation. Overall, our results support a role for the right lateral frontal region in verbal suppression and, for the first time, in strategy generation/use...|$|R
