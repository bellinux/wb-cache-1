1102|2806|Public
25|$|The <b>Classification</b> <b>Tree</b> Editor (CTE) is a {{software}} tool for test design that implements the <b>classification</b> <b>tree</b> method.|$|E
25|$|While {{the method}} {{can be applied}} using a pen and a paper, the usual way {{involves}} the usage of the <b>Classification</b> <b>Tree</b> Editor, a software tool implementing the <b>classification</b> <b>tree</b> method.|$|E
25|$|The <b>classification</b> <b>tree</b> {{editor for}} {{embedded}} systems also based upon this edition.|$|E
40|$|Purpose: <b>Classification</b> <b>trees</b> are {{increasingly}} being used to classifying patients {{according to the presence}} or absence of a disease or health outcome. A limitation of <b>classification</b> <b>trees</b> is their limited predictive accuracy. In the data-mining and machine learning literature, boosting has been developed to improve classification. Boosting with <b>classification</b> <b>trees</b> iteratively grows <b>classification</b> <b>trees</b> in a sequence of reweighted datasets. In a given iteration, subjects that were misclassified in the previous iteration are weighted more highly than subjects that were correctly classified. Classifications from each of the <b>classification</b> <b>trees</b> in the sequence are combined through a weighted majority vote to produce a final classification. The authors' objective was to examine whether boosting improved the accuracy of <b>classification</b> <b>trees</b> for predicting outcomes in cardiovascular patients. Methods: We examined the utility of boosting <b>classification</b> <b>trees</b> for classifying 30 -day mortality outcomes in patients hospitalized with either acute myocardial infarction or congestive heart failure. Results: Improvements in the misclassification rate using boosted <b>classification</b> <b>trees</b> were at best minor compared to when conventional <b>classification</b> <b>trees</b> were used. Minor to modest improvements to sensitivity were observed, with only a negligible reduction in specificity. For predicting cardiovascular mortality, boosted <b>classification</b> <b>trees</b> had high specificity, but low sensitivity. Conclusions: Gains in predictive accuracy for predicting cardiovascular outcomes were less impressive than gains in performance observed in the data mining literature...|$|R
40|$|<b>Classification</b> <b>trees</b> {{based on}} imprecise probabilities provide an {{advancement}} of classical <b>classification</b> <b>trees.</b> The Gini Index is the default splitting criterion in classical <b>classification</b> <b>trees,</b> while in <b>classification</b> <b>trees</b> based on imprecise probabilities, {{an extension of}} the Shannon entropy has been introduced as the splitting criterion. However, the use of these empirical entropy measures as split selection criteria can lead to a bias in variable selection, such that variables are preferred for features other than their information content. This bias is not eliminated by the imprecise probability approach. The source of variable selection bias for the estimated Shannon entropy, as well as possible corrections, are outlined. The variable selection performance of the biased and corrected estimators are evaluated in a simulation study. Additional results from research on variable selection bias in classical <b>classification</b> <b>trees</b> are incorporated, implying further investigation of alternative split selection criteria in <b>classification</b> <b>trees</b> based on imprecise probabilities. ...|$|R
40|$|<b>Classification</b> <b>trees</b> are {{a popular}} {{statistical}} tool with multiple applications. Recent advancements of traditional <b>classification</b> <b>trees,</b> {{such as the}} approach of <b>classification</b> <b>trees</b> based on imprecise probabilities by Abellán and Moral (2004), effectively address their tendency to overfitting. However, another flaw inherent in traditional <b>classification</b> <b>trees</b> is not eliminated by the imprecise probability approach: Due to a systematic finite sample-bias in the estimator of the entropy criterion employed in variable selection, categorical predictor variables with low information content are preferred {{if they have a}} high number of categories. Mechanisms involved in variable selection in <b>classification</b> <b>trees</b> based on imprecise probabilities are outlined theoretically as well as by means of simulation studies. Corrected estimators are proposed, which prove to be capable of reducing estimation bias as a source of variable selection bias...|$|R
25|$|CTM allows {{modeling}} of hierarchical refinements in the <b>classification</b> <b>tree,</b> also called implicit dependencies.|$|E
25|$|Classification Trees {{in terms}} of the <b>Classification</b> <b>Tree</b> Method must not be {{confused}} with decision trees.|$|E
25|$|Prerequisites for {{applying}} the <b>classification</b> <b>tree</b> method (CTM) is the selection (or definition) {{of a system}} under test.|$|E
40|$|<b>Classification</b> <b>trees</b> {{have been}} {{successfully}} used in several application fields. However, continuous attributes cannot be used directly when building <b>classification</b> <b>trees,</b> but they must be first discretized with clustering techniques, which require some degree of subjectivity. We propose an approach to build <b>classification</b> <b>trees</b> that {{does not require the}} discretization of the continuous attributes. The approach is an extension of existing methods for building <b>classification</b> <b>trees</b> and is based on the information gain yielded by discrete and continuous attributes. Data from a software development case study are analyzed with both the proposed approach and C 4. 5 to show the approach's applicability and benefits over C 4. 5...|$|R
40|$|In this paper, {{we present}} {{empirical}} and theoretical results on <b>classification</b> <b>trees</b> for randomized response data. We considered a dichotomous sensitive response variable with the true status intentionally misclassified by the respondents using rules prescribed by a randomized response method. We assumed that <b>classification</b> <b>trees</b> are grown using the Pearson chi-square test as a splitting criterion, {{and that the}} randomized response data are analyzed using <b>classification</b> <b>trees</b> {{as if they were}} not perturbed. We proved that <b>classification</b> <b>trees</b> analyzing observed randomized response data and estimated true data have a one-to-one correspondence in terms of ranking the splitting variables. This is illustrated using two real data set...|$|R
40|$|The use of <b>classification</b> <b>trees</b> for {{modeling}} and predicting {{the passage of}} molecules through the blood−brain barrier was evaluated. The models were built and evaluated using a data set of 147 molecules extracted from the literature. In the first step, single <b>classification</b> <b>trees</b> were built and evaluated for their predictive abilities. In the second step, {{attempts were made to}} improve the predictive abilities using a set of 150 <b>classification</b> <b>trees</b> in a boosting approach. Two boosting algorithms, discrete and real adaptive boosting, were used and compared. High-predictive <b>classification</b> <b>trees</b> were obtained for the data set used, and the models could be improved with boosting. In the context of this research, discrete adaptive boosting gives slightly better results than real adaptive boosting...|$|R
25|$|Applying the <b>classification</b> <b>tree</b> method, the {{identification}} of test relevant aspects gives the classifications: User Privilege, Operation and Access Method.|$|E
25|$|When {{test design}} with the <b>classification</b> <b>tree</b> method is {{performed}} without proper test decomposition, classification trees can get large and cumbersome.|$|E
25|$|With a {{selected}} system under test, {{the first step}} of the <b>classification</b> <b>tree</b> method is the identification of test relevant aspects.|$|E
40|$|Multi-layer perceptrons {{and trained}} <b>classification</b> <b>trees</b> {{are two very}} {{different}} techniaues which have recentlv become popuiar. Given enough data and time, both methods are capable of performing arbitrary non-linear classification. These two techniques, which developed out of different research communities, have not been previously compared on real-world problems. We first consider the important differences between multi-layer perceptrons and <b>classification</b> <b>trees</b> and conclude {{that there is not}} enough theoretical basis for the clear-cut superiority of one technique over the other. For this reason, we performed a number of empirical tests on quite different problems in power system load forecasting and speaker-independent vowel identification. We compared the performance for classification and prediction in terms of accuracy outside the training set. In all cases, even with various sizes of training sets, the multi-layer perceptron performed as well as or better than the trained <b>classification</b> <b>trees.</b> We are confident that the univariate version of the trained <b>classification</b> <b>trees</b> do not perform as well as the multi-layer perceptron. More studies are needed, however, on the comparative performance of the linear combination version of the <b>classification</b> <b>trees.</b> I...|$|R
40|$|The goal of Speech Understanding Systems (SUS) is {{to extract}} {{meanings}} from {{a sequence of}} hypothetical words generated by a speech recognizer. Recently SUSs tend to rely on robust matchers to perform this task. This thesis describes a new method using <b>classification</b> <b>trees</b> acting as a robust matcher for speech understanding. <b>Classification</b> <b>trees</b> are used as a learning method to learn rules automatically from training data. This thesis investigates uses of <b>classification</b> <b>trees</b> in speech system and some general algorithms applied on <b>classification</b> <b>trees.</b> The linguistic approach requires more human time because of the overhead associated with handling {{a large number of}} rules, whereas the proposed approach eliminates the need to handcode and debug the rules. Also, this approach is highly resistant to errors by the speaker or by the speech recognizer by depending on some semantically important words rather than entire word sequence. Furthermore, by re-training <b>classification</b> <b>trees</b> on a new set of training data later, system improvement is done easily and automatically. The thesis discusses a speech understanding system built at McGill University using the DARPA-sponsored Air Travel Information System (ATIS) task as training corpus and testbed...|$|R
5000|$|CHAID (CHi-squared Automatic Interaction Detector). Performs {{multi-level}} splits when computing <b>classification</b> <b>trees.</b>|$|R
25|$|Grochtmann and Wegener {{presented}} their tool, the <b>Classification</b> <b>Tree</b> Editor (CTE) which supports both partitioning {{as well as}} test case generation.|$|E
25|$|In {{addition}} to Boolean dependency rules referring to classes of the <b>classification</b> <b>tree,</b> Numerical Constraints allow to specify formulas with classifications as variables, which will evaluate to the selected class {{in a test}} case.|$|E
25|$|Prioritized {{test case}} generation: It is {{possible}} to assign weights to {{the elements of the}} <b>classification</b> <b>tree</b> in terms of occurrence and error probability or risk. These weights are then used during test case generation to prioritize test cases. Risk-based and statistical testing is also available.|$|E
40|$|<b>Classification</b> <b>trees</b> {{are one of}} {{the most}} popular types of classifiers, with ease of {{implementation}} and interpretation being among their attractive features. Despite the widespread use of <b>classification</b> <b>trees,</b> theoretical analysis of their performance is scarce. In this paper, we show that a new family of <b>classification</b> <b>trees,</b> called dyadic <b>classification</b> <b>trees</b> (DCTs), are near optimal (in a minimax sense) for a very broad range of classification problems. This demonstrates that other schemes (e. g., neural networks, support vector machines) cannot perform significantly better than DCTs in many cases. We also show that this near optimal performance is attained with linear (in the number of training data) complexity growing and pruning algorithms. Moreover, the performance of DCTs on benchmark datasets compares favorably to that of standard CART, which is generally more computationally intensive and which does not possess similar near optimality properties. Our analysis stems from theoretica...|$|R
40|$|This paper {{reports on}} the {{experience}} in the applications of the classification-tree methodology to a real-life system. As {{a result of the}} study, we found the need to supplement the original method by introducing a new relation operator and a new technique for constructing <b>classification</b> <b>trees.</b> Our supplement helps to enforce the completeness of test cases and improve on the effectiveness of <b>classification</b> <b>trees...</b>|$|R
3000|$|For Step 3, <b>classification</b> <b>trees</b> are {{selected}} as the machine learning technique, with default Python sklearn.tree.DecisionTreeClassifier (...) settings applied.|$|R
25|$|The {{first step}} of the <b>classification</b> <b>tree</b> method now is complete. Of course, there are further {{possible}} test aspects to include, e.g. access speed of the connection, number of database records present in the database, etc. Using the graphical representation {{in terms of a}} tree, the selected aspects and their corresponding values can quickly be reviewed.|$|E
25|$|One way of {{modelling}} constraints {{is using}} the refinement mechanism in the <b>classification</b> <b>tree</b> method. This, however, {{does not allow for}} modelling constraints between classes of different classifications. Lehmann and Wegener introduced Dependency Rules based on Boolean expressions with their incarnation of the CTE. Further features include the automated generation of test suites using combinatorial test design (e.g. all-pairs testing).|$|E
25|$|With the {{addition}} of valid transitions between individual classes of a classification, classifications {{can be interpreted as}} a state machine, and therefore the whole <b>classification</b> <b>tree</b> as a Statechart. This defines an allowed order of class usages in test steps and allows to automatically create test sequences. Different coverage levels are available, such as state coverage, transitions coverage and coverage of state pairs and transition pairs.|$|E
40|$|The classification-tree method {{developed}} by Grochtmann and Grimm facilitates {{the identification of}} test cases from functional specifications via the construction of <b>classification</b> <b>trees.</b> Their method has been enhanced by Chen and Poon through the classification-tree construction and restructuring methodologies. We find, however that the restructuring algorithm by Chen and Poon is applicable only to certain types of <b>classification</b> <b>trees.</b> We introduce a new tree-restructuring algorithm to supplement their work. published_or_final_versio...|$|R
40|$|Various {{statistical}} classification methods, including discriminant analysis, logistic regression, and cluster analysis, {{have been}} used with antibiotic resistance analysis (ARA) data to construct models for bacterial source tracking (BST). We applied the statistical method known as <b>classification</b> <b>trees</b> to build a model for BST for the Anacostia Watershed in Maryland. <b>Classification</b> <b>trees</b> have more flexibility than other statistical classification approaches based on standard statistical methods to accommodate complex interactions among ARA variables. This article describes the use of <b>classification</b> <b>trees</b> for BST and includes discussion of its principal parameters and features. Anacostia Watershed ARA data are used to illustrate the application of <b>classification</b> <b>trees,</b> and we report the BST results for the watershed. Bacterial source tracking (BST) with antibiotic resistance analysis (ARA) has been conducted using various statistical classification models to identify sources of bacterial contamination of surface waters (3 – 5, 9 – 15). Isolates are obtained from fecal samples from known sources, such as humans, pets, livestock, and wildlife, and are tested for antibiotic resistance against a panel of antibiotics at different concentrations. Th...|$|R
3000|$|The {{result can}} be a large {{ensemble}} <b>classification</b> <b>trees</b> (e.g., 300) that one might call a Bayesian forest.p [...]...|$|R
2500|$|The <b>classification</b> <b>tree</b> method {{first was}} {{intended}} for the design and specification of abstract test cases. With the <b>classification</b> <b>tree</b> method for embedded systems, test implementation can also be performed. Several additional features are integrated with the method: ...|$|E
2500|$|In 2014, Berner started {{releasing}} its <b>classification</b> <b>tree</b> editor {{under the}} brand name [...]|$|E
2500|$|Recent {{enhancements}} to the <b>classification</b> <b>tree</b> method {{include the}} prioritized test case generation: It {{is possible to}} assign weights to {{the elements of the}} <b>classification</b> <b>tree</b> in terms of occurrence and error probability or risk. These weights are then used during test case generation to prioritize test cases. Statistical testing is also available (e.g. for wear and fatigue [...] tests) by interpreting the element weights as a discrete probability distribution.|$|E
30|$|Smith and Smith (2001) [6] {{proposed}} and applied nonparametric regression and <b>classification</b> <b>trees</b> as models to predict incident clearance time.|$|R
40|$|Model-based {{development}} uses {{modeling and}} simulation as essential means for specification, rapid prototyping, design, and realization of embedded systems. The classification-tree method complements model-based development with a formal approach for test case description and automation. This paper shows how “raw ” <b>classification</b> <b>trees</b> are transformed into complete <b>classification</b> <b>trees</b> using an extensible tool, the classification-tree transformer (CTT). This tool and its domain specific extensions are generated using the graph rewriting system PROGRES...|$|R
40|$|Objectives: Demonstration of the {{applicability}} of a framework called indirect classification to the example of glaucoma classification. Indirect classification combines medical a priori knowledge and statistical classification methods. The method is compared to direct classification approaches {{with respect to the}} estimated misclassification error. Methods: Indirect classification is applied using <b>classification</b> <b>trees</b> and the diagnosis of glaucoma. Misclassification errors are reduced by bootstrap aggregation. As direct classification methods linear discriminant analysis, <b>classification</b> <b>trees</b> and bootstrap aggregated <b>classification</b> <b>trees</b> are utilized in the problem of glaucoma diagnosis. Misclassification rates are estimated via 10 -fold cross-validation. Results: Indirect classification techniques reduce the misclassification error in the context of glaucoma classification compared to direct classification methods. Conclusions: Embedding a priori knowledge into statistical classification techniques can improve misclassification results. Indirect classification offers a framework to realize this combination...|$|R
