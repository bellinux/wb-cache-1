3|10000|Public
40|$|Establishment of {{a working}} group of {{regional}} experts in Marine Protected Areas (MPAs); inventory and status of existing MPAs; gap analysis;establishment of <b>common</b> <b>data</b> <b>requirements</b> and protocols;development of a regional action plan;training and capacity building; outreach activities; proposal f 0 r management of existing and creation of new MPAs...|$|E
40|$|Hazardous {{materials}} management includes interactions among materials, personnel, facilities, hazards, and processes of various groups within a DOE site`s environmental, safety & health (ES&H) and line organizations. Although each group {{is charged with}} addressing a particular aspect of these properties and interactions, the information it requires must be gathered into a coherent set of common data for accurate and consistent hazardous material management and regulatory reporting. It is these <b>common</b> <b>data</b> <b>requirements</b> which the Cradle-to-Grave Tracking and Information System (CGTIS) is designed to satisfy. CGTIS collects information at {{the point at which}} a process begins or a material enters a facility, and maintains that information, for hazards management and regulatory reporting, throughout the entire life-cycle by providing direct on-line links to a site`s multitude of data bases to bring information together into one common data model...|$|E
40|$|The aim of {{the project}} was to build EUROMOD, a tax-benefit microsimulation model {{covering}} all 15 Member States of the European Union. This has been achieved, and baseline results are available for 14 countries. (Validated results for Sweden will appear shortly.) EUROMOD has been used for a number of policy-related exercises ranging from studies of the relationship of public spending on social benefits to poverty and the implications of a common European minimum pension, to the impact of welfare benefits on work incentives and the consequences of non-indexation of taxes and contributions. In addition, the model is ready to be used {{for a wide range of}} new applications. Not only can it be used to explore the impact of prospective (and hypothetical) changes in social and fiscal policy on poverty and inequality; it can also estimate the cost of reforms, provide options for financing mechanisms, and establish the effect of the reforms in other dimensions such as the work incentives of household members and any implied redistribution within the household. In many ways, EUROMOD is ahead it its time. When the project first started in 1998 (and when the idea was first conceived in 1996) the priorities set at the Lisbon European Council could not have been fully anticipated. It is now clear that the project was timely. EUROMOD is ready to play a role in analysing changes in social and fiscal policies proposed by Member States with reference to agreed benchmarks for the reduction of poverty and social exclusion. The project final report describes in some detail the process of model construction. It was a very complex project that was more demanding for all concerned than could have been anticipated. In some respects it was more akin to an engineering enterprise than a social science research project. In building EUROMOD, particular emphasis has been placed on 2 ̆ 022 transparency of methods: it is therefore open to critiques of the approach as a whole, as well as criticism and suggestion on matters of detail; 2 ̆ 022 designing a model that is flexible and adaptable: to make the range of uses as wide as possible and to maximise the length of its useful life; 2 ̆ 022 consistency and comparability across countries: developing harmonisation of methods, assumptions and input and output concepts is a major part of building an integrated European model. Concretely, it involved: 2 ̆ 022 identifying common structural characteristics in national policies 2 ̆ 022 identifying <b>common</b> <b>data</b> <b>requirements</b> 2 ̆ 022 parameterising and generalising as many aspects of the model as possible, including the definitions of the income base and unit of assessment or entitlement for each tax and benefit, the effective equivalence scales inherent in social benefit payments, and the output income measure. This approach not only allows each system to be modelled in a manner that is comparable to existing national practice, it also provides the model user with a much greater range of choice and greater flexibility than 2 ̆ 013 we believe 2 ̆ 013 is available in any other existing tax benefit model. Before the project began, the degree of experience and expertise with tax-benefit modelling in Member States varied greatly. As is well known, the tax and transfer systems also vary widely in underlying philosophy, as well as in current structure and size. The national sources of microdata with which to build the model were not equally suited to the task. One of the project 2 ̆ 019 s most significant achievements is its success in bringing tax-benefit modelling capacity in all Member States up to the level of best practice in the EU...|$|E
40|$|In {{this paper}} we {{describe}} an approach and system for managing enterprise semi-structured data that is high-throughput, nimble, and scalable. We present the NETMARK system, which {{provides for a}} “schemaless” way of managing semi-structured documents. We describe in particular detail the unique underlying data storage approach and efficient query processing mechanisms given this storage system. We present an extensive benchmark evaluation of the NETMARK system and also compare it with related XML management systems. At {{the heart of the}} approach is the philosophy of a focus on most <b>common</b> <b>data</b> management <b>requirements</b> in the enterprise, and not burdening users and application developers with unnecessary complexity and formal schemas. 1...|$|R
40|$|Mobile Crowd Sensing (MCS) {{is a new}} {{paradigm}} of sensing, which can achieve a flexible and scalable sensing coverage with a low deployment cost, by employing mobile users/devices to perform sensing tasks. In this work, we propose a novel MCS framework with data reuse, where multiple tasks with <b>common</b> <b>data</b> <b>requirement</b> can share (reuse) the <b>common</b> <b>data</b> with each other through an MCS platform. We study the optimal assignment of mobile users and tasks (with data reuse) systematically, under both information symmetry and asymmetry, {{depending on whether the}} user cost and the task valuation are public information. In the former case, we formulate the assignment problem as a generalized Knapsack problem and solve the problem by using classic algorithms. In the latter case, we propose a truthful and optimal double auction mechanism, built upon the above Knapsack assignment problem, to elicit the private information of both users and tasks and meanwhile achieve the same optimal assignment as under information symmetry. Simulation results show by allowing data reuse among tasks, the social welfare can be increased up to 100 ~ 380 %, comparing with those without data reuse. We further show that the proposed double auction is not budget balance for the auctioneer, mainly due to the data reuse among tasks. To this end, we further introduce a reserve price into the double auction (for each data item) to achieve a desired tradeoff between the budget balance and the social efficiency. Comment: This manuscript serves as the online technical report for the paper published in IEEE GLOBECOM 201...|$|R
40|$|This report compiles {{information}} about recent {{progress in the}} application of computer simulation modeling to planning and management of recreation use, particularly in parks and wilderness. Early modeling efforts are described in a chapter that provides an historical perspective. Another chapter provides an overview of modeling options, <b>common</b> <b>data</b> input <b>requirements,</b> and useful model outputs. The bulk of the report consists of case studies that illustrate a broad array of recreational situations and management applications for simulation modeling. A final chapter describes some future directions for modeling work. Although simulation of recreation use is already a tool for planning and management, its utility could be greatly enhanced with further work in software development, increased understanding of appropriate methodologies, and greater attention to model verification and validation...|$|R
40|$|Abstract _________________________________________________ Cole, David N., comp. 2005. Computer {{simulation}} {{modeling of}} recreation use: current status, case studies, and future directions. Gen. Tech. Rep. RMRS-GTR- 143. Fort Collins, CO: U. S. Department of Agriculture, Forest Service, Rocky Mountain Research Station. 75 p. This report compiles information about recent {{progress in the}} application of computer simulation modeling to planning and management of recreation use, particularly in parks and wilderness. Early modeling efforts are described in a chapter that provides an historical perspective. Another chapter provides an overview of modeling options, <b>common</b> <b>data</b> input <b>requirements,</b> and useful model outputs. The bulk of the report consists of case studies that illustrate a broad array of recreational situations and management applications for simulation modeling. A final chapter describes some future directions for modeling work. Although simulation of recreation use is already a tool for planning and management, its utility could be greatly enhanced with further work in software development, increased understanding of appropriate methodologies, and greater attention to model verification and validation...|$|R
40|$|The {{paper is}} {{motivated}} by a present lack of clear model performance guidelines for shelf sea and estuarine modellers seeking to demonstrate to clients and end users that a model is fit for purpose. It addresses the common problems associated with data availability, errors, and uncertainty and examines the model build process, including calibration and validation. It also looks at <b>common</b> assumptions, <b>data</b> input <b>requirements,</b> and statistical analyses {{that can be applied}} to assess the performance of models of estuaries and shelf seas. Specifically, it takes account of inherent modelling uncertainties and defines metrics of performance based on practical experience. It is intended as a reference point both for numerical modellers and for specialists tasked with interpreting the accuracy and validity of results from hydrodynamic, wave, and sediment models...|$|R
5000|$|In United States {{military}} contracts, {{the contract}} <b>data</b> <b>requirements</b> list (CDRL, pronounced SEE-drill) {{is a list}} of authorized <b>data</b> <b>requirements</b> for a specific procurement that forms a part of the contract.|$|R
30|$|Apart from {{fulfilling}} the above properties, an ideal biometric cryptosystem shall not cause {{a decrease in}} biometric performance {{with respect to the}} corresponding unprotected system [19]. The vast majority of existing techniques do not satisfy desired template protection requirements in practice, mostly resulting in a trade-off between privacy protection and biometric performance [20]. The incorporation of multiple biometric characteristics to biometric cryptosystems has been found to improve biometric performance [7], while the protection of multi-biometric templates is especially crucial as they contain information regarding multiple characteristics of the same subject [21]. In contrast to conventional biometric systems, where fusion may take place at score or decision level [7], with respect to template protection schemes, feature-level fusion has been identified as most suitable. A separate storage of two or more protected biometric templates would enable parallelized attacks. In contrast, a single protected template, which has been obtained by means of feature-level fusion, is expected to improve privacy protection, since the fused template comprises more biometric information [21]. This is analogous to an access control system which requires multiple low strength (few bits) keys, where each key can be attacked individually. Such a system is less secure than one which uses a single key with a larger number of bits. Obviously, the development of multi-biometric cryptosystems is accompanied by further issues such as <b>common</b> <b>data</b> representation, storage <b>requirement,</b> or feature alignment [22].|$|R
30|$|The V nodes <b>data</b> <b>requirements</b> are user need specific, P nodes <b>data</b> <b>requirements</b> are {{organizational}} specific {{whereas the}} S node <b>data</b> <b>requirements</b> are administration, system support and management specific. The replication granularity for V node is record level dump task specific, for P node first complete dump is copied and further differential dump is used based on check pointing and for S nodes complete {{copy of the}} database dump is used among them.|$|R
5000|$|Standardisation of {{documents}} and electronic <b>data</b> <b>requirements</b> ...|$|R
50|$|The CDRL {{identifies}} what data {{products are}} to be formally delivered to the government by a contractor, as well as when and possibly how (e.g. format and quantity) they {{are to be}} delivered. The list typically consists of a series of individual data items, each of which is recorded on a Data Item form (DD Form 1423) containing the tailored <b>data</b> <b>requirements</b> and delivery information. The CDRL is the standard format for identifying potential <b>data</b> <b>requirements</b> in a solicitation, and deliverable <b>data</b> <b>requirements</b> in a contract. The purpose of the CDRL is to provide a standardized method of clearly and unambiguously delineating the government's minimum essential data needs. The CDRL groups all of the <b>data</b> <b>requirements</b> in a single place rather than having them scattered throughout the solicitation or contract.|$|R
40|$|An {{effort to}} define a <b>common</b> <b>data</b> format for voting systems should begin with a data model that {{specifies}} the relevant concepts. To accelerate adoption and minimize conflicts, such a model should define the smallest set of concepts needed by the desired functionality. We present a small data model that suffices to cover the election definition and vote <b>data</b> reporting <b>requirements</b> of VVSG 2. 0, {{with the exception of}} reporting for ranked order contests. This model may be used as the basis for continued work, in whole or in part, without restriction. ...|$|R
5000|$|<b>Data</b> <b>requirements</b> {{are small}} and no {{calibration}} is necessary 3 ...|$|R
5000|$|The {{conversion}} strategy needs to carefully examine the <b>data</b> <b>requirements</b> ...|$|R
50|$|In December 2010 the ECB the Governing Council of the ECB {{announced}} {{its decision to}} establish loan level <b>data</b> <b>requirements</b> (referred to as the ABS initiative). As of the summer of 2012 the published <b>data</b> <b>requirements</b> {{will have to be}} complied with before a financial institution can pledge ABS as collateral with the ECB.|$|R
40|$|The {{session on}} {{techniques}} {{and resources for}} storm-scale numerical weather prediction are reviewed. The recommendations of this group are broken down into three area: modeling and prediction, <b>data</b> <b>requirements</b> in support of modeling and prediction, and data management. The current status, modeling and technological recommendations, <b>data</b> <b>requirements</b> in support of modeling and prediction, and data management are addressed...|$|R
30|$|In engineering, {{different}} groups of synchrophasor applications have different <b>requirements</b> on synchrophasor <b>data,</b> such as data rate, data volume, data quality, and data security. It {{is important to understand}} the variety of synchrophasor applications and their <b>data</b> <b>requirements.</b> It is also advantageous to develop a set of consistent and quantifiable <b>data</b> <b>requirements</b> for the applications, which help existing and new users learn the applications’ capability and suitability in their particular scenarios. The <b>data</b> quality <b>requirements</b> for synchrophasor applications are investigated in this paper.|$|R
5000|$|Amendment 21-38 of Part 21 was {{published}} May 26, 1972. [...] This {{was the next}} rule change to affect PMAs. This rule eliminated the incorporation by reference of type certification requirements in favor of PMA-specific <b>data</b> submission <b>requirements.</b> This change established the separate process and separate <b>requirements</b> for <b>data</b> that must be submitted by an applicant for a PMA (prior to this there was no explicit distinction between the application <b>data</b> <b>requirements</b> for type certificated products and the <b>data</b> <b>requirements</b> for PMAed articles).|$|R
5000|$|Each CDRL {{data item}} should be linked {{directly}} to {{statement of work}} (SOW) tasks and managed by the program office <b>data</b> manager. <b>Data</b> <b>requirements</b> can also be identified in the contract via special contract clauses (e.g., DFARS), which define special data provisions such as rights in data, warranty, etc. SOW guidance of MIL-HDBK-245D describes the desired relationship: [...] "Work requirements should be specified in the SOW, and all <b>data</b> <b>requirements</b> for delivery, format, and content {{should be in the}} contract <b>data</b> <b>requirements</b> list in conjunction with the appropriate Data Item Description (DID) respectively, with none of the requirements restated {{in other parts of the}} contract." ...|$|R
5000|$|Data models, i.e. <b>data</b> <b>requirements</b> {{expressed}} as a documented data model of some sort ...|$|R
2500|$|Since such a bus {{architecture}} cannot {{keep up with}} the <b>data</b> <b>requirements</b> of the LHC experiments, ...|$|R
5000|$|Experience with IDEF1 {{revealed}} that the translation of information requirements into database designs was more difficult than had originally been anticipated. The most beneficial value of the IDEF1 information modeling technique was its ability to represent data independent of how those data were to be stored and used. It provided data modelers and data analysts {{with a way to}} represent <b>data</b> <b>requirements</b> during the requirements-gathering process. This allowed designers to decide which DBMS to use after the nature of the <b>data</b> <b>requirements</b> was understood and thus reduced the [...] "misfit" [...] between <b>data</b> <b>requirements</b> and the capabilities and limitations of the DBMS. The translation of IDEF1 models to database designs, however, proved to be difficult.|$|R
40|$|Soil loss {{estimation}} is {{a necessary}} adjunct {{to the design of}} appropriate conservation practices and land development in general. Current soil loss estimation techniques are cumbersome, have a large <b>data</b> <b>requirement</b> and are inflexible to changing agricultural practices. The Soil Loss Estimation Model for Southern Africa (SLEMSA) is examined {{in the light of the}} special criteria necessary for land development planning. SLEMSA meets most objectives more sucessfully than other methods but retains weaknesses such as lack of <b>common</b> agricultural baseline <b>data</b> that limits its potential use...|$|R
40|$|Essential {{ancillary}} <b>data</b> <b>requirements</b> for the {{validation of}} surrogate measurements of bedload: non-invasive bed material grain size and definitive measurements of bedload flux This item was submitted to Loughborough University's Institutional Repository by the/an author. Citation: REID, I [...] et al, 2010. Essential ancillary <b>data</b> <b>requirements</b> for the validation of surrogate measurements of bedload: non-invasive bed material grain size and definitive measurements of bedload flux. Published online a...|$|R
30|$|<b>Data</b> <b>requirements</b> are {{prescribed}} by the REACH Regulation and cannot be tightened by demands {{in the field}} of IT.|$|R
5000|$|The EDRC Project of FIATECH Capturing Equipment <b>Data</b> <b>Requirements</b> Using ISO 15926 and Assessing Conformance. Example {{data and}} videos.|$|R
5000|$|The {{documentation}} of the <b>data</b> <b>requirements</b> and structural business process (activity) rules. In DoDAF V1.5, this was the OV-7.|$|R
5000|$|Logical Data Model (OV-7) : Documentation of the <b>data</b> <b>requirements</b> and {{structural}} business process {{rules of the}} Operational View.|$|R
30|$|A {{detailed}} description of the ForWaDy model including its general <b>data</b> <b>requirements</b> are provided in Seely et al. (2015, 1997).|$|R
30|$|User {{parameters}}: These parameters reflect user’s <b>data</b> <b>requirements</b> at {{the application}} layer such as required data rate and high priority data.|$|R
40|$|Earth science {{research}} and application <b>requirements</b> for multispectral <b>data</b> {{have often been}} driven by currently available remote sensing technology. Few parametric studies exist that specify data required for certain applications. Consequently, <b>data</b> <b>requirements</b> are often defined based on the best data available or on what has worked successfully in the past. Since properites such as spatial resolution, swath width, spectral bands, signal-to-noise ratio (SNR), data quantization, and band-to-band registration drive sensor platform and spaceraft system architecture and cost, analysis of these criteria is important to objectively optimize system design. Remote sensing <b>data</b> <b>requirements</b> are also linked to calibration and characterization methods. Parameters such as spatial resolution, radiometric accuracy, and geopositional accuracy affect the complexity and cost of calibration methods. However, there are few studies that quantify the true accuracies required for specific problems. As calibration methods and standards are proposed, {{it is important that}} they be tied to well-known <b>data</b> <b>requirements.</b> The Application Research Toolbox (ART) developed at Stennis Space Center provides a simulation-based method for multispectral <b>data</b> <b>requirements</b> development. The ART produces simulated data sets from hyperspectral data through band synthesis. Parameters such as spectral band shape and width, SNR, data quantization, spatial resolution, and band-to-band registration can be varied to create many different simulated data products. Simulated data utility can then be assessed for different applications so that requirements can be better understood. This paper describes the ART and its applicability for rigorously deriving remote sensing <b>data</b> <b>requirements...</b>|$|R
40|$|Any {{significant}} real-world {{application of}} mobile augmented reality {{will require a}} large model of location-bound data. While it may appear that a natural approach is to develop application-specific data formats and management strategies, {{we have found that}} such an approach actually prevents reuse of the data and ultimately produces additional complexity in developing the application. In contrast we describe a three-tier architecture to manage a <b>common</b> <b>data</b> model for a set of applications. It is inspired by current Internet application frameworks and consists of a central storage layer using a <b>common</b> <b>data</b> model, a transformation layer responsible for filtering and adapting the <b>data</b> to the <b>requirements</b> of a particular applications on request, and finally of the applications itself. We demonstrate our architecture in a scenario consisting of two multi-user capable mobile AR applications for collaborative navigation and annotation in a city environment...|$|R
40|$|Abstract. In this paper, {{we address}} the problem of eliciting, {{communicating}} and validating the static <b>data</b> <b>requirements</b> of a software engineering project, while improving the end-user involvement. For this purpose, given an environment for which electronic forms are a privileged way to exchange information and stakeholders familiar with form-based (computer) interaction, we propose to use form-based user-drawn interfaces as a two-way channel to interactively capture and validate static <b>data</b> <b>requirements</b> with end-users, by specializing and integrating standard techniques to help acquire data specifications from existing artifacts. Since the main principles of our approach are already presented in [1], we here focus on discussing two fundamental aspects of this research, namely the means to make end-users major stakeholders in the <b>data</b> <b>requirements</b> process, and the challenges facing the validation of such a transversal research...|$|R
5000|$|Apache OFBiz is a {{framework}} that provides a <b>common</b> <b>data</b> model and a rich set of business process.All applications are built around a common architecture using <b>common</b> <b>data,</b> logic and process components.Beyond the framework itself, Apache OFBiz offers functionality including: ...|$|R
