17|10000|Public
50|$|Sabdatharavali (A star <b>cluster</b> <b>of</b> <b>words)</b> is a Malayalam {{dictionary}} {{having more}} than 1800 pages and considered as the most authentic Malayalam dictionary of the 20th century. The {{first part of this}} dictionary authored by Sreekanteswaram Padmanabha Pillai was published in 1918.|$|E
5000|$|Generally, {{a written}} symbol or spoken word is {{considered}} the sign, but a <b>cluster</b> <b>of</b> <b>words</b> {{can also be considered}} signs with a referent idea. However, not every combination of words has meaning or a referent idea. Some combinations of words are nonsensical and constitute what some call [...] "word salad". Linguist Noam Chomsky is famous for his phrase, [...] "Colorless green ideas sleep furiously", which is used to demonstrate the idea between syntax and meaning.|$|E
5000|$|Grantland Rice wrote: These are the gladdest of {{possible}} words,"Yale Was Unable to Score"Sweeter than {{song from the}} clear singing birds,"Yale Was Unable to Score"Words that are sweeter than nectar and honeySweeter by far than the jungle of money,Words that are roseate, golden and sunny,"Yale Was Unable to Score"Find in the classics another such phrase,"Commodores Draw With the Blue,"Phrase that is all to the ripple and razzle,Canonized <b>cluster</b> <b>of</b> <b>words</b> on the dazzle,Words that have Emerson smashed to a frazzle"Commodores Draw with the Blue" ...|$|E
5000|$|The {{output of}} a word-sense {{induction}} algorithm is a <b>clustering</b> <b>of</b> {{contexts in which}} the target word occurs or a <b>clustering</b> <b>of</b> <b>words</b> related to the target word. Three main methods have been proposed in the literature: ...|$|R
40|$|Often, the {{classification}} <b>of</b> <b>words</b> {{does not go}} beyond "difficult" (i. e., infrequent, late-learned, nonimageable, etc.) or "easy" (i. e., frequent, early-learned, imageable, etc.) words. In the present study, we used a latent cluster analysis to divide 703 Dutch words with scores for eight word properties into seven <b>clusters</b> <b>of</b> <b>words.</b> Each <b>cluster</b> represents a group <b>of</b> <b>words</b> that share a particular configuration <b>of</b> <b>word</b> properties. This model was empirically validated with three data sets from Grades 2 to 4 children who made either a lexical decision task or a use decision task with a selection <b>of</b> the <b>words.</b> Significant {{differences were found between}} the <b>clusters</b> <b>of</b> <b>words</b> within the three data sets. Implications for further study and for practice are discussed...|$|R
50|$|In natural {{language}} processing, Brown clustering or IBM clustering {{is a form}} <b>of</b> hierarchical <b>clustering</b> <b>of</b> <b>words</b> based on the contexts in which they occur, proposed by Peter Brown, Vincent Della Pietra, Peter deSouza, Jennifer Lai, and Robert Mercer of IBM {{in the context of}} language modeling. The intuition behind the method is that a class-based language model (also called cluster -gram model), i.e. one where probabilities <b>of</b> <b>words</b> are based on the classes (<b>clusters)</b> <b>of</b> previous <b>words,</b> is used to address the data sparsity problem inherent in language modeling.|$|R
5000|$|She {{was educated}} at Sidwell Friends School, and {{graduated}} in 1980. From there {{she went to}} Yale University and graduated with a bachelor's degree in 1984. She studied poetry at Boston University under Derek Walcott and got her Master's in 1987. Her mother said to her, [...] "That poet you love, Derek Walcott, is teaching at Boston University. Why don't you apply?" [...] Alexander originally entered studying fiction writing, but Walcott looked at her diary and saw the poetry potential. Alexander said, [...] "He gave me a huge gift. He took a <b>cluster</b> <b>of</b> <b>words</b> and he lineated it. And I saw it." ...|$|E
50|$|Several {{approaches}} {{have been proposed}} {{based on the information}} contents of the resulting blocks: matrix-based approaches such as SVD and BVD, and graph-based approaches. Information-theoretic algorithms iteratively assign each row to a cluster of documents and each column to a <b>cluster</b> <b>of</b> <b>words</b> such that the mutual information is maximized. Matrix-based methods focus on the decomposition of matrices into blocks such that the error between the original matrix and the regenerated matrices from the decomposition is minimized. Graph-based methods tend to minimize the cuts between the clusters. Given two groups of documents d1 and d2, the number of cuts can be measured as the number of words that occur in documents of groups d1 and d2.|$|E
5000|$|Others {{have offered}} {{perspectives}} on Mugambi's views on liberation and reconstruction. In attempting a definition, the late Hannah Wangeci Kinoti {{explained that the}} idea of reconstruction assumes that there is a framework which was previously there. She {{went on to say that}} [...] "a <b>cluster</b> <b>of</b> <b>words</b> associated with the verb reconstruct should quicken our vision of asking the Church in African to rise up and do more, purposefully and decisively." [...] She suggested that the concept of reconstruction implies a process of [...] "review and then move" [...] - to create something more suitable to the prevailing environment. Other terms can be invoked: rebuild, reassemble, re-establish, recreate, reform, renovate, regenerate, remake, remodel, restore, or re-organise.|$|E
40|$|Categorization {{dynamics}} as <b>clustering</b> <b>of</b> <b>words</b> in word relation is {{studied by}} a constructive approach which is suit for inquiry of evolutionary linguistic with dynamic view <b>of</b> language. <b>Word</b> meaning {{is represented by}} relationship among words to some extent and the relationship should be derived from way of usage of language. Based on this usage-base view, an algorithm to evaluate word relationship is defined. Using the algorithm, cluster structure and its dynamics <b>of</b> <b>words</b> are shown in a model with communicating artificial agents. The relevance <b>of</b> <b>clustering</b> with categorization is discussed. Submission Type: regular paper Topic Areas: Evolutionary Computational Linguistics Author of Record: Takashi Hashimoto (takashi@brain. riken. go. jp) Under consideration for other conferences (specify) ? submission to other conference(s) :none Modeling Categorization Dynamics through Conversation by Constructive Approach Abstract Categorization dynamics as <b>clustering</b> <b>of</b> <b>words</b> in word relation is stu [...] ...|$|R
40|$|This paper {{describes}} a data-driven method for hierarchical <b>clustering</b> <b>of</b> <b>words</b> and <b>clustering</b> <b>of</b> multiword compounds. A large vocabulary <b>of</b> English <b>words</b> (70, 000 words) is clustered bottom-up, {{with respect to}} corpora {{ranging in size from}} 5 million to 50 million words, using mutual information as an objective function. The resulting hierarchical <b>clusters</b> <b>of</b> <b>words</b> are then naturally transformed to a bit-string representation <b>of</b> (i. e. <b>word</b> bits for) all the words in the vocabulary. Evaluation <b>of</b> the <b>word</b> bits is carried out through the measurement of the error rate of the ATR Decision-Tree Part-Of-Speech Tagger. The same clustering technique is then applied to the classification of multiword compounds. In order to avoid the explosion of the number of compounds to be handled, compounds in a small subclass are bundled and treated as a single compound. Another merit of this approach is that we can avoid the data sparseness problem which is ubiquitous in corpus statistics. The quality of one of the obtained compound classes is examined and compared to a conventional approach...|$|R
40|$|In {{content-based}} filtering systems, {{content of}} items {{is used to}} recommend new items to the users. It is usually represented by words in natural language where meanings <b>of</b> <b>words</b> are often ambiguous. We studied <b>clustering</b> <b>of</b> <b>words</b> based on their semantic similarity. Then we used word clusters to represent items for recommending new items by content-based filtering. In the paper we present our empirical results...|$|R
5000|$|The {{following}} is an excerpt from a speech titled [...] "Americanism" [...] given by Warren G. Harding in 1920 regarding aid to European nations devastated by World War I. Terms bolded are those a critic might associate with the key term [...] "Old World stabilization" [...] and terms underlined are those a critic might associate with the key term [...] "stabilize America."It's time to idealize, but it's very practical {{to make sure our}} own house is in perfect order before we attempt the miracle of Old World stabilization. Call it the selfishness of nationality if you will. I think it's an inspiration to patriotic devotion to safeguard America first, to stabilize America first, to prosper America first, to think of America first, to exalt America first, to live for and revere America first. Let the internationalist dream and the Bolshevist destroy.In this example, the critic would examine the <b>cluster</b> <b>of</b> <b>words</b> and phrases around the key term [...] "Old world stabilization" [...] in order to find certain patterns. Terms like idealize, miracle, dream, and attempt depict reconstruction through aid as an unproven strategy; internationalist and Bolshevist characterize supporters of aid as foreign and Communist; and the ending word 'destroy' implies a disastrous consequence for the proposal. In all, a critic can assume that these clusters work together to present aid for European reconstruction as an irresponsible and un-American plan.|$|E
40|$|Stemming or suffix stripping, an {{important}} part of the modern Information Retrieval systems, is to find the root word (stem) out of a given <b>cluster</b> <b>of</b> <b>words.</b> Existing algorithms targeting this problem have been developed in a haphazard manner. In this work, we model this problem as an optimization problem. An Integer Program is being developed to overcome the shortcomings of the existing approaches. The sample results of the proposed method are also being compared with an established technique in the field for English language. An AMPL code for the same IP has also been given. Comment: 14 pages, 4 table...|$|E
40|$|Abstract—Topic models {{provide a}} {{convenient}} way to analyze large of unclassified text. A topic contains a <b>cluster</b> <b>of</b> <b>words</b> that frequently occur together. A topic modeling can connect words with similar meanings and distinguish between uses of words with multiple meanings. This paper provides two categories {{that can be}} under the field of topic modeling. First one discusses the area of methods of topic modeling, which has four methods that can be considerable under this category. These methods are Latent semantic analysis (LSA), Probabilistic latent semantic analysis (PLSA), Latent Dirichlet allocation (LDA), and Correlated topic model (CTM). The second category is called topic evolution models, which model topics by considering a...|$|E
40|$|Polysemy {{is one of}} {{the major}} causes of {{difficulties}} in se- mantic <b>clustering</b> <b>of</b> <b>words</b> in a corpus. In this pal) er, we first; give a definition of polysemy from the viewpoint <b>of</b> <b>clustering</b> and then, based on this detinition, we prol) ose a clustering method which reeogniscs verbal 1) olysenfics fi'om a textual corpus. The results of experi,nensdemonstrate t;hc cffeetivencss of the prol) osed method...|$|R
40|$|The article {{reports a}} corpus {{study of the}} {{parliamentary}} proceedings of the 42 nd Bulgarian Parliament. On the basis of keyword lists the salience <b>of</b> <b>words</b> and concepts is explored. Associations are sought between objects and names established via specific <b>clustering</b> <b>of</b> <b>words</b> in the corpus. Conversely, prominent names and themes from public life are examined through keywords and frequencies in the parliamentary minutes. The research proceeds from the assumption that <b>clustering</b> <b>of</b> lexis of high frequency can be revealing about prominent tendencies in social life...|$|R
40|$|We {{propose a}} new method of {{classifying}} documents into categories. We define for each category a finite mixture model based on soft <b>clustering</b> <b>of</b> <b>words.</b> We treat {{the problem of}} classifying documents as that of conducting statistical hypothesis testing over finite mixture models, and employ the EM algorithm to efficiently estimate parameters in a finite mixture model. Exper- imental results indicate that our method outperforms existing methods...|$|R
3000|$|The {{center of}} the MDS space is {{inhabited}} by a <b>cluster</b> <b>of</b> <b>words</b> that relate to Schwartz’ core political value of [...] "free enterprise", with the words, market, fiscal, free, people, work, hard frequently co-occurring. Here the cluster reflects support of a free market and the wish for fiscal discipline. Such fiscal conservatism and free market attitudes are characterized by statements such as [...] "I believe in a free market society which enables hard work to equal success". The {{lower end of the}} cluster leans towards the patriotism/traditional morality cluster. The term military is often used in conjunction with economic statements even though, conceptually, it would fit better into the following cluster.|$|E
30|$|We use {{a set of}} US patents issued between 1976 and 2010 [68]. We {{analyze the}} {{appearance}} of words in their titles. Whenever two or more words appear in a title of a patent we create a link between them {{at the moment when}} the patent was issued. To improve readability we filter out stopwords and the generic frequent words: “method,” “device” and “apparatus.” Our video (Figure 2 D) shows that {{at the beginning of the}} period techniques related to “engine” and “combustion” were popular, and later start to cluster together with “motor” and “vehicle.” Another cluster is sparked by patents about “magnetic” “recording” and “image” “processing.” It merges with a <b>cluster</b> <b>of</b> <b>words</b> related to “semiconductor” and “liquid” “crystal” to form the largest cluster of connected keywords at the end of the period.|$|E
40|$|Studies of Russian, Polish, and Lithuanian {{language}} learners converge on {{the finding that}} morphological features of nouns are first generalized to word clusters of high morpho-phonological similarities such as diminutives, that grammatical categorisation is are more easily applied to novel words that fall into these clusters. The present thesis explores whether the facilitating effect of diminutives on the acquisition of complex noun morphology can be extended to Serbian, a south Slavic language, morphologically similar to Russian and Polish. Specifically, the thesis explores the role of parameters responsible for the obtained diminutive advantage: high frequency of a particular <b>cluster</b> <b>of</b> <b>words</b> in child-directed speech (CDS) and morpho-phonological homogeneity within this cluster. A corpus analysis {{of the distribution of}} diminutives in Serbian CDS indicated a rather unexpected difference in frequency relative to Russian and Polish CDS, despite the high similarity of the diminutive derivation across these three Slavic languages. Out {{of the total number of}} nouns in Serbian CDS only 7...|$|E
40|$|In {{this paper}} {{we present a}} word {{encoding}} and clustering technique that groups web documents based on the importance <b>of</b> the <b>words</b> that appear in the documents. We use a two level self-organizing map architecture to generate <b>clusters</b> <b>of</b> <b>words</b> and documents. We propose that by capturing <b>word</b> importance information <b>of</b> <b>words,</b> similar documents can be then clustered to assist in web document retrieval. A web document retrieval system is presented to demonstrate how this approach could be integrated into web search. 1...|$|R
40|$|Categorization {{dynamics}} as the <b>clustering</b> <b>of</b> <b>words</b> in word relation is {{studied by}} a constructive approach which is suited to inquire evolutionary linguistics with dynamical view on language. Word meaning {{is represented by}} relationship among words. The relationship should be derived from usage of language. Being founded on this usage-based view, we define an algorithm to evaluate word relationship. Using the algorithm, cluster structure and its dynamics <b>of</b> <b>words</b> are shown in a model with communicating artificial agents. The relevance <b>of</b> <b>clustering</b> with linguistic categorization is discussed...|$|R
40|$|We {{present a}} novel fully {{unsupervised}} algorithm for POS induction from plain text, {{motivated by the}} cognitive notion of prototypes. The algorithm first identifies landmark <b>clusters</b> <b>of</b> <b>words,</b> serving as the cores of the induced POS categories. The rest <b>of</b> the <b>words</b> are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task. ...|$|R
40|$|Similarities among words affect {{language}} acquisition and processing in a multi-relational way barely {{accounted for in}} the literature. We propose a multiplex network representation of word similarities in a mental lexicon as a natural framework for investigating large-scale cognitive patterns. Our model accounts for semantic, taxonomic, and phonological interactions and identifies a <b>cluster</b> <b>of</b> <b>words</b> of higher frequency, easier to identify, memorise and learn and with more meanings than expected at random. This cluster emerges around age 7 yr through an explosive transition not reproduced by null models. We relate this phenomenon to polysemy, i. e. redundancy in word meanings. We show that the word cluster acts as a core for the lexicon, increasing both its navigability and robustness to degradation in cognitive impairments. Our findings provide quantitative confirmation of existing psycholinguistic conjectures about core structure in the mental lexicon {{and the importance of}} integrating multi-relational word-word interactions in suitable frameworks. Comment: 11 pages, 4 figures and 1 tabl...|$|E
40|$|Ioulios Polydeukes, more {{commonly}} known as Pollux, was a Greek sophist and lexicographer active in the closing decades of the second century a. d. His Onomasticon {{is one of the}} most important lexicographical texts of the Imperial period. It is essentially a set of word lists dedicated to collecting clusters of related words on topics from a vast range of different areas of intellectual activity and everyday life. The text survives only in epitomized form, and shows signs of interpolation as well as abridgement. Nevertheless, the consensus is that the bulk of what survives is Pollux’ own work, and that reading it in Eric Bethe's Teubner edition gives an accurate cumulative impression of Pollux’ standard procedures and preoccupations, even if we cannot be entirely confident that any particular <b>cluster</b> <b>of</b> <b>words</b> had exactly the same form within the text's original design. It is divided into ten books, each with its own dedicatory preface addressed to the Emperor Commodus. Each book has its own distinctive focus on certain key themes, although the ordering principles are much clearer in some than in others. PostprintPeer reviewe...|$|E
40|$|The rapid {{development}} of online social media sites {{is accompanied by}} the generation of tremendous web contents. Web users are shifting from data consumers to data p roducers. As a result, topic detection and tracking without taking users ’ interests into account is not enough. This paper presents a statistical model that can detect interpretable trends and topics from document streams, where each trend (short for trending story) corresponds {{to a series of}} continuing events or a storyline. A topic is represented by a <b>cluster</b> <b>of</b> <b>words</b> frequently co-occurred. A trend can contain multiple topics and a topic can be shared by different trends. In addition, by leveraging a Recurrent Chinese Restaurant Process (RCRP), the number of trends in our model can be determined automatically without human intervention, so that our model can better generalize to unseen data. Furthermore, our proposed model incorporates user interest to fully simulate the generation process of web contents, which offers the opportunity for personalized recommendation in online social media. Experiments on three different datasets indicated that our proposed model can capture meaningful topics and trends, monitor rise and fall of detected trends, outperform baseline approach in terms of perplexity on held-out dataset, and improve the result of user participation prediction by leveraging users ’ interests to different trends...|$|E
40|$|We {{describe}} a new powerful text categorization method {{that is based}} on a combination of distributional features with a support vector machine (SVM) classifier. Our feature selection approach uses distributional <b>clustering</b> <b>of</b> <b>words</b> via the recently introduced information bottleneck method, which generates a more e#cient representation of the documents. When combined with the classification power of Support Vector Machines we produce the best known multilabel categorization results on the 20 Newsgroups dataset...|$|R
40|$|Background: The Ideology Project at the University of Oslo (Department of Psychology) is {{presently}} developing an archival method for measuring ideological changes in society through statistical {{changes in the}} frequency <b>of</b> specific <b>words</b> and phrases in written mass media language. At present, in psychological research, the method of choice for answering research questions on a society-wide scale is population surveys. This new method, using internet archives on newspapers going back several decades, constitutes a potential new way of studying ideology, hopefully supplementing the information gained through surveys. Research on media archives has many advantages over existing research methods, being unobtrusive, retrospective in nature, and relatively inexpensive. The Ideology Project has used statistical changes in the use <b>of</b> <b>words</b> and phrases to reveal and describe ideological changes in Norway, England, USA, Ghana and Turkey. Aim: This thesis has three main purposes. The first is to present the method {{as it has been}} used in recent articles. The second and primary purpose is to present various improvements on the method as an instrument for measuring ideological change through changes in language use. The third is to perform preliminary validation against results from the longitudinal population survey Norsk Monitor. Presented improvements: Automated data acquisition, increasing speed by a factor of more than 1000. Statistical data on the development of 10 000 “neutral” words has been compiled as comparison material. Baseline adjustment has been implemented on the basis of mean changes in the development patterns of the compiled comparison material. An empirical approach to testing if conceptual word clusters are used together in natural language is presented. Methods for determining representativity of development trends for <b>words</b> and <b>clusters</b> <b>of</b> <b>words</b> from one newspaper have been improved. Trend analysis for <b>words</b> and <b>clusters</b> <b>of</b> <b>words</b> has been systematized and improved. Preliminary validation: The presented comparison between value indexes from the longitudinal population survey Norsk Monitor and development patterns for selected <b>clusters</b> <b>of</b> <b>words</b> show strikingly similar development, indicating an exiting area of future research...|$|R
40|$|This is {{the first}} release of the generalised-brown software. It {{consists}} of two modules for constructing <b>clusters</b> <b>of</b> <b>word</b> types: 	The merge_generator module is an extension <b>of</b> Brown <b>clustering</b> (based on the C++ 11 implementation of wcluster) that tracks state {{in the form of}} a merge log 	The cluster_generator is a python script that reconstructs the state <b>of</b> the Brown <b>clustering</b> algorithm from the merge log in order to vary the granularity <b>of</b> the output <b>clusters...</b>|$|R
40|$|This paper proposes an {{algorithm}} to hierarchically cluster documents. Each {{cluster is}} actually a cluster of documents and an associated <b>cluster</b> <b>of</b> <b>words,</b> thus a document–word co-cluster. Note that, the vector model for documents creates the document–word matrix, of which every co-cluster is a submatrix. One would intuitively expect a submatrix made up of high values {{to be a good}} document cluster, with the corresponding word cluster containing its most distinctive features. Our algorithm looks to exploit this. We have defined matrix density, and our algorithm basically uses matrix density considerations in its working. The algorithm is a partitional–agglomerative algorithm. The partitioning step involves the identification of dense submatrices so that the respective row sets partition the row set of the complete matrix. The hierarchical agglomerative step involves merging the most “similar ” submatrices until we are down to the required number of clusters (if we want a flat clustering) or until we have just the single complete matrix left (if we are interested in a hierarchical arrangement of documents). It also generates apt labels for each cluster or hierarchy node. The similarity measure between clusters used for merging is {{based on the fact that}} the clusters here are co-clusters, and is a key point of difference from existing agglomerative algorithms. We will refer to the proposed algorithm as RPSA (Rowset Partitioning and Submatrix Agglomeration). We have compared it as a clustering algorithm with Spherical K-Means and Spectral Graph Partitioning. We have also evaluated some hierarchies generated by the algorithm...|$|E
40|$|Studies of Russian, Polish, and Lithuanian {{language}} learners converge on {{the finding that}} morphological features of nouns are first generalized to word clusters of high morpho-phonological similarities such as diminutives, that grammatical categorisation is are more easily applied to novel words that fall into these clusters. The present thesis explores whether the facilitating effect of diminutives on the acquisition of complex noun morphology can be extended to Serbian, a south Slavic language, morphologically similar to Russian and Polish. Specifically, the thesis explores the role of parameters responsible for the obtained diminutive advantage: high frequency of a particular <b>cluster</b> <b>of</b> <b>words</b> in child-directed speech (CDS) and morpho-phonological homogeneity within this cluster. A corpus analysis {{of the distribution of}} diminutives in Serbian CDS indicated a rather unexpected difference in frequency relative to Russian and Polish CDS, despite the high similarity of the diminutive derivation across these three Slavic languages. Out {{of the total number of}} nouns in Serbian CDS only 7 % were diminutives, compared to 20 - 30 % in Polish and 45 % in Russian. Two experimental studies explored whether the low frequency of diminutives in Serbian CDS attenuates the diminutive advantage in morphology learning compared to Russian and Polish. In the first two experiments, Serbian children exhibited a strong diminutive advantage for both gender agreement and case marking in the same range as Russian children, indicating that morpho-phonological homogeneity within the cluster of diminutives may play as important a role as their frequency for grammatical categorisation of novel nouns. A third study investigated in more detail the effects of morpho-phonological homogeneity on the emergence of the diminutive advantage using a gender-agreement task with novel nouns in simplex and pseudo-diminutive form over four sessions with Serbian children. The results showed a pseudo-diminutive advantage for gender agreement by Session 2, suggesting that the categorisation of nouns into grammatical categories is based on morpho-phonological homogeneity of the word cluster, emerges relatively fast, and can occur despite the much lower frequency of diminutives in Serbian CDS. Finally, a series of neural network simulations designed to capture the pattern of results from the third experimental study was used to examine to what extent a simple associative learning mechanism, relying on morpho-phonological similarity of the noun endings, can explain the findings. The performance of three models, a whole-word feed-forward network, a Simple Recurrent Network (SRN) and a last-syllable feed-forward network, was compared to the experimental data. The superior fit of the SRN suggests that gender learning is based on a very fast sequential build-up of representations of the entire word, allowing the system to exploit the predictive power of word stems to anticipate regularised endings. Overall, the findings of this thesis contribute to our general understanding of mechanisms responsible for the acquisition of complex inflectional noun morphology in two ways. First, by extending experimental studies and neural network simulations to Serbian, the results underline the universality of the idea that noun morphology is learned and processed through a single-route associative mechanism based on the frequency and morpho-phonological structure of nouns. More specifically, the results from experimental studies and neural network simulations demonstrate that for diminutives, the low-level grammatical categorisation is based mainly on the morpho-phonological similarity of word endings, and can emerge after just a few exposures. And second, the neural network simulations suggest that during the process of categorisation of nouns into gender categories, learners rely not only on predictable information from the noun endings, but also on phonological regularities in the stems of nouns. Taken together, these findings contribute also to a better understanding of the facilitating role of CDS in morphology acquisition. EThOS - Electronic Theses Online ServiceGBUnited Kingdo...|$|E
40|$|Vector space {{representations}} <b>of</b> <b>words</b> capture {{many aspects}} <b>of</b> <b>word</b> similarity, but such methods {{tend to make}} vector spaces in which antonyms (as well as synonyms) are close to each other. We present a new signed spectral normalized graph cut algorithm, signed clustering, that overlays existing thesauri upon distributionally derived vector representations <b>of</b> <b>words,</b> so that antonym relationships between word pairs are represented by negative weights. Our signed clustering algorithm produces <b>clusters</b> <b>of</b> <b>words</b> which simultaneously capture distributional and synonym relations. We evaluate these clusters against the SimLex- 999 dataset (Hill et al., 2014) of human judgments <b>of</b> <b>word</b> pair similarities, and also show the benefit <b>of</b> using our <b>clusters</b> to predict the sentiment of a given text...|$|R
40|$|Chronological corpora are {{collections}} of texts ordered in time. In bag-of-words approaches, data are typically the frequencies <b>of</b> individual <b>words</b> {{in the set}} of texts being grouped into equal-distant time points. In our work the temporal course <b>of</b> a <b>word</b> occurrence {{is viewed as a}} proxy <b>of</b> a <b>word</b> life-cycle: recognition <b>of</b> temporal shapes and <b>clustering</b> <b>of</b> <b>words</b> having similar life-cycles are the basic objective. However, the strong asymmetry of frequency spectrum typical of textual data has {{to be taken into account}} when defining the specific purpose <b>of</b> <b>clustering</b> and, hence, any type of further processing of data. By adopting a functional data approach and a distance-based curve <b>clustering,</b> the effect <b>of</b> selected data transformations on the generation <b>of</b> <b>word</b> groups is examined...|$|R
40|$|Information Retrieval Systems {{typically}} distinguish be-tween content bearing {{words and}} terms on a stop list. But “content-bearing “ is {{relative to a}} collection. For optimal retrieval efficiency, it is desirable to have au-tomated methods for custom building a stop list. This paper defines the notion <b>of</b> serial <b>clustering</b> <b>of</b> <b>words</b> in text, and explores the value <b>of</b> such <b>clustering</b> as an indicator <b>of</b> a <b>word</b> bearing cent ent. The numerical measures we propose may also be of value in assigning weights to terms in requests. Experimental support is obtained from natural text databases in three different languages. 1. Introduction an...|$|R
40|$|Abstract—Clustering {{methods have}} been {{extensively}} used in the solution of many Information Processing tasks in order to capture unknown object categories. This paper presents an approach to Word Sense Disambiguation based on clustering. The underlying {{idea is that the}} <b>clustering</b> <b>of</b> <b>word</b> senses provides a useful way to discover semantically related senses. We evaluate our proposal regarding both fine- and coarse-grained disambiguation. Experimental results over Senseval- 3 all-words, SemCor 2. 0 and SemEval- 2007 corpora are presented. Promising values of precision and recall are obtained. Index Terms—Word sense disambiguation, clustering. I...|$|R
