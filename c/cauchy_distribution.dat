482|156|Public
25|$|This is a {{standard}} <b>Cauchy</b> <b>distribution.</b>|$|E
25|$|Remark 3. An {{example of}} a {{distribution}} {{for which there is}} no expected value is <b>Cauchy</b> <b>distribution.</b>|$|E
25|$|The {{median of}} a <b>Cauchy</b> <b>distribution</b> with {{location}} parameter x0 and scale parameter y isnbsp&x0, the location parameter.|$|E
5000|$|... #Caption: A {{variety of}} <b>Cauchy</b> <b>distributions</b> for various {{location}} and scale parameters. <b>Cauchy</b> <b>distributions</b> {{are examples of}} fat-tailed distributions.|$|R
5000|$|... #Article: McCullagh's {{parametrization}} of the <b>Cauchy</b> <b>distributions</b> ...|$|R
5000|$|McCullagh's {{parametrization}} of the <b>Cauchy</b> <b>distributions</b> / (1:C) ...|$|R
25|$|Since {{the normal}} {{distribution}}, the <b>Cauchy</b> <b>distribution,</b> and the Lévy distribution {{all have the}} above property, it follows that they are special cases of stable distributions.|$|E
25|$|To compare {{competing}} {{statistics for}} small samples under realistic data conditions. Although type I error and power properties of statistics {{can be calculated}} for data drawn from classical theoretical distributions (e.g., normal curve, <b>Cauchy</b> <b>distribution)</b> for asymptotic conditions (i. e, infinite sample size and infinitesimally small treatment effect), real data often do not have such distributions.|$|E
25|$|One can {{generate}} Student-t samples {{by taking the}} ratio of variables from the normal distribution and the square-root of chi-squared distribution. If we use instead of the normal distribution, e.g., the Irwin–Hall distribution, we obtain over-all a symmetric 4-parameter distribution, which includes the normal, the uniform, the triangular, the Student-t and the <b>Cauchy</b> <b>distribution.</b> This is also more flexible than some other symmetric generalizations of the Gaussian distribution.|$|E
5000|$|... #Subtitle level 2: Relation {{to normal}} and <b>Cauchy</b> <b>distributions</b> ...|$|R
3000|$|... is {{the tail}} constant. The GCD family {{contains}} the Meridian [13] and <b>Cauchy</b> <b>distributions</b> as special cases, that is, for [...]...|$|R
5000|$|... where [...] The {{first term}} is {{the ratio of}} two <b>Cauchy</b> <b>distributions</b> while the last term {{is the product of}} two such distributions.|$|R
25|$|The Gaussian {{distribution}} {{belongs to}} the family of stable distributions which are the attractors of sums of independent, identically distributed distributions whether or not the mean or variance is finite. Except for the Gaussian which is a limiting case, all stable distributions have heavy tails and infinite variance. It {{is one of the few}} distributions that are stable and that have probability density functions that can be expressed analytically, the others being the <b>Cauchy</b> <b>distribution</b> and the Lévy distribution.|$|E
25|$|The uniform {{distribution}} {{as might be}} expected does not obey Benford's law. In contrast, the ratio distribution of two {{uniform distribution}}s is well described by Benford's law. Benford's law also describes the exponential distribution and the ratio distribution of two exponential distributions well. Although the half-normal distribution does not obey Benford's law, the ratio distribution of two half-normal distributions does. Neither the right-truncated normal distribution nor the ratio distribution of two right-truncated normal distributions are well described by Benford's law. This is not surprising as this distribution is weighted towards larger numbers. Neither the normal distribution nor the ratio distribution of two normal distributions (the <b>Cauchy</b> <b>distribution)</b> obey Benford's law. The fit of chi square distribution depends on the degrees of freedom (df) with good agreement with df = 1 and decreasing agreement as the df increases. The F distribution is fitted well for low degrees of freedom. With increasing dfs the fit decreases but much more slowly than the chi square distribution. The fit of the log-normal distribution depends on the mean and the variance of the distribution. The variance has a much greater effect on the fit than does the mean. Larger values of both parameters result in better agreement with the law. The ratio of two log normal distributions is a log normal so this distribution was not examined.|$|E
500|$|The Shannon entropy of the <b>Cauchy</b> <b>distribution</b> {{is equal}} to , which also {{involves}} [...]|$|E
50|$|Since normal <b>distributions</b> and <b>Cauchy</b> <b>distributions</b> are stable distributions, {{they are}} each are closed under {{convolution}} (up to change of scale), and {{it follows that}} the Voigt distributions are also closed under convolution.|$|R
3000|$|... model {{contains}} the Gaussian and the <b>Cauchy</b> <b>distributions</b> as special cases, {{depending on the}} degrees of freedom parameter. It is shown in [18] that the myriad estimator is also optimal for the generalized- [...]...|$|R
40|$|Recent work on minimum variances estimators {{based on}} <b>Cauchy</b> <b>distributions</b> appear {{relevant}} to orbital drag estimation. Samples form <b>Cauchy</b> <b>distributions</b> which {{are part of}} a class of heavy-tailed distributions, are characterized by long stretches of fairly small variation, punctuated by large variations that are many times larger than could be expected from a Gaussian. Such behavior can occur when solar storms perturb the atmosphere. In this context, the present work describes an embedding of the scalar Idan-Speyer Cauchy Estimator to estimate density corrections, within an Extended Kalman Filter that estimates the state of a low Earth orbiter. In contrast to the baseline Kalman approach, the larger formal errors of the present approach fully and conservatively bound the predictive error distribution, {{even in the face of}} unanticipated density disturbances of hundreds of percent...|$|R
500|$|... is the {{fundamental}} solution of the Laplace equation in the upper half-plane. [...] It represents the electrostatic potential in a semi-infinite plate whose potential along the edge is held at fixed at the delta function. The Poisson kernel is also {{closely related to the}} <b>Cauchy</b> <b>distribution.</b> [...] This semigroup evolves according to the equation ...|$|E
500|$|The <b>Cauchy</b> <b>distribution</b> {{plays an}} {{important}} role in potential theory because it is the simplest Furstenberg measure, the classical Poisson kernel associated with a Brownian motion in a half-plane. [...] Conjugate harmonic functions and so also the Hilbert transform are associated with the asymptotics of the Poisson kernel. [...] The Hilbert transform H is the integral transform given by the Cauchy principal value of the singular integral ...|$|E
500|$|An infinitesimal {{formula for}} an {{infinitely}} tall, unit impulse delta function (infinitesimal version of <b>Cauchy</b> <b>distribution)</b> explicitly appears in an 1827 text of Augustin Louis Cauchy. Siméon Denis Poisson considered {{the issue in}} connection with the study of wave propagation as did Gustav Kirchhoff somewhat later. [...] Kirchhoff and Hermann von Helmholtz also introduced the unit impulse as a limit of Gaussians, which also corresponded to Lord Kelvin's notion of a point heat source. [...] At the end of the 19th century, Oliver Heaviside used formal Fourier series to manipulate the unit impulse. The Dirac delta function as such was introduced as a [...] "convenient notation" [...] by Paul Dirac in his influential 1930 book The Principles of Quantum Mechanics. [...] He called it the [...] "delta function" [...] since he used it as a continuous analogue of the discrete Kronecker delta.|$|E
40|$|One of {{the most}} {{efficient}} techniques for processing interval and fuzzy data is a Monte-Carlo type technique of Cauchy deviates that uses <b>Cauchy</b> <b>distributions.</b> This technique is mathematically valid, but somewhat counterintuitive. In this paper, following the ideas of Paul Werbos, we provide a natural neural network explanation for this technique...|$|R
40|$|AbstractThis paper {{proposes a}} unified {{treatment}} of maximum likelihood estimates of angular Gaussian and multivariate <b>Cauchy</b> <b>distributions</b> {{in both the}} real and the complex case. The complex case is relevant in shape analysis. We describe in full generality the set of maxima of the corresponding log-likelihood functions with respect to an arbitrary probability measure. Our tools are the convexity of log-likelihood functions and their behaviour at infinity...|$|R
40|$|AbstractThe noncentral {{configuration}} density, derived {{under an}} elliptical model, generalizes and corrects the Gaussian configuration and some Pearson results. Partition theory is {{then used to}} obtain explicit configuration densities associated with matrix variate symmetric Kotz type distributions (including the normal distribution), matrix variate Pearson type VII distributions (including t and <b>Cauchy</b> <b>distributions),</b> the matrix variate symmetric Bessel distribution (including the Laplace distribution) and the matrix variate symmetric Jensen-logistic distribution...|$|R
2500|$|Student's {{t-distribution}} (continuous), {{of which}} the <b>Cauchy</b> <b>distribution</b> is a special case ...|$|E
2500|$|For example, suppose X has a {{standard}} <b>Cauchy</b> <b>distribution.</b> [...] Then [...] [...] This is not differentiable at t = 0, {{showing that the}} <b>Cauchy</b> <b>distribution</b> has no expectation. [...] Also, the characteristic function of the sample mean [...] of n independent observations has characteristic function , using the result from the previous section. [...] This is the characteristic function of the standard Cauchy distribution: thus, the sample mean has the same distribution as the population itself.|$|E
2500|$|The {{expected}} value {{does not exist}} for random variables having some distributions with large [...] "tails", such as the <b>Cauchy</b> <b>distribution.</b> [...] For random variables such as these, the long-tails of the distribution prevent the sum/integral from converging.|$|E
40|$|Bicentricity {{and other}} similar noncrystallographic centres of {{symmetry}} affect intensities of X-rays diffracted by single crystals in different hkl directions. Hence their statistical distribution would also be modified {{and this has been}} studied for crystals obeying Gaussian statistics. This work has now been extended to crystals obeying Cauchy statistics. N(z) against z curves for acentric, centric and biceninc cases for both Gaussian and <b>Cauchy</b> <b>distributions</b> have been compared...|$|R
40|$|We refine recent {{existence}} and uniqueness results, for the barycenter of points at infinity of Hadamard manifolds, to measures on the sphere at infinity of symmetric spaces of non compact type and, more specifically, to measures concentrated on single orbits. The barycenter will {{be interpreted as}} the maximum likelihood estimate (MLE) of generalized <b>Cauchy</b> <b>distributions</b> on Furstenberg boundaries. As a spin-off, a new proof of the general Knight–Meyer characterization theorem will be given...|$|R
40|$|Brascamp–Lieb-type, {{weighted}} Poincaré-type {{and related}} analytic inequalities are studied for multidimensional <b>Cauchy</b> <b>distributions</b> and more general κconcave probability measures (in {{the hierarchy of}} convex measures). In analogy with the limiting (infinite dimensional log-concave) Gaussian model, the weighted inequalities fully describe the measure concentration and large deviation properties of this family of measures. Cheeger-type isoperimetric inequalities are investigated similarly giving rise to a common weight {{in the class of}} concave probability measures under consideration...|$|R
2500|$|The {{standard}} deviation of a (univariate) probability distribution {{is the same as}} that of a random variable having that distribution. [...] Not all random variables have a {{standard deviation}}, since these expected values need not exist. For example, the {{standard deviation of}} a random variable that follows a <b>Cauchy</b> <b>distribution</b> is undefined because its expected value μ is undefined.|$|E
2500|$|Distributions [...] {{which can}} arise {{in this way}} are called stable. [...] Clearly, the normal {{distribution}} is stable, {{but there are also}} other stable distributions, such as the <b>Cauchy</b> <b>distribution,</b> for which the mean or variance are not defined. [...] The scaling factor [...] may be proportional to , for any it may also be multiplied by a slowly varying function of [...]|$|E
2500|$|If a {{continuous}} distribution {{does not have}} an expected value, as is the case for the <b>Cauchy</b> <b>distribution,</b> it does not have a variance either. Many other distributions for which the expected value does exist also do not have a finite variance because the integral in the variance definition diverges. An example is a Pareto distribution whose index [...] satisfies ...|$|E
40|$|AbstractWe refine recent {{existence}} and uniqueness results, for the barycenter of points at infinity of Hadamard manifolds, to measures on the sphere at infinity of symmetric spaces of non compact type and, more specifically, to measures concentrated on single orbits. The barycenter will {{be interpreted as}} the maximum likelihood estimate (MLE) of generalized <b>Cauchy</b> <b>distributions</b> on Furstenberg boundaries. As a spin-off, a new proof of the general Knight–Meyer characterization theorem will be given...|$|R
40|$|First order {{autoregressive model}} indexed by a {{supercritical}} Galton-Watson branching process is discussed. Limiting distributions {{of the least}} squares estimates are derived both for the stationary and explosive cases. It is shown that a certain random variable inherent in the branching process is acting as a mixing variable in limiting mixture distributions. In particular, with explosive Gaussian case, we obtain a mixture of <b>Cauchy</b> <b>distributions</b> rather than <b>Cauchy.</b> AR(1) Branching process Cauchy mixture Limiting mixture distribution...|$|R
40|$|Evolutionary {{approaches}} to protein-ligand docking typically use a real-value encoding and mutation operators based on Gaussian and <b>Cauchy</b> <b>distributions.</b> The choice of mutation {{is important for}} an efficient algorithm for this problem. We investigate the effect of mutation operators by locality analysis. High locality means that small variations in the genotype imply small variations in the phenotype. Results show that Gaussian-based operators have stronger locality than Cauchy-based ones, especially if an annealing scheme is used to control the variance...|$|R
