1903|10000|Public
5|$|The {{most common}} {{approach}} of GWA studies is the case-control setup, which compares two {{large groups of}} individuals, one healthy control group and one case group affected by a disease. All individuals in each group are genotyped {{for the majority of}} common known SNPs. The exact number of SNPs depends on the genotyping technology, but are typically one million or more. For each of these SNPs it is then investigated if the allele frequency is significantly altered between the case and the control group. In such setups, the fundamental unit for reporting effect sizes is the odds ratio. The odds ratio is the ratio of two odds, which in the context of GWA studies are the odds of disease for individuals having a specific allele and the odds of disease for individuals who do not have that same allele. When the allele frequency in the case group is much higher than in the control group, the odds ratio is higher than 1, and vice versa for lower allele frequency. Additionally, a P-value for the significance of the odds ratio is typically calculated using a simple <b>chi-squared</b> <b>test.</b> Finding odds ratios that are significantly different from 1 is the objective of the GWA study because this shows that a SNP is associated with disease.|$|E
25|$|Pearson's <b>chi-squared</b> <b>test.</b> A {{hypothesis}} test using normal approximation for discrete data.|$|E
25|$|The {{locations}} of the quantiles can then be used to test for differences between samples (in the variables not being split) using the <b>chi-squared</b> <b>test.</b>|$|E
30|$|Horie et al. [3] {{evaluated}} the strength data of structural lumber, and used <b>Chi-square</b> <b>test</b> and KS test as goodness-of-fit measures. In the current study, however, the <b>Chi-square</b> <b>test</b> was not used, because the sample data {{had to be}} binned before generating the <b>Chi-square</b> <b>test.</b> As we know, {{the value of the}} <b>Chi-square</b> <b>test</b> statistic is dependent on how the data are binned. In addition, the Weibull distribution employed in this study was not 3 P Weibull but 2 P Weibull, because the theoretical and physical meanings of the location parameter in 3 P Weibull distribution were not clear [3].|$|R
30|$|In general, the <b>chi-square</b> <b>test</b> is less {{sensitive}} and less efficient than the Kolmogorov test or the Anderson-Darling test {{due to the}} fact that the <b>chi-square</b> <b>test</b> coarsens data by placing data into discrete bins.|$|R
40|$|Applied {{researchers}} have employed <b>chi-square</b> <b>tests</b> {{for more than}} one hundred years. This paper addresses the question of how one should follow a statistically significant <b>chi-square</b> <b>test</b> result {{in order to determine the}} source of that result. Four approaches were evaluated: calculating residuals, comparing cells, ransacking, and partitioning. Data from two recent journal articles were used to illustrate these approaches. A call is made for greater consideration of foundational techniques such as the <b>chi-square</b> <b>tests...</b>|$|R
25|$|The p-value was {{introduced}} by Karl Pearson in the Pearson's <b>chi-squared</b> <b>test,</b> where he defined P (original notation) as {{the probability that the}} statistic would be at or above a given level. This is a one-tailed definition, and the chi-squared distribution is asymmetric, only assuming positive or zero values, and has only one tail, the upper one. It measures goodness of fit of data with a theoretical distribution, with zero corresponding to exact agreement with the theoretical distribution; the p-value thus measures how likely the fit would be this bad or worse.|$|E
25|$|For the {{important}} {{case in which}} the data are hypothesized to follow the normal distribution, depending {{on the nature of the}} test statistic and thus the underlying hypothesis of the test statistic, different null hypothesis tests have been developed. Some such tests are z-test for normal distribution, t-test for Student's t-distribution, f-test for f-distribution. When the data do not follow a normal distribution, it can still be possible to approximate the distribution of these test statistics by a normal distribution by invoking the central limit theorem for large samples, as in the case of Pearson's <b>chi-squared</b> <b>test.</b>|$|E
2500|$|Chi-squared {{distribution}}, {{the distribution}} of a sum of squared standard normal variables; useful e.g. for inference regarding the sample variance of normally distributed samples (see <b>chi-squared</b> <b>test)</b> ...|$|E
30|$|Huang et al. (2007) {{proposed}} a statistical method {{that uses the}} <b>chi-square</b> <b>test</b> and conditional probability. The study sought to determine any drug-drug interaction for the ADRs. Firstly, a <b>chi-square</b> <b>test</b> {{is used to calculate}} the dependency of all drug and symptom pairs. The <b>chi-square</b> <b>test,</b> however, can only show the relative strength of association, but cannot distinguish whether the ADR is caused by drug-drug interaction or a single drug. The use of a conditional probability resolves this problem.|$|R
30|$|All {{data are}} {{reported}} as descriptive variables, normal distributed data should be displayed as means ± standard deviations (SD) and non-parametric data as median (interquartile range, IQR). Accordingly, to compare distributions between groups, we used T-test, non-parametric Kruskal-Wallis <b>test</b> or <b>chi-square</b> <b>test.</b> Pairwise post-hoc comparisons were performed with unpaired-samples t-test, Mann-Whitney <b>test</b> or <b>chi-square</b> <b>test,</b> as appropriate; {{in all cases}} we used a Sidak correction. <b>Chi-square</b> <b>test</b> was computed with Yates’ continuity correction for 2 [*]×[*] 2 contingency tables.|$|R
40|$|This {{paper is}} the sixth {{in a series}} of {{statistics}} articles recently published by Australian Critical Care. In this paper we explore the most commonly used statistical tests to compare groups of data at the nominal level of measurement. The chosen statistical <b>tests</b> are the <b>chi-square</b> <b>test,</b> <b>chi-square</b> <b>test</b> for goodness of fit, <b>chi-square</b> <b>test</b> for independence, Fisher's exact test, McNemar's test and the use of confidence intervals for proportions. Examples of how to use and interpret the tests are provided. Full Tex...|$|R
2500|$|The p-value {{was first}} {{formally}} introduced by Karl Pearson, in his Pearson's <b>chi-squared</b> <b>test,</b> using the chi-squared distribution and notated as capital P. The p-values for the chi-squared distribution (for various values of χ2 and degrees of freedom), now notated as P, was calculated in , collected in [...]|$|E
2500|$|Significance {{testing is}} largely {{the product of}} Karl Pearson (p-value, Pearson's <b>chi-squared</b> <b>test),</b> William Sealy Gosset (Student's t-distribution), and Ronald Fisher ("null hypothesis", {{analysis}} of variance, [...] "significance test"), while hypothesis testing was developed by Jerzy Neyman and Egon Pearson (son of Karl). Ronald Fisher began his life in statistics as a Bayesian (Zabell 1992), but Fisher soon grew disenchanted with the subjectivity involved (namely use {{of the principle of}} indifference when determining prior probabilities), and sought to provide a more [...] "objective" [...] approach to inductive inference..|$|E
2500|$|Karl Pearson was {{important}} in {{the founding of the}} school of biometrics, which was a competing theory to describe evolution and population inheritance {{at the turn of the}} 20th century. His series of eighteen papers, [...] "Mathematical Contributions to the Theory of Evolution" [...] established him as the founder of the biometrical school for inheritance. In fact, Pearson devoted much time during 1893 to 1904 to developing statistical techniques for biometry. These techniques, which are widely used today for statistical analysis, include the <b>chi-squared</b> <b>test,</b> standard deviation, and correlation and regression coefficients. Pearson's Law of Ancestral Heredity stated that germ plasm consisted of heritable elements inherited from the parents as well as from more distant ancestors, the proportion of which varied for different traits. Karl Pearson was a follower of Galton, and although the two differed in some respects, Pearson used a substantial amount of Francis Galton's statistical concepts in his formulation of the biometrical school for inheritance, such as the law of regression. The biometric school, unlike the Mendelians, focused not on providing a mechanism for inheritance, but rather on providing a mathematical description for inheritance that was not causal in nature. While Galton proposed a discontinuous theory of evolution, in which species would have to change via large jumps rather than small changes that built up over time, Pearson pointed out flaws in Galton's argument and actually used Galton's ideas to further a continuous theory of evolution, whereas the Mendelian's favored a discontinuous theory of evolution. While Galton focused primarily on the application of statistical methods to the study of heredity, Pearson and his colleague Weldon expanded statistical reasoning to the fields of inheritance, variation, correlation, and natural and sexual selection.|$|E
30|$|The {{values are}} {{expressed}} as means with 95 % confidence intervals (CI). The Mc Nemar <b>chi-square</b> <b>test</b> {{was performed to}} compare the sensitivity and specificity between 4 DST and FDG, and the <b>chi-square</b> <b>test</b> for independence was performed to compare spread lesion or the recurrence rate between 4 DST and FDG.|$|R
40|$|In this paper, {{we suggest}} {{the use of}} the <b>chi-square</b> <b>test</b> for {{detecting}} backoff misbehaviour in IEEE 802. 11 EDCA networks. A performance evaluation is performed to compare the <b>chi-square</b> <b>test</b> with two other methods, known in the literature. To perform a suitable comparison, these two methods are extended to support EDCA and the BEB mechanism. We assume a misbehaviour model, which can be easily executed by a selfish user. We show that the <b>chi-square</b> <b>test</b> outperforms the other methods in terms of the probability of misbehaviour detection and time required to positively identify a misbehaving node...|$|R
5000|$|Normality test using Jarque-Bera test, Shapiro-Wilk <b>test,</b> and <b>Chi-square</b> <b>test</b> methods.|$|R
50|$|Just as the Wilson {{interval}} mirrors Pearson's <b>chi-squared</b> <b>test,</b> the Wilson interval with continuity correction {{mirrors the}} equivalent Yates' <b>chi-squared</b> <b>test.</b>|$|E
5000|$|For {{samples of}} a {{reasonable}} size, the G-test and the <b>chi-squared</b> <b>test</b> {{will lead to}} the same conclusions. However, the approximation to the theoretical chi-squared distribution for the G-test is better than for the Pearson's <b>chi-squared</b> <b>test.</b> In cases where [...] for some cell case the G-test is always better than the <b>chi-squared</b> <b>test.</b>|$|E
50|$|Cochran-Mantel-Haenszel <b>chi-squared</b> <b>test.</b>|$|E
40|$|An {{upper and}} lower bound are {{presented}} for {{the difference between the}} distribution functions of noncentral chi-square variables with the same degrees of freedom and different noncentralities. The inequalities are applied in a comparison of two approximations to the power of Pearson's <b>chi-square</b> <b>test.</b> Noncentral [chi] 2 distribution Pearson's <b>chi-square</b> <b>test</b> contamination family exponential family...|$|R
30|$|Categorical {{variables}} {{were analyzed using}} the <b>chi-square</b> <b>test.</b> Fisher’s exact test was used whenever {{the distribution of the}} variable under analysis rendered the <b>chi-square</b> <b>test</b> inviable. The continuous quantitative {{variables were}} compared using Student’s t test. All statistical analyses were performed using the SPSS statistical software program (SPSS Inc., USA), adopting a significance level of 5  %.|$|R
40|$|We {{show that}} the {{sequence}} of <b>chi-square</b> <b>tests</b> is asymptotically minimax if a number of cells increases with increasing sample size. The proof utilizes theorem about asymptotic normality of <b>chi-square</b> <b>test</b> statistics obtained under new compact assumptions. (orig.) Available from TIB Hannover: RR 5549 (185) +a / FIZ - Fachinformationszzentrum Karlsruhe / TIB - Technische InformationsbibliothekSIGLEDEGerman...|$|R
5000|$|A <b>chi-squared</b> <b>test,</b> {{also written}} as [...] test, is any {{statistical}} hypothesis test wherein the sampling {{distribution of the}} test statistic is a chi-squared distribution when the null hypothesis is true. Without other qualification, 'chi-squared test' often is used as short for Pearson's <b>chi-squared</b> <b>test.</b>|$|E
5000|$|... #Subtitle level 2: Example <b>chi-squared</b> <b>test</b> for {{categorical}} data ...|$|E
50|$|Cramér's V - {{a measure}} of {{correlation}} for the <b>chi-squared</b> <b>test.</b>|$|E
30|$|The {{quantitative}} analysis {{was carried out}} via a <b>Chi-square</b> <b>test</b> {{to examine the relationship}} between nominal variables. Moreover, the analysis of the deviations of the observed frequencies from a theoretical random distribution was carried out by a Binomial test in the case of the study of two categories and by <b>Chi-square</b> <b>test</b> when more categories were involved.|$|R
40|$|The {{examination}} of cross-classified category data {{is common in}} evaluation and research, with Karl Pearson’s family of <b>chi-square</b> <b>tests</b> representing {{one of the most}} utilized statistical analyses for answering questions about the association or difference between categorical variables. Unfortu-nately, these tests are also among the more commonly misinterpreted statistical tests in the field. The problem is not that researchers and evaluators misapply the results of <b>chi-square</b> <b>tests,</b> but rather they tend to over interpret or incorrectly interpret the results, leading to statements that may have limited or no statistical support based on the analyses preformed. This paper attempts to clarify any confusion about the uses and interpretations of the family of <b>chi-square</b> <b>tests</b> developed by Pearson, focusing primarily on the <b>chi-square</b> <b>tests</b> of independence and homogeneity of variance (identity of distributions). A brief survey of the recent evaluation lit-erature is presented to illustrate the prevalence of the <b>chi-square</b> <b>test</b> and to offer examples of how these tests are misinterpreted. While the omnibus form of all three tests in the Karl Pearson family of chi-square tests—independence, homogeneity, and goodness-of-fit,—use essentially the same formula, each of these three tests is, in fact, distinct with specific hypotheses, sampling approaches, interpretations, and options following rejection of the null hypothesis. Finally, a little known option, the use and interpretation of post hoc comparisons based on Goodman’s procedure (Goodman, 1963) following the rejection of the <b>chi-square</b> <b>test</b> of homogeneity, is described in detail...|$|R
40|$|The multivariate {{asymptotic}} distribution of sequential <b>Chi-square</b> <b>test</b> statistics is investigated. It is shown that: (a) when sequential Chi-square statistics are calculated for nested models {{on the same}} data, the statistics have an asymptotic intercorrelation which may be expressed in closed form, and which is, in many cases, quite high; and (b) sequential <b>Chi-square</b> difference <b>tests</b> are asymptotically independent. Some Monte Carlo evidence on {{the applicability of the}} theory is provided. Key words: Asymptotic distribution theory, sequential <b>Chi-square</b> <b>tests.</b> 1...|$|R
5000|$|... #Subtitle level 2: <b>Chi-squared</b> <b>test</b> for {{variance}} {{in a normal}} population ...|$|E
5000|$|... #Subtitle level 2: Example: Pearson's <b>chi-squared</b> <b>test</b> versus {{an exact}} test ...|$|E
50|$|Pearson's <b>chi-squared</b> <b>test.</b> A {{hypothesis}} test using normal approximation for discrete data.|$|E
30|$|To examine {{potential}} {{differences between the}} categorical variables, the <b>Chi-square</b> <b>test</b> was used.|$|R
30|$|In {{order to}} test this {{hypothesis}} we have applied <b>Chi-square</b> <b>test</b> Table 12.|$|R
40|$|The <b>chi-square</b> <b>test</b> was {{compared}} with the Fisher exact test using Ns ranging from 3 to 69. Contrary to expectations, there was closer agreement between the two tests with the smaller Ns. The <b>chi-square</b> <b>test</b> gave very good approximations of the true probabilities—especially when the two groups being compared were nearly equal in sample size— but only if it {{was used as a}} one-tailed test. If the groups being com-pared had very unequal sample sizes (ratios of 7 : 1 and greater), the <b>chi-square</b> <b>test</b> sometimes gave questionable results. A table of one-tailed probabilities is provided. ■! ■ I ■■ ■ — [...] m...|$|R
