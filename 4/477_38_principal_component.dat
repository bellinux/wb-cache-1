0|10000|Public
5000|$|<b>Principal</b> <b>component</b> {{analysis}} (PCA) and <b>principal</b> <b>component</b> regression ...|$|R
40|$|Statistically Quality Control is {{a problem}} solving {{technique}} that used to check, control, analyze, bring off and repair product with statistical methods. One of the method that used statistically control quality is <b>Principal</b> <b>Component</b> Analysis. <b>Principal</b> <b>Component</b> Analysis is a multivariate technique that used to reduce the dimension of data. <b>Principal</b> <b>component</b> is concerned with explaining the variance-covariance structure {{of a set of}} variables through a few linear combinations of these variables. Statistically quality control with <b>principal</b> <b>components</b> is used by constructing multivariate control charts which consist of Ellipse Chart for first two <b>principal</b> <b>components</b> and Chart for unexplained <b>principal</b> <b>components</b> in Ellipse Chart. Keywords : <b>Principal</b> <b>Component</b> Analysis, Ellipse Chart, Chart...|$|R
40|$|This paper {{presents}} a method called quadtree <b>principal</b> <b>components</b> analysis for facial expression classification. The quadtree <b>principal</b> <b>components</b> analysis {{is an image}} transformation that {{takes its name from}} the quadtree partition scheme on which it is based. The quadtree <b>principal</b> <b>components</b> analysis method implements a global-local decomposition of the input face image. This solves the problems associated with the existing <b>principal</b> <b>components</b> analysis and local <b>principal</b> <b>components</b> analysis methods when applied to facial expression classification...|$|R
40|$|In data mining, <b>principal</b> <b>component</b> {{analysis}} {{is a popular}} dimension reduction technique. It also provides a good remedy for the multicollinearity problem, but its interpretation of input space is not as good. To overcome the interpretation problem, <b>principal</b> <b>components</b> (cluster components) are obtained through variable clustering, which was implemented with PROC VARCLUS. The procedure uses oblique <b>principal</b> <b>components</b> analysis and binary iterative splits for variable clustering, and it provides non-orthogonal <b>principal</b> <b>components.</b> Even if this procedure sacrifices the orthogonal property among <b>principal</b> <b>components,</b> it provides good interpretable <b>principal</b> <b>components</b> and well-explained cluster structures of variables. However, the PROC VARCLUS implementation is inefficient to deal with high-dimensional data. We introduce the two-stage, variable clustering technique for large data sets. This technique uses global clusters, sub-clusters, and their <b>principal</b> <b>components...</b>|$|R
30|$|Step 2 Calculate the <b>principal</b> <b>component</b> loading matrix by <b>principal</b> <b>component</b> analysis.|$|R
3000|$|... where <b>principal</b> <b>component</b> (PC) = {ξ 1 … ξ m } are the m <b>principal</b> <b>components</b> and w ij [...] is {{the weight}} of the j th {{variable}} for the i th <b>principal</b> <b>component.</b>|$|R
40|$|Probabilistic <b>Principal</b> <b>Component</b> Analysis is a {{reformulation}} of {{the common}} multivariate analysis technique known as <b>Principal</b> <b>Component</b> Analysis. It employs a latent variable model framework similar to factor analysis allowing to establish a maximum likelihood solution for the parameters that comprise the model. One of the main assumptions of Probabilistic <b>Principal</b> <b>Component</b> Analysis is that observed data is independent and identically distributed. This assumption is inadequate for many applications, in particular, for modeling sequential data. In this paper, the authors introduce a temporal version of Probabilistic <b>Principal</b> <b>Component</b> Analysis by using a hidden Markov model {{in order to obtain}} optimized representations of observed data through time. Combining Probabilistic <b>Principal</b> <b>Component</b> Analyzers with a hidden Markov model, it is possible to enhance the capabilities of transformation and reduction of time series vectors. In order to find automatically the dimensionality of the principal subspace associated with these Probabilistic <b>Principal</b> <b>Component</b> Analyzers through time, a Bayesian treatment of the <b>Principal</b> <b>Component</b> model is introduced as well. Keywords: Hidden Markov models, <b>principal</b> <b>component</b> analysis, bayesian <b>principal</b> <b>component</b> analysis, EM algorithm, model selection 1...|$|R
30|$|Rotate the <b>principal</b> <b>components</b> using varimax {{rotation}} [57]. Varimax rotation is an orthogonal transform that rotates the <b>principal</b> <b>components</b> {{such that the}} variance of the factors is maximized. This rotation improves the interpretability of the <b>principal</b> <b>components.</b>|$|R
40|$|The {{essence of}} <b>principal</b> <b>components</b> {{analysis}} {{and the problem}} of dimension reduction are described. A method of <b>principal</b> <b>components</b> calculation is presented, which is based on the covariance matrix eigenvalues determination. Practical implementations of <b>principal</b> <b>components</b> analysis are described, which are based on QR-algorithm. Application of <b>principal</b> <b>components</b> analysis in space images classification for the reduction of training samples dimension is discussed...|$|R
30|$|PCA {{was used}} for the {{reduction}} of data dimensionality from the original 11 variables to 4 <b>principal</b> <b>components</b> explaining 72 % of the total data variance. The first <b>principal</b> <b>component</b> characterized the organic contamination of treated wastewater, and the second <b>principal</b> <b>component</b> characterized the nitrification processes in BWWTP. The third <b>principal</b> <b>component</b> indicated phosphorus-based compounds, and the fourth <b>principal</b> <b>component</b> presented the influence of effluent from the coking plant. Factor analysis and Ward’s hierarchical clustering method confirmed the relationships between the original parameters revealed by PCA.|$|R
30|$|<b>Principal</b> <b>component</b> {{analysis}} method [21, 22] {{is used to}} compute <b>principal</b> <b>component</b> which reduces the dimension of extracted audio features. It retains {{as much as possible}} variance in the audio features. <b>Principal</b> <b>components</b> are extracted by a linear transformation to a new set of features which are uncorrelated and are ordered according to their importance. <b>Principal</b> <b>component</b> is computed using singular value decomposition algorithm.|$|R
40|$|FIGURE 13. Sheared <b>principal</b> <b>component</b> {{analysis}} of morphometric data of Gephyrocharax chocoensis (specimens grouped by absence / presence of humeral blotch) : (A) plot of {{first and second}} <b>principal</b> <b>components</b> (PC 1 vs. PC 2); (B) plot of second and third <b>principal</b> <b>components</b> (PC 2 vs. PC 3). Sheared principal component: H, <b>principal</b> <b>component</b> centered by groups: S. Percentage of explained variance beside each component...|$|R
40|$|FIGURE 32. Sheared <b>principal</b> <b>components</b> {{analysis}} of morphometric data of Gephyrocharax valencia comparing among species organized by geographic groups: (A) left plots showing {{first and second}} <b>principal</b> <b>components</b> (PC 1 vs. PC 2); (B) right plots showing second and third <b>principal</b> <b>components</b> (PC 2 vs. PC 3). Sheared principal component: H, <b>principal</b> <b>component</b> centered by groups: S. Percentage of explained variance beside each component...|$|R
40|$|We analyse {{the shape}} of the {{rotation}} curves by means of the <b>Principal</b> <b>Component</b> Analysis (PCA) and find that the first two <b>principal</b> <b>components</b> account for about 90 % of the total variance in rotation curve shapes. The most important physical parameter (the first <b>principal</b> <b>component)</b> works for the rotation curve in a similar way at all positions within galaxies. The second <b>principal</b> <b>component</b> mainly determines the changes of the central rotation curve. We have compared the first and second <b>principal</b> <b>components</b> with the physical properties of the sample of galaxies and found that the first <b>principal</b> <b>component</b> is clearly related to the size parameters like the luminous mass of the galaxy. SS hale also shown that the second <b>principal</b> <b>component</b> is most likely related to the luminous mass density. We conclude that the first two <b>principal</b> <b>components</b> largely determine {{the shape of}} the rotation curve. We have constructed synthetic rotation curves based on the <b>principal</b> <b>components.</b> We argue that the synthetic rotation curves constructed in this way have many advantages over the conventional ways of describing {{the shape of the}} rotation curve: more objective, comprehensive and widely applicable...|$|R
30|$|Apply <b>principal</b> <b>component</b> {{analysis}} (PCA) {{to reduce}} the data dimensionality. Keep the first p - 1 <b>principal</b> <b>components.</b>|$|R
40|$|Kernel <b>principal</b> <b>component</b> analysis(PCA) maps {{observations}} in nonlinear feature space to a reduced di-mensional plane of <b>principal</b> <b>components.</b> We {{do not need}} to specify the feature space explicitly because the procedure uses the kernel trick. In this paper, we propose a graphical scheme to represent variables in the kernel <b>principal</b> <b>component</b> analysis. In addition, we propose an index for individual variables to measure the impor-tance in the <b>principal</b> <b>component</b> plane...|$|R
30|$|A <b>principal</b> <b>component</b> {{analysis}} (PCA) can {{be regarded}} as a dimensionality reduction technique by projecting the data onto a subspace spanned by the eigenvectors (<b>principal</b> <b>components)</b> of the data covariance matrix [20]. For this study, we used PCA to define the feature transformation matrix A, such that the rows of A are given by the <b>principal</b> <b>components.</b> Therefore, the marker ATw can be expressed as a weighted sum of <b>principal</b> <b>components.</b>|$|R
40|$|<b>Principal</b> <b>component</b> {{regression}} (PCR) is a two-stage {{procedure that}} selects some <b>principal</b> <b>components</b> and then constructs a regression model regarding them as new explanatory variables. Note that the <b>principal</b> <b>components</b> are obtained from only explanatory variables and not considered with the response variable. To address this problem, we propose the sparse <b>principal</b> <b>component</b> regression (SPCR) {{that is a}} one-stage procedure for PCR. SPCR enables us to adaptively obtain sparse <b>principal</b> <b>component</b> loadings {{that are related to}} the response variable and select the number of <b>principal</b> <b>components</b> simultaneously. SPCR can be obtained by the convex optimization problem for each of parameters with the coordinate descent algorithm. Monte Carlo simulations and real data analyses are performed to illustrate the effectiveness of SPCR. Comment: 24 page...|$|R
40|$|This paper {{proposes a}} novel methodology, {{based on the}} Common <b>Principal</b> <b>Component</b> analysis, {{allowing}} one to estimate the factors driving the term structure of interest rates, {{in the presence of}} time-varying covariance structure. The advantages of this method are first, that, unlike classical <b>principal</b> <b>component</b> analysis, common factors can be estimated without assuming that the volatility of the factors is constant; and second, that the factor structure can be decomposed into permanent and transitory common factors. We conclude that only permanent factors are relevant for modeling the dynamics of interest rates, and that the common <b>principal</b> <b>component</b> approach appears to be more accurate than the classical <b>principal</b> <b>component</b> one to estimate the risk factor structure. Term Structure of Interest Rates, <b>Principal</b> <b>Component</b> Analy-sis, Common <b>Principal</b> <b>Component</b> Analysis...|$|R
3000|$|... ϕxk and Ψxl are the <b>principal</b> <b>components</b> after decomposing pxt and rxt, respectively, {{using the}} {{weighted}} <b>principal</b> <b>components</b> algorithm.|$|R
40|$|In this study, <b>Principal</b> <b>Components</b> and Functional <b>Principal</b> <b>Components</b> Analyses {{discussed}} comparatively. Gross Domestic Product (GDP) data is {{analyzed by}} Functional <b>Principal</b> <b>Component</b> Analysis and {{the advantages of}} dealing data in a functional way is presented. GDP data (between 1987 and 2001) for seven regions of Turkey is studied by Functional <b>Principal</b> <b>Component</b> Analysis, {{it is found that}} % 99 of the variation structure {{can be explained by the}} first <b>principal</b> <b>component</b> function. It is also revealed that the variation between regions began to increase after the year 1996. However, it began to decrease rapidly after year 2000...|$|R
40|$|<b>Principal</b> <b>components</b> {{analysis}} {{relates to}} the eigenvalue distribution of Wishart matrices. Given few observations and very many variables this distribution maps to eigenvalue statistics in the Gaussian orthogonal ensemble. <b>Principal</b> <b>components</b> selection can then be based on existing analytical results. <b>Principal</b> <b>components</b> analysis Random matrix theory...|$|R
5000|$|Subsequent <b>principal</b> <b>components</b> can be {{computed}} {{by subtracting}} component [...] from [...] (see Gram-Schmidt) and then repeating this algorithm {{to find the}} next <b>principal</b> <b>component.</b> However this simple approach is not numerically stable if more than {{a small number of}} <b>principal</b> <b>components</b> are required, because imprecisions in the calculations will additively affect the estimates of subsequent <b>principal</b> <b>components.</b> More advanced methods build on this basic idea, as with the closely related Lanczos algorithm.|$|R
40|$|We wish {{to present}} a method to {{quantify}} the value modifying effects when comparing animal farms. To achieve our objective, multi-variable statistical methods were needed. We used a <b>principal</b> <b>component</b> analysis to originate three separate <b>principal</b> <b>components</b> from nine variables that determine the value of farms. A cluster analysis {{was carried out in}} order to classify farms as poor, average and excellent. The question may arise as to which <b>principal</b> <b>components</b> and which variables determine this classification. After pointing out the significance of variables and <b>principal</b> <b>components</b> in determining the quality of farms, we analysed the relationships between <b>principal</b> <b>components</b> and market prices. Some farms did not show the expected results by the discriminant analysis, so we supposed that the third <b>principal</b> <b>component</b> plays a great role in calculating prices. To prove this supposition, we applied the logistic regression method. This method shows how great a role the <b>principal</b> <b>components</b> play in classifying farms on the basis of price categories...|$|R
40|$|<b>Principal</b> <b>component</b> {{analysis}} was used to determine the dimensionality and structure of three data sets consisting of the capacity factors of eleven to twenty different solutes measured in nine different mobile phase compositions consisting of water and methanol and/or acetonitrile on three reversed-phase columns. <b>Principal</b> <b>component</b> analysis showed that two <b>principal</b> <b>components</b> could account for the total variance in the data and that the percentage variance explained by the first <b>principal</b> <b>component</b> (about 80 - 95 %) was much greater than the percentage explained by the second <b>principal</b> <b>component,</b> but that the percentage depended strongly on the choice of solutes for the sample. The first <b>principal</b> <b>component</b> could be associated with solvent strength and solvent strength selectivity and the second <b>principal</b> <b>component</b> with modifier selectivity. Solutes that showed strong modifier selectivity could be distinguished from solutes that have almost zero modifier selectivity, which could be useful for the definition of an empirical solvent strength scal...|$|R
40|$|Dimension {{reduction}} {{techniques are}} {{at the core of}} the statistical analysis of high-dimensional and functional observations. Whether the data are vector- or function-valued, <b>principal</b> <b>component</b> techniques, in this context, play a central role. The success of <b>principal</b> <b>components</b> in the dimension reduction problem is explained by the fact that, for any K <-p, the K first coefficients in the expansion of a p-dimensional random vector X in terms of its <b>principal</b> <b>components</b> is providing the best linear K-dimensional summary of X in the mean square sense. The same property holds true for a random function and its functional <b>principal</b> <b>component</b> expansion. This optimality feature, however, no longer holds true in a time series context: <b>principal</b> <b>components</b> and functional <b>principal</b> <b>components,</b> when the observations are serially dependent, are losing their optimal dimension reduction property to the so-called dynamic <b>principal</b> <b>components</b> introduced by Brillinger in 1981 in the vector case and, in the functional case, their functional extension proposed by Hormann, Kidzinski and Hallin in 2015. info:eu-repo/semantics/publishe...|$|R
40|$|In <b>principal</b> <b>component</b> {{regression}} {{there is}} a problem of selecting the number of <b>principal</b> <b>components</b> to be retained in the model. Those <b>principal</b> <b>components</b> corresponding to near-zero eigenvalues can ruin the precision of the regression coefficients estimator and therefore must be eliminated from the model. However, when the eigenspectrum gradually decays, it is difficult to decide how many <b>principal</b> <b>components</b> should be retained. In such situations cross-validation is usually used. This paper suggests another way of choosing the number of <b>principal</b> <b>components</b> by using the information complexity criterion ICOMP developed by Bozdogan for model selection (1988). An example of choosing the number of <b>principal</b> <b>components</b> to be retained in the model that predicts the venturi meter drift is given. The data for the example is from Florida Power Corporation's Crystal River Nuclear Power Plant...|$|R
50|$|Careful {{comparison}} of the random initiation approach to <b>principal</b> <b>component</b> initialization for one-dimensional SOM (models of principal curves) demonstrated that the advantages of <b>principal</b> <b>component</b> SOM initialization are not universal. The best initialization method depends on the geometry of the specific dataset. <b>Principal</b> <b>component</b> initialization is preferable (in dimension one) if the principal curve approximating the dataset can be univalently and linearly projected on the first <b>principal</b> <b>component</b> (quasilinear sets). For nonlinear datasets, however, random initiation performs better.|$|R
30|$|Eigenvalues: The {{variance}} or eigenvalue of the <b>principal</b> <b>components</b> {{given in}} Table 4 reveals that, the first 14 <b>principal</b> <b>components</b> have greater than one eigenvalue, the first 8 <b>principal</b> <b>components</b> have greater than 2 eigenvalue {{and the first}} 5 components have greater than 3.76 eigenvalue. Since the eigenvalue of a <b>principal</b> <b>components</b> <  1 implies that from total variance the variance accounted by a component is less than one, the <b>principal</b> <b>component</b> with large eigenvalue were chosen to explain the variation in the data set (usually with eigenvalue >  1). Based on this aspect the first four or seven factors or 13 can be taken.|$|R
40|$|International audienceGiven a {{stationary}} multidimensional {{process and the}} process which is deduced after centering, we wish to study possible links between the <b>principal</b> <b>component</b> analyses, in the frequency domain, of these two processes. It is well-known that there is, a priori, no obvious relationship between the centered and non-centered <b>principal</b> <b>component</b> analyses in the temporal domain. Furthermore, {{we also know that}} <b>principal</b> <b>component</b> analysis in the frequency domain is reduced to <b>principal</b> <b>component</b> analysis of each spectral component. In this paper, we show the remarkable result that the centered and non-centered <b>principal</b> <b>component</b> analyses in the frequency domain are equal except for a given frequency...|$|R
40|$|We {{develop the}} {{necessary}} methodology to conduct <b>principal</b> <b>component</b> analysis at high frequency. We construct estimators of realized eigenvalues, eigenvectors, and <b>principal</b> <b>components</b> {{and provide the}} asymptotic distribution of these estimators. Empirically, we study the high frequency covariance structure of the constituents of the S&P 100 Index using as little as one week of high frequency data at a time. The explanatory power of the high frequency <b>principal</b> <b>components</b> varies over time. During the recent financial crisis, the first <b>principal</b> <b>component</b> becomes increasingly dominant, explaining up to 60 % of the variation on its own, while the second <b>principal</b> <b>component</b> drives the common variation of financial sector stocks...|$|R
5000|$|<b>Principal</b> <b>component</b> {{analysis}} - {{transformation of}} a sample of correlated variables into uncorrelated variables (called <b>principal</b> <b>components),</b> mostly used in exploratory data analysis ...|$|R
40|$|<b>Principal</b> <b>Component</b> Analysis (PCA) {{has been}} {{implemented}} by several neural methods. We discuss a Network which {{has previously been}} shown to find the <b>Principal</b> <b>Component</b> subspace though not the actual <b>Principal</b> <b>Components</b> themselves. By introducing a constraint to the learning rule (we do not allow the weights to become negative) we cause the same network to find the actual <b>Principal</b> <b>Components.</b> We then use the network to identify individual independent sources when the signals from such sources are ORed together...|$|R
40|$|AbstractBased on {{the concept}} of {{complexity}} or minimum description length developed by Kolmogorov, Rissanen, Wallace, and others, an index of predictive power is proposed as a criterion to select the <b>principal</b> <b>components</b> of a random vector distributed in a parametric family. This criterion, when applied to the <b>principal</b> <b>components</b> selection, considers the lost information due to the reduction of the parameters as well as the observed variables. The <b>principal</b> <b>components,</b> obtained by minimizing the index of predictive power, turn out to be identical to the classical <b>principal</b> <b>components</b> when the assumed distribution is normal. A test procedure for the <b>principal</b> <b>components</b> selection is constructed and discussed. Finally, <b>principal</b> <b>components</b> for a type of ϵ-contaminated normal family are given, and are shown to converge to those of the normal distribution. Results from a simulation study are also presented...|$|R
40|$|Classical <b>principal</b> <b>component</b> {{analysis}} on manifolds, e. g. on Kendall’s shape spaces, {{is carried out}} in the tangent space of a Euclidean mean equipped with a Euclidean metric. We propose a method of <b>principal</b> <b>component</b> analysis for Riemannian manifolds based on geodesics of the intrinsic metric and provide for a numerical implementation in case of spheres. This method allows e. g. to compare <b>principal</b> <b>component</b> geodesics of different data samples. In order to determine <b>principal</b> <b>component</b> geodesics, we show that in general, due to curvature, the <b>principal</b> <b>component</b> geodesics do not pass through the intrinsic mean. As a consequence other means, different from the intrinsic mean, enter the setting allowing for several choices of a definition for geodesic variance. In conclusion we apply our method to the space of planar triangular shapes and compare our findings with standard Euclidean <b>principal</b> <b>component</b> analysis...|$|R
40|$|PRIN obtains the <b>principal</b> <b>{{components}}</b> of a {{group of}} series. The number of such components obtained may be a fixed number, or it may be determined by the amount of variance in the original series explained by the <b>principal</b> <b>components.</b> <b>Principal</b> <b>components</b> are a set of orthogonal vectors with the same number of observations as the original set of series which explain as much variance as possible of the original series. Users of this procedure should be familiar with the method and uses of <b>principal</b> <b>components,</b> which are described in many standard texts such as Harman or Theil (see the references). Usage: To obtain <b>principal</b> <b>components</b> in TSP give the word PRIN followed by a list of series whose <b>principal</b> <b>components</b> you want. The options determine how many <b>principal</b> <b>components</b> will be found. The resulting <b>principal</b> <b>components</b> are also series and are stored in data storage under the names created from the NAME = option. Options: NAME = the prefix to be given to the names of the <b>principal</b> components: the <b>components</b> will be called prefix 1, prefix 2, and so forth. You may use any legal TSP name as the name for the <b>principal</b> <b>components,</b> but the names generated by adding the numbers must also be legal TSP names (i. e., of the appropriate length). NCOM = the maximum number of components to be determined. The actual number will be the minimum of the numbe...|$|R
